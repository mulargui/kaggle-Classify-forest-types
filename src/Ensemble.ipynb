{"cells":[{"metadata":{},"cell_type":"markdown","source":"I started this competition investigating neural networks with this kernel https://www.kaggle.com/mulargui/keras-nn\nNow switching to using ensembles in this new kernel. As of today V6 is the most performant version.\nYou can find all my notes and versions at https://github.com/mulargui/kaggle-Classify-forest-types"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#load data\ndftrain=pd.read_csv('/kaggle/input/learn-together/train.csv')\ndftest=pd.read_csv('/kaggle/input/learn-together/test.csv')\n\n####### DATA PREPARATION #####\n#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)\n\n# one data set with all features\nX = pd.concat([x,x_predict],keys=[0,1])","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### FEATURE ENGINEERING #####\n#https://www.kaggle.com/evimarp/top-6-roosevelt-national-forest-competition\nfrom itertools import combinations\nfrom bisect import bisect\nX['Euclidean_distance_to_hydro'] = (X.Vertical_Distance_To_Hydrology**2 \n                                         + X.Horizontal_Distance_To_Hydrology**2)**.5\n\ncols = [\n        'Horizontal_Distance_To_Roadways',\n        'Horizontal_Distance_To_Fire_Points',\n        'Horizontal_Distance_To_Hydrology',\n]\nX['distance_mean'] = X[cols].mean(axis=1)\nX['distance_sum'] = X[cols].sum(axis=1)\nX['distance_road_fire'] = X[cols[:2]].mean(axis=1)\nX['distance_hydro_fire'] = X[cols[1:]].mean(axis=1)\nX['distance_road_hydro'] = X[[cols[0], cols[2]]].mean(axis=1)\n    \nX['distance_sum_road_fire'] = X[cols[:2]].sum(axis=1)\nX['distance_sum_hydro_fire'] = X[cols[1:]].sum(axis=1)\nX['distance_sum_road_hydro'] = X[[cols[0], cols[2]]].sum(axis=1)\n    \nX['distance_dif_road_fire'] = X[cols[0]] - X[cols[1]]\nX['distance_dif_hydro_road'] = X[cols[2]] - X[cols[0]]\nX['distance_dif_hydro_fire'] = X[cols[2]] - X[cols[1]]\n    \n# Vertical distances measures\ncolv = ['Elevation', 'Vertical_Distance_To_Hydrology']\nX['Vertical_dif'] = X[colv[0]] - X[colv[1]]\nX['Vertical_sum'] = X[colv].sum(axis=1)\n    \nSHADES = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    \nX['shade_noon_diff'] = X['Hillshade_9am'] - X['Hillshade_Noon']\nX['shade_3pm_diff'] = X['Hillshade_Noon'] - X['Hillshade_3pm']\nX['shade_all_diff'] = X['Hillshade_9am'] - X['Hillshade_3pm']\nX['shade_sum'] = X[SHADES].sum(axis=1)\nX['shade_mean'] = X[SHADES].mean(axis=1)\n  \nX['ElevationHydro'] = X['Elevation'] - 0.25 * X['Euclidean_distance_to_hydro']\nX['ElevationV'] = X['Elevation'] - X['Vertical_Distance_To_Hydrology']\nX['ElevationH'] = X['Elevation'] - 0.19 * X['Horizontal_Distance_To_Hydrology']\n\nX['Elevation2'] = X['Elevation']**2\nX['ElevationLog'] = np.log1p(X['Elevation'])\n\nX['Aspect_cos'] = np.cos(np.radians(X.Aspect))\nX['Aspect_sin'] = np.sin(np.radians(X.Aspect))\n#df['Slope_sin'] = np.sin(np.radians(df.Slope))\nX['Aspectcos_Slope'] = X.Slope * X.Aspect_cos\n#df['Aspectsin_Slope'] = df.Slope * df.Aspect_sin\n    \ncardinals = [i for i in range(45, 361, 90)]\npoints = ['N', 'E', 'S', 'W']\nX['Cardinal'] = X.Aspect.apply(lambda x: points[bisect(cardinals, x) % 4])\n\nd = {'N': 0, 'E': 1, 'S': 0, 'W':-1}\nX['Cardinal'] = X.Cardinal.apply(lambda x: d[x])\n\n#adding features based on https://douglas-fraser.com/forest_cover_management.pdf pages 21,22\n#note: not all climatic and geologic codes have a soil type\ncolumns=['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6']\nX['Climatic2'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type7', 'Soil_Type8']\nX['Climatic3'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13']\nX['Climatic4'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type14', 'Soil_Type15']\nX['Climatic5'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type16', 'Soil_Type17', 'Soil_Type18']\nX['Climatic6'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n    'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n    'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34']\nX['Climatic7'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\nX['Climatic8'] = np.select([X[columns].sum(1).gt(0)], [1])\n\ncolumns=['Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type19', 'Soil_Type20',\n    'Soil_Type21']\nX['Geologic1'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type9', 'Soil_Type22', 'Soil_Type23']\nX['Geologic2'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type7', 'Soil_Type8']\nX['Geologic5'] = np.select([X[columns].sum(1).gt(0)], [1])\ncolumns=['Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6',\n    'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type18', 'Soil_Type24',\n    'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30',\n    'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', \n    'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\nX['Geologic7'] = np.select([X[columns].sum(1).gt(0)], [1])\n\n#Reversing One-Hot-Encoding to Categorical attributes, several articles recommend it for decision tree algorithms\n#Doing it for Soil_Type, Wilderness_Area, Geologic and Climatic\n#we are also replacing the categorical values by random numbers to difficult to the algorythm to find relationships between the values\nX['Tmp']=np.where(X.loc[:, 'Soil_Type1':'Soil_Type40'])[1] +1\ncols = [c for c in X.columns if c[:9] != 'Soil_Type']\nX=X[cols]\nX=X.rename(columns = {'Tmp':'Soil_Type'})\n\nmin=X['Soil_Type'].min()\nmax=X['Soil_Type'].max()\ns=np.random.randint(low=1, high=np.iinfo(np.int64).max, size=(max-min+1), dtype='int64')\nfor i in range (min,max+1):\n    X['Soil_Type'].replace(to_replace=i, value=s[i-min], inplace=True)\n\nX['Tmp']=np.where(X.loc[:, 'Wilderness_Area1':'Wilderness_Area4'])[1] +1\ncols = [c for c in X.columns if c[:15] != 'Wilderness_Area']\nX=X[cols]\nX=X.rename(columns = {'Tmp':'Wilderness_Area'})\n\nmin=X['Wilderness_Area'].min()\nmax=X['Wilderness_Area'].max()\ns=np.random.randint(low=1, high=np.iinfo(np.int64).max, size=(max-min+1), dtype='int64')\nfor i in range (min,max+1):\n    X['Wilderness_Area'].replace(to_replace=i, value=s[i-min], inplace=True)\n\nX['Tmp']=np.where(X.loc[:, 'Climatic2':'Climatic8'])[1] +1\ncols = [c for c in X.columns if c[:8] != 'Climatic']\nX=X[cols]\nX=X.rename(columns = {'Tmp':'Climatic'})\n\nmin=X['Climatic'].min()\nmax=X['Climatic'].max()\ns=np.random.randint(low=1, high=np.iinfo(np.int64).max, size=(max-min+1), dtype='int64')\nfor i in range (min,max+1):\n    X['Climatic'].replace(to_replace=i, value=s[i-min], inplace=True)\n\nX['Tmp']=np.where(X.loc[:, 'Geologic1':'Geologic7'])[1] +1\ncols = [c for c in X.columns if c[:8] != 'Geologic']\nX=X[cols]\nX=X.rename(columns = {'Tmp':'Geologic'})\n\nmin=X['Geologic'].min()\nmax=X['Geologic'].max()\ns=np.random.randint(low=1, high=np.iinfo(np.int64).max, size=(max-min+1), dtype='int64')\nfor i in range (min,max+1):\n    X['Geologic'].replace(to_replace=i, value=s[i-min], inplace=True)\n\n#https://www.kaggle.com/arateris/2-layer-k-fold-learning-forest-cover \n# Add PCA features\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=0.99).fit(X)\ntrans = pca.transform(X)\n\nfor i in range(trans.shape[1]):\n    col_name= 'pca'+str(i+1)\n    X[col_name] = trans[:,i]\n\n# Adding Gaussian Mixture features to perform some unsupervised learning hints from the full data\n#https://www.kaggle.com/arateris/2-layer-k-fold-learning-forest-cover \n#https://www.kaggle.com/stevegreenau/stacking-multiple-classifiers-clustering\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.preprocessing import StandardScaler\ngmix = GaussianMixture(n_components=10) \ngaussian = gmix.fit_predict(StandardScaler().fit_transform(X))\nX['GM'] = gaussian\n\n# Scale and bin features\nfrom sklearn.preprocessing import MinMaxScaler\nX.loc[:, :] = np.floor(MinMaxScaler((0, 100)).fit_transform(X))\nX = X.astype('int8')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#break it down again in train and test\nx,x_predict = X.xs(0),X.xs(1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### THIS IS THE ENSEMBLE SECTION ######\n#https://www.kaggle.com/kwabenantim/forest-cover-stacking-multiple-classifiers\nimport random\nfrom lightgbm import LGBMClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nrandomstate = 1\nrandom.seed(randomstate)\nnp.random.seed(randomstate)\n\nab_clf = AdaBoostClassifier(n_estimators=200,\n                            base_estimator=DecisionTreeClassifier(\n                                min_samples_leaf=2,\n                                random_state=randomstate),\n                            random_state=randomstate)\n\n#max_features = min(30, x.columns.size)\nmax_features = 30\net_clf = ExtraTreesClassifier(n_estimators=300,\n                              min_samples_leaf=2,\n                              min_samples_split=2,\n                              max_depth=50,\n                              max_features=max_features,\n                              random_state=randomstate,\n                              n_jobs=1)\n\nlg_clf = LGBMClassifier(n_estimators=300,\n                        num_leaves=128,\n                        verbose=-1,\n                        random_state=randomstate,\n                        n_jobs=1)\n\nrf_clf = RandomForestClassifier(n_estimators=300,\n                                random_state=randomstate,\n                                n_jobs=1)\n\n#Added a KNN classifier to the ensemble\n#https://www.kaggle.com/edumunozsala/feature-eng-and-a-simple-stacked-model\nknn_clf = KNeighborsClassifier(n_neighbors=y.nunique(), n_jobs=1)\n\nensemble = [('AdaBoostClassifier', ab_clf),\n            ('ExtraTreesClassifier', et_clf),\n            ('LGBMClassifier', lg_clf),\n            ('KNNClassifier', knn_clf),\n            ('RandomForestClassifier', rf_clf)\n]\n\n#Cross-validating classifiers\nfor label, clf in ensemble:\n    score = cross_val_score(clf, x, y,\n                            cv=5,\n                            scoring='accuracy',\n                            verbose=0,\n                            n_jobs=-1)\n# Fitting stack\nstack = StackingCVClassifier(classifiers=[ab_clf, et_clf, lg_clf, knn_clf, rf_clf],\n                             meta_classifier=rf_clf,\n                             cv=5,\n                             stratify=True,\n                             shuffle=True,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             random_state=randomstate)\n\nstack = stack.fit(x, y)\n\ny_predict = stack.predict(x_predict)\ny_predict = pd.Series(y_predict, index=x_predict.index, dtype=y.dtype)","execution_count":5,"outputs":[{"output_type":"stream","text":"Fitting 5 classifiers...\nFitting classifier1: adaboostclassifier (1/5)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.5min finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"Fitting classifier2: extratreesclassifier (2/5)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.3min finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"Fitting classifier3: lgbmclassifier (3/5)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.6min finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"Fitting classifier4: kneighborsclassifier (4/5)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    2.5s finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"Fitting classifier5: randomforestclassifier (5/5)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   52.5s finished\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save predictions to a file for submission\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict})\noutput.to_csv('submission.csv', index=False)\n\n#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}