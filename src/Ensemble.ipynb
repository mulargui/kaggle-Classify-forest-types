{"cells":[{"metadata":{},"cell_type":"markdown","source":"> I started this competition investigating neural networks with this kernel https://www.kaggle.com/mulargui/keras-nn\nNow switching to using ensembles in this new kernel.\nYou can find all my notes and versions at https://github.com/mulargui/kaggle-Classify-forest-types"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#load data\ndftrain=pd.read_csv('/kaggle/input/learn-together/train.csv')\ndftest=pd.read_csv('/kaggle/input/learn-together/test.csv')\n\n####### DATA PREPARATION #####","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/arateris/2-layer-k-fold-learning-forest-cover \n#Fixing Hillshade_3pm\n#replacing the zeros for better guess, mainly to avoid zeros in the feature engineering and fake outliers. \nnum_train = len(dftrain)\ntmp = dftrain.drop('Cover_Type', axis = 1)\nall_data = tmp.append(dftest)\n\ncols_for_HS = ['Aspect','Slope', 'Hillshade_9am','Hillshade_Noon']\nHS_zero = all_data[all_data.Hillshade_3pm==0]\nHS_train = all_data[all_data.Hillshade_3pm!=0]\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf_hs = RandomForestRegressor(n_estimators=100).fit(HS_train[cols_for_HS], HS_train.Hillshade_3pm)\nout = rf_hs.predict(HS_zero[cols_for_HS]).astype(int)\n\n#I couldn't make this line work, feature not used\n#all_data.loc[HS_zero.index,'Hillshade_3pm'] = out\n#dftrain['Hillshade_3pm']= all_data.loc[:num_train,'Hillshade_3pm']\n#dftest['Hillshade_3pm']= all_data.loc[num_train:,'Hillshade_3pm']\n\n# Add PCA features\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=0.99).fit(all_data)\ntrans = pca.transform(all_data)\n\nfor i in range(trans.shape[1]):\n    col_name= 'pca'+str(i+1)\n    dftrain[col_name] = trans[:num_train, i]\n    dftest[col_name] = trans[num_train:, i]\n\n#https://www.kaggle.com/evimarp/top-6-roosevelt-national-forest-competition\ndef euclidean(df):\n    df['Euclidean_distance_to_hydro'] = (df.Vertical_Distance_To_Hydrology**2 \n                                         + df.Horizontal_Distance_To_Hydrology**2)**.5\n    return df\n\ndftrain = euclidean(dftrain)\ndftest = euclidean(dftest)\n\nfrom itertools import combinations\ndef distances(df):\n    cols = [\n        'Horizontal_Distance_To_Roadways',\n        'Horizontal_Distance_To_Fire_Points',\n        'Horizontal_Distance_To_Hydrology',\n    ]\n    df['distance_mean'] = df[cols].mean(axis=1)\n    df['distance_sum'] = df[cols].sum(axis=1)\n    df['distance_road_fire'] = df[cols[:2]].mean(axis=1)\n    df['distance_hydro_fire'] = df[cols[1:]].mean(axis=1)\n    df['distance_road_hydro'] = df[[cols[0], cols[2]]].mean(axis=1)\n    \n    df['distance_sum_road_fire'] = df[cols[:2]].sum(axis=1)\n    df['distance_sum_hydro_fire'] = df[cols[1:]].sum(axis=1)\n    df['distance_sum_road_hydro'] = df[[cols[0], cols[2]]].sum(axis=1)\n    \n    df['distance_dif_road_fire'] = df[cols[0]] - df[cols[1]]\n    df['distance_dif_hydro_road'] = df[cols[2]] - df[cols[0]]\n    df['distance_dif_hydro_fire'] = df[cols[2]] - df[cols[1]]\n    \n    # Vertical distances measures\n    colv = ['Elevation', 'Vertical_Distance_To_Hydrology']\n    df['Vertical_dif'] = df[colv[0]] - df[colv[1]]\n    df['Vertical_sum'] = df[colv].sum(axis=1)\n    \n    return df\n  \ndftrain = distances(dftrain)\ndftest = distances(dftest)\n    \ndef shade(df):\n    SHADES = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    \n    df['shade_noon_diff'] = df['Hillshade_9am'] - df['Hillshade_Noon']\n    df['shade_3pm_diff'] = df['Hillshade_Noon'] - df['Hillshade_3pm']\n    df['shade_all_diff'] = df['Hillshade_9am'] - df['Hillshade_3pm']\n    df['shade_sum'] = df[SHADES].sum(axis=1)\n    df['shade_mean'] = df[SHADES].mean(axis=1)\n    \n    return df\n\ndftrain = shade(dftrain)\ndftest = shade(dftest)\n\ndef elevation(df):\n    df['ElevationHydro'] = df['Elevation'] - 0.25 * df['Euclidean_distance_to_hydro']\n    return df\n\ndftrain = elevation(dftrain)\ndftest = elevation(dftest)\n\ndef elevationV(df):\n    df['ElevationV'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    return df\n\ndftrain = elevationV(dftrain)\ndftest = elevationV(dftest)\n\ndef elevationH(df):\n    df['ElevationH'] = df['Elevation'] - 0.19 * df['Horizontal_Distance_To_Hydrology']\n    return df\n\ndftrain = elevationH(dftrain)\ndftest = elevationH(dftest)\n\ndef kernel_features(df):\n    df['Elevation2'] = df['Elevation']**2\n    df['ElevationLog'] = np.log1p(df['Elevation'])\n    return df\n\ndftrain = kernel_features(dftrain)\ndftest = kernel_features(dftest)\n\ndef degree(df):\n    df['Aspect_cos'] = np.cos(np.radians(df.Aspect))\n    df['Aspect_sin'] = np.sin(np.radians(df.Aspect))\n    #df['Slope_sin'] = np.sin(np.radians(df.Slope))\n    df['Aspectcos_Slope'] = df.Slope * df.Aspect_cos\n    #df['Aspectsin_Slope'] = df.Slope * df.Aspect_sin\n    \n    return df\n\ndftrain = degree(dftrain)\ndftest = degree(dftest)\n\nfrom bisect import bisect\ncardinals = [i for i in range(45, 361, 90)]\npoints = ['N', 'E', 'S', 'W']\n\ndef cardinal(df):\n    df['Cardinal'] = df.Aspect.apply(lambda x: points[bisect(cardinals, x) % 4])\n    return df\n\ndftrain = cardinal(dftrain)\ndftest = cardinal(dftest)\n\ndef cardinal_num(df):\n    d = {'N': 0, 'E': 1, 'S': 0, 'W':-1}\n    df['Cardinal'] = df.Cardinal.apply(lambda x: d[x])\n    return df\n\ndftrain = cardinal_num(dftrain)\ndftest = cardinal_num(dftest)\n\n#adding features based on https://douglas-fraser.com/forest_cover_management.pdf pages 21,22\n#note: not all climatic and geologic codes have a soil type\n\ndef Climatic2(row): \n    if (row['Soil_Type1'] == 1) or (row['Soil_Type2'] == 1) or (row['Soil_Type3'] == 1) or (row['Soil_Type4'] == 1) \\\n        or (row['Soil_Type5'] == 1) or (row['Soil_Type6'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic2'] = dftrain.apply (lambda row: Climatic2(row), axis=1)\ndftest['Climatic2'] = dftest.apply (lambda row: Climatic2(row), axis=1)\n\ndef Climatic3(row): \n    if (row['Soil_Type7'] == 1) or (row['Soil_Type8'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic3'] = dftrain.apply (lambda row: Climatic3(row), axis=1)\ndftest['Climatic3'] = dftest.apply (lambda row: Climatic3(row), axis=1)\n\ndef Climatic4(row): \n    if (row['Soil_Type9'] == 1) or (row['Soil_Type10'] == 1) or (row['Soil_Type11'] == 1) or (row['Soil_Type12'] == 1) \\\n        or (row['Soil_Type13'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic4'] = dftrain.apply (lambda row: Climatic4(row), axis=1)\ndftest['Climatic4'] = dftest.apply (lambda row: Climatic4(row), axis=1)\n\ndef Climatic5(row): \n    if (row['Soil_Type14'] == 1) or (row['Soil_Type15'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic5'] = dftrain.apply (lambda row: Climatic5(row), axis=1)\ndftest['Climatic5'] = dftest.apply (lambda row: Climatic5(row), axis=1)\n\ndef Climatic6(row): \n    if (row['Soil_Type16'] == 1) or (row['Soil_Type17'] == 1) or (row['Soil_Type18'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic6'] = dftrain.apply (lambda row: Climatic6(row), axis=1)\ndftest['Climatic6'] = dftest.apply (lambda row: Climatic6(row), axis=1)\n\ndef Climatic7(row): \n    if (row['Soil_Type19'] == 1) or (row['Soil_Type20'] == 1) or (row['Soil_Type21'] == 1) or (row['Soil_Type22'] == 1) \\\n        or (row['Soil_Type23'] == 1) or (row['Soil_Type24'] == 1) or (row['Soil_Type25'] == 1) or (row['Soil_Type26'] == 1) \\\n        or (row['Soil_Type27'] == 1) or (row['Soil_Type28'] == 1) or (row['Soil_Type29'] == 1) or (row['Soil_Type30'] == 1) \\\n        or (row['Soil_Type31'] == 1) or (row['Soil_Type32'] == 1) or (row['Soil_Type33'] == 1) or (row['Soil_Type34'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic7'] = dftrain.apply (lambda row: Climatic7(row), axis=1)\ndftest['Climatic7'] = dftest.apply (lambda row: Climatic7(row), axis=1)\n\ndef Climatic8(row): \n    if (row['Soil_Type35'] == 1) or (row['Soil_Type36'] == 1) or (row['Soil_Type37'] == 1) or (row['Soil_Type38'] == 1) \\\n        or (row['Soil_Type39'] == 1) or (row['Soil_Type40'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic8'] = dftrain.apply (lambda row: Climatic8(row), axis=1)\ndftest['Climatic8'] = dftest.apply (lambda row: Climatic8(row), axis=1)\n\ndef Geologic1(row): \n    if (row['Soil_Type14'] == 1) or (row['Soil_Type15'] == 1) or (row['Soil_Type16'] == 1) or (row['Soil_Type17'] == 1) \\\n        or (row['Soil_Type19'] == 1) or (row['Soil_Type20'] == 1) or (row['Soil_Type21'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic1'] = dftrain.apply (lambda row: Geologic1(row), axis=1)\ndftest['Geologic1'] = dftest.apply (lambda row: Geologic1(row), axis=1)\n\ndef Geologic2(row): \n    if (row['Soil_Type9'] == 1) or (row['Soil_Type22'] == 1) or (row['Soil_Type23'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic2'] = dftrain.apply (lambda row: Geologic2(row), axis=1)\ndftest['Geologic2'] = dftest.apply (lambda row: Geologic2(row), axis=1)\n\ndef Geologic5(row): \n    if (row['Soil_Type7'] == 1) or (row['Soil_Type8'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic5'] = dftrain.apply (lambda row: Geologic5(row), axis=1)\ndftest['Geologic5'] = dftest.apply (lambda row: Geologic5(row), axis=1)\n\ndef Geologic7(row): \n    if (row['Soil_Type1'] == 1) or (row['Soil_Type2'] == 1) or (row['Soil_Type3'] == 1) or (row['Soil_Type4'] == 1) \\\n        or (row['Soil_Type5'] == 1) or (row['Soil_Type6'] == 1) or (row['Soil_Type10'] == 1) \\\n        or (row['Soil_Type11'] == 1) or (row['Soil_Type12'] == 1) or (row['Soil_Type13'] == 1) or (row['Soil_Type18'] == 1) \\\n        or (row['Soil_Type24'] == 1) or (row['Soil_Type25'] == 1) or (row['Soil_Type26'] == 1) or (row['Soil_Type27'] == 1) \\\n        or (row['Soil_Type28'] == 1) or (row['Soil_Type29'] == 1) or (row['Soil_Type30'] == 1) or (row['Soil_Type31'] == 1) \\\n        or (row['Soil_Type32'] == 1) or (row['Soil_Type33'] == 1) or (row['Soil_Type34'] == 1) or (row['Soil_Type35'] == 1) \\\n        or (row['Soil_Type36'] == 1) or (row['Soil_Type37'] == 1) or (row['Soil_Type38'] == 1) or (row['Soil_Type39'] == 1) \\\n        or (row['Soil_Type40'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic7'] = dftrain.apply (lambda row: Geologic7(row), axis=1)\ndftest['Geologic7'] = dftest.apply (lambda row: Geologic7(row), axis=1)\n\n#Reversing One-Hot-Encoding to Categorical attributes, several articles recommend it for decision tree algorithms\n#Doing it for Soil_Type, Wilderness_Area, Geologic and Climatic\n#we are also replacing the categorical values by random numbers to difficult to the algorythm to find relationships between the values\ndftrain['Tmp']=np.where(dftrain.loc[:, 'Soil_Type1':'Soil_Type40'])[1] +1\ndftest['Tmp']=np.where(dftest.loc[:, 'Soil_Type1':'Soil_Type40'])[1] +1\n\ncols = [c for c in dftrain.columns if c[:9] != 'Soil_Type']\ndftrain=dftrain[cols]\ncols = [c for c in dftest.columns if c[:9] != 'Soil_Type']\ndftest=dftest[cols]\n\ndftrain=dftrain.rename(columns = {'Tmp':'Soil_Type'})\ndftest=dftest.rename(columns = {'Tmp':'Soil_Type'})\n\nmin=dftrain['Soil_Type'].min()\nmin2=dftest['Soil_Type'].min()\nif (min2 < min):\n    min=min2\n\nmax=dftrain['Soil_Type'].max()\nmax2=dftest['Soil_Type'].max()\nif (max2 > max):\n    max=max2\n\ns=np.random.randint(low=1, high=np.iinfo(np.int64).max, size=(max-min+1), dtype='int64')\n\nfor i in range (min,max+1):\n    dftrain['Soil_Type'].replace(to_replace=i, value=s[i-min], inplace=True)\n    dftest['Soil_Type'].replace(to_replace=i, value=s[i-min], inplace=True)\n\ndftrain['Tmp']=np.where(dftrain.loc[:, 'Wilderness_Area1':'Wilderness_Area4'])[1] +1\ndftest['Tmp']=np.where(dftest.loc[:, 'Wilderness_Area1':'Wilderness_Area4'])[1] +1\n\ncols = [c for c in dftrain.columns if c[:15] != 'Wilderness_Area']\ndftrain=dftrain[cols]\ncols = [c for c in dftest.columns if c[:15] != 'Wilderness_Area']\ndftest=dftest[cols]\n\ndftrain=dftrain.rename(columns = {'Tmp':'Wilderness_Area'})\ndftest=dftest.rename(columns = {'Tmp':'Wilderness_Area'})\n\nmin=dftrain['Wilderness_Area'].min()\nmin2=dftest['Wilderness_Area'].min()\nif (min2 < min):\n    min=min2\n\nmax=dftrain['Wilderness_Area'].max()\nmax2=dftest['Wilderness_Area'].max()\nif (max2 > max):\n    max=max2\n\ns=np.random.randint(low=1, high=np.iinfo(np.int64).max, size=(max-min+1), dtype='int64')\n\nfor i in range (min,max+1):\n    dftrain['Wilderness_Area'].replace(to_replace=i, value=s[i-min], inplace=True)\n    dftest['Wilderness_Area'].replace(to_replace=i, value=s[i-min], inplace=True)\n\ndftrain['Tmp']=np.where(dftrain.loc[:, 'Climatic2':'Climatic8'])[1] +1\ndftest['Tmp']=np.where(dftest.loc[:, 'Climatic2':'Climatic8'])[1] +1\n\ncols = [c for c in dftrain.columns if c[:8] != 'Climatic']\ndftrain=dftrain[cols]\ncols = [c for c in dftest.columns if c[:8] != 'Climatic']\ndftest=dftest[cols]\n\ndftrain=dftrain.rename(columns = {'Tmp':'Climatic'})\ndftest=dftest.rename(columns = {'Tmp':'Climatic'})\n\nmin=dftrain['Climatic'].min()\nmin2=dftest['Climatic'].min()\nif (min2 < min):\n    min=min2\n\nmax=dftrain['Climatic'].max()\nmax2=dftest['Climatic'].max()\nif (max2 > max):\n    max=max2\n\ns=np.random.randint(low=1, high=np.iinfo(np.int64).max, size=(max-min+1), dtype='int64')\n\nfor i in range (min,max+1):\n    dftrain['Climatic'].replace(to_replace=i, value=s[i-min], inplace=True)\n    dftest['Climatic'].replace(to_replace=i, value=s[i-min], inplace=True)\n\ndftrain['Tmp']=np.where(dftrain.loc[:, 'Geologic1':'Geologic7'])[1] +1\ndftest['Tmp']=np.where(dftest.loc[:, 'Geologic1':'Geologic7'])[1] +1\n\ncols = [c for c in dftrain.columns if c[:8] != 'Geologic']\ndftrain=dftrain[cols]\ncols = [c for c in dftest.columns if c[:8] != 'Geologic']\ndftest=dftest[cols]\n\ndftrain=dftrain.rename(columns = {'Tmp':'Geologic'})\ndftest=dftest.rename(columns = {'Tmp':'Geologic'})\n\nmin=dftrain['Geologic'].min()\nmin2=dftest['Geologic'].min()\nif (min2 < min):\n    min=min2\n\nmax=dftrain['Geologic'].max()\nmax2=dftest['Geologic'].max()\nif (max2 > max):\n    max=max2\n\ns=np.random.randint(low=1, high=np.iinfo(np.int64).max, size=(max-min+1), dtype='int64')\n\nfor i in range (min,max+1):\n    dftrain['Geologic'].replace(to_replace=i, value=s[i-min], inplace=True)\n    dftest['Geologic'].replace(to_replace=i, value=s[i-min], inplace=True)","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)\n\n# Scale and bin features\nfrom sklearn.preprocessing import MinMaxScaler\nX = pd.concat([x,x_predict],keys=[0,1])\nX.loc[:, :] = np.floor(MinMaxScaler((0, 100)).fit_transform(X))\nX = X.astype('int8')\nx,x_predict = X.xs(0),X.xs(1)","execution_count":3,"outputs":[{"output_type":"stream","text":"(15120, 43)\n(565892, 43)\n(15120, 43)\n(565892, 43)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"###### THIS IS THE ENSEMBLE SECTION ######\n#https://www.kaggle.com/kwabenantim/forest-cover-stacking-multiple-classifiers\nimport random\nfrom lightgbm import LGBMClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nrandomstate = 1\nrandom.seed(randomstate)\nnp.random.seed(randomstate)\n\n#max_features = min(30, x.columns.size)\nmax_features = 30\n\nab_clf = AdaBoostClassifier(n_estimators=200,\n                            base_estimator=DecisionTreeClassifier(\n                                min_samples_leaf=2,\n                                random_state=randomstate),\n                            random_state=randomstate)\n\net_clf = ExtraTreesClassifier(n_estimators=300,\n                              min_samples_leaf=2,\n                              min_samples_split=2,\n                              max_depth=50,\n                              max_features=max_features,\n                              random_state=randomstate,\n                              n_jobs=1)\n\nlg_clf = LGBMClassifier(n_estimators=300,\n                        num_leaves=128,\n                        verbose=-1,\n                        random_state=randomstate,\n                        n_jobs=1)\n\nrf_clf = RandomForestClassifier(n_estimators=300,\n                                random_state=randomstate,\n                                n_jobs=1)\n\nensemble = [('AdaBoostClassifier', ab_clf),\n            ('ExtraTreesClassifier', et_clf),\n            ('LGBMClassifier', lg_clf),\n            ('RandomForestClassifier', rf_clf)]\n\n#Cross-validating classifiers\nfor label, clf in ensemble:\n    score = cross_val_score(clf, x, y,\n                            cv=5,\n                            scoring='accuracy',\n                            verbose=0,\n                            n_jobs=-1)\n# Fitting stack\nstack = StackingCVClassifier(classifiers=[ab_clf, et_clf, lg_clf, rf_clf],\n                             meta_classifier=rf_clf,\n                             cv=5,\n                             stratify=True,\n                             shuffle=True,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             random_state=randomstate)\n\nstack = stack.fit(x, y)\n\ny_predict = stack.predict(x_predict)\ny_predict = pd.Series(y_predict, index=x_predict.index, dtype=y.dtype)","execution_count":5,"outputs":[{"output_type":"stream","text":"Fitting 4 classifiers...\nFitting classifier1: adaboostclassifier (1/4)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  5.3min finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"Fitting classifier2: extratreesclassifier (2/4)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  1.4min finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"Fitting classifier3: lgbmclassifier (3/4)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  2.8min finished\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n","name":"stderr"},{"output_type":"stream","text":"Fitting classifier4: randomforestclassifier (4/4)\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:   54.8s finished\n","name":"stderr"},{"output_type":"error","ename":"NameError","evalue":"name 'predictions' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-286eece7007d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0my_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save predictions to a file for submission\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict})\noutput.to_csv('submission.csv', index=False)\n\n#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}