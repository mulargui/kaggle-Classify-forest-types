{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first experiment in this competition. Whereas XGBoost is highly recommended I rather tried to see how far I can go with an NN (using Keras).\n\nThis is the basic model and with 250 epochs has an accuracy of 80% (really poor).\n\nI'll continue for a few days researching how much I can optimize this model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\nprint(os.listdir(\"../working/\"))\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['learn-together']\n['__notebook_source__.ipynb', '.ipynb_checkpoints']\n/kaggle/input/learn-together/sample_submission.csv\n/kaggle/input/learn-together/test.csv\n/kaggle/input/learn-together/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load data, I had an issue with the data and hacked the data into the kernel\n#dftrain=pd.read_csv('/kaggle/input/train.csv')\n#dftest=pd.read_csv('/kaggle/input/test.csv')\ndftrain=pd.read_csv('/kaggle/input/learn-together/train.csv')\ndftest=pd.read_csv('/kaggle/input/learn-together/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#file shape\nprint(dftrain.head())\nprint(dftrain.shape[0])\nprint(dftest.head())\nprint(dftest.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features types\nprint(dftrain.dtypes)\nprint(dftest.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate files - nans per feature\nprint(dftrain.isnull().sum(axis = 0))\nprint(dftest.isnull().sum(axis = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate files - no rows with all zeros\nprint(dftrain[dftrain.drop(['Id','Cover_Type'], axis=1).eq(0).all(1)].empty)\nprint(dftest[dftest.drop('Id', axis=1).eq(0).all(1)].empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#force all types to float\nx = x.astype(float)\nx_predict = x_predict.astype(float)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize features\n#in the future it can be done more elegantly, for now just using the max min values of the data that we know\n#x['Elevation']=(x['Elevation']-x['Elevation'].min())/(x['Elevation'].max()-x['Elevation'].min())                             \nx['Elevation']=(x['Elevation']-1859)/(3858-1859)                             \nx['Aspect']=x['Aspect']/360                      \nx['Slope']=x['Slope']/66                      \nx['Horizontal_Distance_To_Hydrology']=x['Horizontal_Distance_To_Hydrology']/1397                      \nx['Vertical_Distance_To_Hydrology']=(x['Vertical_Distance_To_Hydrology']+173)/(601+173)                             \nx['Horizontal_Distance_To_Roadways']=x['Horizontal_Distance_To_Roadways']/7117                      \nx['Hillshade_9am']=x['Hillshade_9am']/254                      \nx['Hillshade_Noon']=x['Hillshade_Noon']/254                      \nx['Hillshade_3pm']=x['Hillshade_3pm']/254                      \nx['Horizontal_Distance_To_Fire_Points']=x['Horizontal_Distance_To_Fire_Points']/67173                      \n                                \nx_predict['Elevation']=(x_predict['Elevation']-1859)/(3858-1859)                             \nx_predict['Aspect']=x_predict['Aspect']/360                      \nx_predict['Slope']=x_predict['Slope']/66                      \nx_predict['Horizontal_Distance_To_Hydrology']=x_predict['Horizontal_Distance_To_Hydrology']/1397                      \nx_predict['Vertical_Distance_To_Hydrology']=(x_predict['Vertical_Distance_To_Hydrology']+173)/(601+173)                             \nx_predict['Horizontal_Distance_To_Roadways']=x_predict['Horizontal_Distance_To_Roadways']/7117                      \nx_predict['Hillshade_9am']=x_predict['Hillshade_9am']/254                      \nx_predict['Hillshade_Noon']=x_predict['Hillshade_Noon']/254                      \nx_predict['Hillshade_3pm']=x_predict['Hillshade_3pm']/254                      \nx_predict['Horizontal_Distance_To_Fire_Points']=x_predict['Horizontal_Distance_To_Fire_Points']/67173                      ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate data - no rows with all zeros\n#x.index[x.eq(0).all(1)]\nprint(x[x.eq(0).all(1)].empty)\nprint(x_predict[x_predict.eq(0).all(1)].empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the label to One Hot Encoding\n#to_categorical requires 0..6 instead of 1..7\ny -=1\ny = y.to_numpy()\n\nnum_classes = 7\n\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y, num_classes)","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the features dataframes to numpy arrays\nx = x.to_numpy()\nx_predict = x_predict.to_numpy()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split in train (80%) and test (20%) sets \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here is the NN model\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\nnum_features = 54\n\nmodel = Sequential()\nmodel.add(Dense(units=num_features, activation='relu', kernel_initializer='normal', input_dim=num_features))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n#              optimizer='Adadelta',\n              optimizer='Adam',\n              metrics=['accuracy'])","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=250)","execution_count":11,"outputs":[{"output_type":"stream","text":"Train on 12096 samples, validate on 3024 samples\nEpoch 1/250\n12096/12096 [==============================] - 2s 124us/sample - loss: 1.2501 - acc: 0.4377 - val_loss: 0.9161 - val_acc: 0.5939\nEpoch 2/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.9538 - acc: 0.5748 - val_loss: 0.8543 - val_acc: 0.6396\nEpoch 3/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.8852 - acc: 0.6049 - val_loss: 0.8288 - val_acc: 0.6366\nEpoch 4/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.8525 - acc: 0.6233 - val_loss: 0.7972 - val_acc: 0.6442\nEpoch 5/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.8159 - acc: 0.6538 - val_loss: 0.7780 - val_acc: 0.6822\nEpoch 6/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.7916 - acc: 0.6654 - val_loss: 0.7343 - val_acc: 0.6951\nEpoch 7/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.7719 - acc: 0.6782 - val_loss: 0.7461 - val_acc: 0.6855\nEpoch 8/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.7648 - acc: 0.6812 - val_loss: 0.7048 - val_acc: 0.7080\nEpoch 9/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.7463 - acc: 0.6884 - val_loss: 0.7159 - val_acc: 0.7001\nEpoch 10/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.7342 - acc: 0.6952 - val_loss: 0.6864 - val_acc: 0.7087\nEpoch 11/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.7332 - acc: 0.6920 - val_loss: 0.6902 - val_acc: 0.7169\nEpoch 12/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.7190 - acc: 0.7035 - val_loss: 0.6789 - val_acc: 0.7159\nEpoch 13/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.7173 - acc: 0.7034 - val_loss: 0.6661 - val_acc: 0.7183\nEpoch 14/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.7117 - acc: 0.7001 - val_loss: 0.6695 - val_acc: 0.7189\nEpoch 15/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.7057 - acc: 0.7032 - val_loss: 0.6718 - val_acc: 0.7173\nEpoch 16/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.7066 - acc: 0.7072 - val_loss: 0.6551 - val_acc: 0.7262\nEpoch 17/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6947 - acc: 0.7105 - val_loss: 0.6671 - val_acc: 0.7275\nEpoch 18/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.6889 - acc: 0.7116 - val_loss: 0.6384 - val_acc: 0.7345\nEpoch 19/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6868 - acc: 0.7117 - val_loss: 0.6365 - val_acc: 0.7272\nEpoch 20/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6843 - acc: 0.7154 - val_loss: 0.6492 - val_acc: 0.7292\nEpoch 21/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.6779 - acc: 0.7202 - val_loss: 0.6262 - val_acc: 0.7318\nEpoch 22/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6766 - acc: 0.7202 - val_loss: 0.6309 - val_acc: 0.7325\nEpoch 23/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6745 - acc: 0.7206 - val_loss: 0.6404 - val_acc: 0.7354\nEpoch 24/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6644 - acc: 0.7250 - val_loss: 0.6359 - val_acc: 0.7321\nEpoch 25/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6685 - acc: 0.7223 - val_loss: 0.6253 - val_acc: 0.7437\nEpoch 26/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6594 - acc: 0.7279 - val_loss: 0.6383 - val_acc: 0.7325\nEpoch 27/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.6574 - acc: 0.7238 - val_loss: 0.6297 - val_acc: 0.7480\nEpoch 28/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6543 - acc: 0.7289 - val_loss: 0.6313 - val_acc: 0.7364\nEpoch 29/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6559 - acc: 0.7270 - val_loss: 0.6202 - val_acc: 0.7424\nEpoch 30/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6472 - acc: 0.7328 - val_loss: 0.6188 - val_acc: 0.7411\nEpoch 31/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6439 - acc: 0.7318 - val_loss: 0.6255 - val_acc: 0.7371\nEpoch 32/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6484 - acc: 0.7298 - val_loss: 0.6033 - val_acc: 0.7467\nEpoch 33/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.6417 - acc: 0.7318 - val_loss: 0.6210 - val_acc: 0.7302\nEpoch 34/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.6469 - acc: 0.7287 - val_loss: 0.6080 - val_acc: 0.7474\nEpoch 35/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6356 - acc: 0.7365 - val_loss: 0.5973 - val_acc: 0.7483\nEpoch 36/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6376 - acc: 0.7334 - val_loss: 0.6128 - val_acc: 0.7414\nEpoch 37/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6367 - acc: 0.7335 - val_loss: 0.6354 - val_acc: 0.7255\nEpoch 38/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6323 - acc: 0.7369 - val_loss: 0.6038 - val_acc: 0.7546\nEpoch 39/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6270 - acc: 0.7355 - val_loss: 0.6058 - val_acc: 0.7431\nEpoch 40/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6259 - acc: 0.7388 - val_loss: 0.6060 - val_acc: 0.7576\nEpoch 41/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6213 - acc: 0.7411 - val_loss: 0.5886 - val_acc: 0.7550\nEpoch 42/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6231 - acc: 0.7414 - val_loss: 0.5753 - val_acc: 0.7556\nEpoch 43/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6165 - acc: 0.7439 - val_loss: 0.5780 - val_acc: 0.7606\nEpoch 44/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6220 - acc: 0.7368 - val_loss: 0.6028 - val_acc: 0.7437\nEpoch 45/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6116 - acc: 0.7470 - val_loss: 0.5715 - val_acc: 0.7626\nEpoch 46/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.6124 - acc: 0.7469 - val_loss: 0.5798 - val_acc: 0.7622\nEpoch 47/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6123 - acc: 0.7445 - val_loss: 0.5913 - val_acc: 0.7517\nEpoch 48/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6073 - acc: 0.7477 - val_loss: 0.5718 - val_acc: 0.7606\nEpoch 49/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6105 - acc: 0.7472 - val_loss: 0.5923 - val_acc: 0.7546\nEpoch 50/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6038 - acc: 0.7436 - val_loss: 0.5894 - val_acc: 0.7417\nEpoch 51/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.6097 - acc: 0.7445 - val_loss: 0.5873 - val_acc: 0.7556\nEpoch 52/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6112 - acc: 0.7467 - val_loss: 0.5837 - val_acc: 0.7553\nEpoch 53/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5992 - acc: 0.7482 - val_loss: 0.6016 - val_acc: 0.7440\nEpoch 54/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5982 - acc: 0.7483 - val_loss: 0.5744 - val_acc: 0.7589\nEpoch 55/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5929 - acc: 0.7511 - val_loss: 0.5824 - val_acc: 0.7576\nEpoch 56/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.6019 - acc: 0.7481 - val_loss: 0.5852 - val_acc: 0.7606\nEpoch 57/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5973 - acc: 0.7533 - val_loss: 0.5722 - val_acc: 0.7586\nEpoch 58/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5984 - acc: 0.7545 - val_loss: 0.5720 - val_acc: 0.7675\n","name":"stdout"},{"output_type":"stream","text":"Epoch 59/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5988 - acc: 0.7517 - val_loss: 0.5739 - val_acc: 0.7609\nEpoch 60/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5926 - acc: 0.7522 - val_loss: 0.5559 - val_acc: 0.7649\nEpoch 61/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5910 - acc: 0.7519 - val_loss: 0.5689 - val_acc: 0.7655\nEpoch 62/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5935 - acc: 0.7505 - val_loss: 0.5927 - val_acc: 0.7556\nEpoch 63/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5875 - acc: 0.7573 - val_loss: 0.5769 - val_acc: 0.7546\nEpoch 64/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.5875 - acc: 0.7588 - val_loss: 0.5542 - val_acc: 0.7708\nEpoch 65/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5869 - acc: 0.7569 - val_loss: 0.5590 - val_acc: 0.7626\nEpoch 66/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5881 - acc: 0.7558 - val_loss: 0.5622 - val_acc: 0.7682\nEpoch 67/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5840 - acc: 0.7589 - val_loss: 0.5843 - val_acc: 0.7553\nEpoch 68/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5832 - acc: 0.7588 - val_loss: 0.5577 - val_acc: 0.7616\nEpoch 69/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5799 - acc: 0.7611 - val_loss: 0.5455 - val_acc: 0.7748\nEpoch 70/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5797 - acc: 0.7639 - val_loss: 0.5603 - val_acc: 0.7679\nEpoch 71/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5825 - acc: 0.7595 - val_loss: 0.5610 - val_acc: 0.7563\nEpoch 72/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5785 - acc: 0.7583 - val_loss: 0.5769 - val_acc: 0.7606\nEpoch 73/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5837 - acc: 0.7592 - val_loss: 0.5648 - val_acc: 0.7593\nEpoch 74/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5828 - acc: 0.7604 - val_loss: 0.5635 - val_acc: 0.7672\nEpoch 75/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5734 - acc: 0.7665 - val_loss: 0.5574 - val_acc: 0.7669\nEpoch 76/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5752 - acc: 0.7623 - val_loss: 0.5425 - val_acc: 0.7794\nEpoch 77/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5714 - acc: 0.7618 - val_loss: 0.5441 - val_acc: 0.7808\nEpoch 78/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5742 - acc: 0.7618 - val_loss: 0.5475 - val_acc: 0.7646\nEpoch 79/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5783 - acc: 0.7638 - val_loss: 0.5549 - val_acc: 0.7698\nEpoch 80/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5662 - acc: 0.7653 - val_loss: 0.5476 - val_acc: 0.7768\nEpoch 81/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5714 - acc: 0.7654 - val_loss: 0.5515 - val_acc: 0.7718\nEpoch 82/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5693 - acc: 0.7641 - val_loss: 0.5634 - val_acc: 0.7679\nEpoch 83/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5614 - acc: 0.7688 - val_loss: 0.5402 - val_acc: 0.7798\nEpoch 84/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5648 - acc: 0.7672 - val_loss: 0.5313 - val_acc: 0.7844\nEpoch 85/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5600 - acc: 0.7672 - val_loss: 0.5700 - val_acc: 0.7748\nEpoch 86/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5687 - acc: 0.7651 - val_loss: 0.5545 - val_acc: 0.7708\nEpoch 87/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5657 - acc: 0.7674 - val_loss: 0.5427 - val_acc: 0.7781\nEpoch 88/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5565 - acc: 0.7724 - val_loss: 0.5531 - val_acc: 0.7765\nEpoch 89/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5600 - acc: 0.7688 - val_loss: 0.5449 - val_acc: 0.7705\nEpoch 90/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5549 - acc: 0.7706 - val_loss: 0.5409 - val_acc: 0.7748\nEpoch 91/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5623 - acc: 0.7672 - val_loss: 0.5502 - val_acc: 0.7768\nEpoch 92/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5555 - acc: 0.7753 - val_loss: 0.5412 - val_acc: 0.7841\nEpoch 93/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5579 - acc: 0.7698 - val_loss: 0.5395 - val_acc: 0.7781\nEpoch 94/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5545 - acc: 0.7716 - val_loss: 0.5268 - val_acc: 0.7884\nEpoch 95/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5534 - acc: 0.7704 - val_loss: 0.5314 - val_acc: 0.7857\nEpoch 96/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5612 - acc: 0.7686 - val_loss: 0.5343 - val_acc: 0.7804\nEpoch 97/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5491 - acc: 0.7752 - val_loss: 0.5361 - val_acc: 0.7817\nEpoch 98/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5522 - acc: 0.7740 - val_loss: 0.5453 - val_acc: 0.7758\nEpoch 99/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5489 - acc: 0.7740 - val_loss: 0.5339 - val_acc: 0.7791\nEpoch 100/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5499 - acc: 0.7684 - val_loss: 0.5355 - val_acc: 0.7781\nEpoch 101/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5476 - acc: 0.7752 - val_loss: 0.5301 - val_acc: 0.7860\nEpoch 102/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5428 - acc: 0.7737 - val_loss: 0.5435 - val_acc: 0.7669\nEpoch 103/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5518 - acc: 0.7717 - val_loss: 0.5499 - val_acc: 0.7735\nEpoch 104/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5548 - acc: 0.7729 - val_loss: 0.5447 - val_acc: 0.7712\nEpoch 105/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5457 - acc: 0.7722 - val_loss: 0.5388 - val_acc: 0.7817\nEpoch 106/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5409 - acc: 0.7755 - val_loss: 0.5335 - val_acc: 0.7897\nEpoch 107/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5454 - acc: 0.7767 - val_loss: 0.5384 - val_acc: 0.7771\nEpoch 108/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5472 - acc: 0.7740 - val_loss: 0.5474 - val_acc: 0.7708\nEpoch 109/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5408 - acc: 0.7787 - val_loss: 0.5235 - val_acc: 0.7864\nEpoch 110/250\n12096/12096 [==============================] - 1s 91us/sample - loss: 0.5407 - acc: 0.7783 - val_loss: 0.5341 - val_acc: 0.7857\nEpoch 111/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5456 - acc: 0.7733 - val_loss: 0.5226 - val_acc: 0.7860\nEpoch 112/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5415 - acc: 0.7804 - val_loss: 0.5359 - val_acc: 0.7827\nEpoch 113/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5425 - acc: 0.7746 - val_loss: 0.5186 - val_acc: 0.7870\nEpoch 114/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5426 - acc: 0.7762 - val_loss: 0.5270 - val_acc: 0.7907\nEpoch 115/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5366 - acc: 0.7797 - val_loss: 0.5466 - val_acc: 0.7814\nEpoch 116/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5411 - acc: 0.7780 - val_loss: 0.5167 - val_acc: 0.7870\n","name":"stdout"},{"output_type":"stream","text":"Epoch 117/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5409 - acc: 0.7795 - val_loss: 0.5302 - val_acc: 0.7864\nEpoch 118/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5410 - acc: 0.7759 - val_loss: 0.5326 - val_acc: 0.7827\nEpoch 119/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5428 - acc: 0.7771 - val_loss: 0.5228 - val_acc: 0.7903\nEpoch 120/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5459 - acc: 0.7758 - val_loss: 0.5371 - val_acc: 0.7765\nEpoch 121/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5388 - acc: 0.7765 - val_loss: 0.5315 - val_acc: 0.7778\nEpoch 122/250\n12096/12096 [==============================] - 1s 91us/sample - loss: 0.5291 - acc: 0.7822 - val_loss: 0.5253 - val_acc: 0.7847\nEpoch 123/250\n12096/12096 [==============================] - 1s 91us/sample - loss: 0.5336 - acc: 0.7815 - val_loss: 0.5214 - val_acc: 0.7884\nEpoch 124/250\n12096/12096 [==============================] - 1s 92us/sample - loss: 0.5336 - acc: 0.7812 - val_loss: 0.5405 - val_acc: 0.7811\nEpoch 125/250\n12096/12096 [==============================] - 1s 92us/sample - loss: 0.5323 - acc: 0.7773 - val_loss: 0.5289 - val_acc: 0.7890\nEpoch 126/250\n12096/12096 [==============================] - 1s 91us/sample - loss: 0.5281 - acc: 0.7847 - val_loss: 0.5242 - val_acc: 0.7851\nEpoch 127/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5282 - acc: 0.7816 - val_loss: 0.5226 - val_acc: 0.7917\nEpoch 128/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5333 - acc: 0.7824 - val_loss: 0.5148 - val_acc: 0.7940\nEpoch 129/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.5289 - acc: 0.7793 - val_loss: 0.5195 - val_acc: 0.7913\nEpoch 130/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5352 - acc: 0.7791 - val_loss: 0.5272 - val_acc: 0.7821\nEpoch 131/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5328 - acc: 0.7794 - val_loss: 0.5314 - val_acc: 0.7784\nEpoch 132/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5296 - acc: 0.7811 - val_loss: 0.5076 - val_acc: 0.8046\nEpoch 133/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5369 - acc: 0.7776 - val_loss: 0.5313 - val_acc: 0.7808\nEpoch 134/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5298 - acc: 0.7815 - val_loss: 0.5180 - val_acc: 0.7821\nEpoch 135/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5266 - acc: 0.7859 - val_loss: 0.5233 - val_acc: 0.7834\nEpoch 136/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5301 - acc: 0.7808 - val_loss: 0.5195 - val_acc: 0.7910\nEpoch 137/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5253 - acc: 0.7856 - val_loss: 0.5248 - val_acc: 0.7874\nEpoch 138/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5240 - acc: 0.7835 - val_loss: 0.5350 - val_acc: 0.7860\nEpoch 139/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5294 - acc: 0.7868 - val_loss: 0.5313 - val_acc: 0.7880\nEpoch 140/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5309 - acc: 0.7851 - val_loss: 0.5167 - val_acc: 0.7867\nEpoch 141/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5249 - acc: 0.7879 - val_loss: 0.5111 - val_acc: 0.7940\nEpoch 142/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5276 - acc: 0.7858 - val_loss: 0.5103 - val_acc: 0.7890\nEpoch 143/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5209 - acc: 0.7868 - val_loss: 0.5155 - val_acc: 0.7976\nEpoch 144/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5278 - acc: 0.7812 - val_loss: 0.5332 - val_acc: 0.7827\nEpoch 145/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5246 - acc: 0.7824 - val_loss: 0.5290 - val_acc: 0.7824\nEpoch 146/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5244 - acc: 0.7858 - val_loss: 0.5244 - val_acc: 0.7927\nEpoch 147/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5324 - acc: 0.7803 - val_loss: 0.5131 - val_acc: 0.7943\nEpoch 148/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5199 - acc: 0.7871 - val_loss: 0.5195 - val_acc: 0.7897\nEpoch 149/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5298 - acc: 0.7821 - val_loss: 0.5277 - val_acc: 0.7844\nEpoch 150/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5223 - acc: 0.7875 - val_loss: 0.5233 - val_acc: 0.7824\nEpoch 151/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5276 - acc: 0.7852 - val_loss: 0.5143 - val_acc: 0.7917\nEpoch 152/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5193 - acc: 0.7857 - val_loss: 0.5135 - val_acc: 0.7900\nEpoch 153/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5125 - acc: 0.7918 - val_loss: 0.5197 - val_acc: 0.7887\nEpoch 154/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5208 - acc: 0.7843 - val_loss: 0.5012 - val_acc: 0.8042\nEpoch 155/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5157 - acc: 0.7899 - val_loss: 0.5133 - val_acc: 0.7880\nEpoch 156/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5208 - acc: 0.7859 - val_loss: 0.5078 - val_acc: 0.7923\nEpoch 157/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5093 - acc: 0.7938 - val_loss: 0.5292 - val_acc: 0.7890\nEpoch 158/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5109 - acc: 0.7896 - val_loss: 0.5150 - val_acc: 0.7847\nEpoch 159/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5202 - acc: 0.7857 - val_loss: 0.5312 - val_acc: 0.7844\nEpoch 160/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5088 - acc: 0.7911 - val_loss: 0.5005 - val_acc: 0.7999\nEpoch 161/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5151 - acc: 0.7911 - val_loss: 0.5216 - val_acc: 0.7884\nEpoch 162/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5102 - acc: 0.7912 - val_loss: 0.5047 - val_acc: 0.7973\nEpoch 163/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5139 - acc: 0.7864 - val_loss: 0.5166 - val_acc: 0.7890\nEpoch 164/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5132 - acc: 0.7870 - val_loss: 0.5092 - val_acc: 0.7880\nEpoch 165/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5085 - acc: 0.7946 - val_loss: 0.5225 - val_acc: 0.7933\nEpoch 166/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5101 - acc: 0.7894 - val_loss: 0.5124 - val_acc: 0.7960\nEpoch 167/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5082 - acc: 0.7942 - val_loss: 0.5328 - val_acc: 0.7851\nEpoch 168/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5094 - acc: 0.7927 - val_loss: 0.5228 - val_acc: 0.7907\nEpoch 169/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5205 - acc: 0.7860 - val_loss: 0.5048 - val_acc: 0.7973\nEpoch 170/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5123 - acc: 0.7937 - val_loss: 0.5051 - val_acc: 0.8013\nEpoch 171/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5147 - acc: 0.7897 - val_loss: 0.5043 - val_acc: 0.7963\nEpoch 172/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5126 - acc: 0.7902 - val_loss: 0.5105 - val_acc: 0.7874\nEpoch 173/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5108 - acc: 0.7922 - val_loss: 0.5043 - val_acc: 0.7943\nEpoch 174/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5108 - acc: 0.7913 - val_loss: 0.5115 - val_acc: 0.7870\n","name":"stdout"},{"output_type":"stream","text":"Epoch 175/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5006 - acc: 0.7948 - val_loss: 0.5009 - val_acc: 0.8056\nEpoch 176/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.5144 - acc: 0.7935 - val_loss: 0.5113 - val_acc: 0.8036\nEpoch 177/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5102 - acc: 0.7860 - val_loss: 0.5207 - val_acc: 0.8019\nEpoch 178/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5108 - acc: 0.7924 - val_loss: 0.5074 - val_acc: 0.7943\nEpoch 179/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5096 - acc: 0.7932 - val_loss: 0.4980 - val_acc: 0.7999\nEpoch 180/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5043 - acc: 0.7917 - val_loss: 0.5093 - val_acc: 0.7983\nEpoch 181/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5043 - acc: 0.7954 - val_loss: 0.5082 - val_acc: 0.7966\nEpoch 182/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5119 - acc: 0.7915 - val_loss: 0.5425 - val_acc: 0.7768\nEpoch 183/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5125 - acc: 0.7933 - val_loss: 0.5107 - val_acc: 0.8036\nEpoch 184/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5101 - acc: 0.7900 - val_loss: 0.4916 - val_acc: 0.8065\nEpoch 185/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5011 - acc: 0.7962 - val_loss: 0.5008 - val_acc: 0.7937\nEpoch 186/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5070 - acc: 0.7964 - val_loss: 0.4908 - val_acc: 0.8032\nEpoch 187/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.4986 - acc: 0.7965 - val_loss: 0.5050 - val_acc: 0.7956\nEpoch 188/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5049 - acc: 0.7970 - val_loss: 0.5059 - val_acc: 0.7917\nEpoch 189/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5128 - acc: 0.7898 - val_loss: 0.5061 - val_acc: 0.7920\nEpoch 190/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5043 - acc: 0.7947 - val_loss: 0.5046 - val_acc: 0.7983\nEpoch 191/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5056 - acc: 0.7942 - val_loss: 0.5069 - val_acc: 0.7999\nEpoch 192/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5129 - acc: 0.7922 - val_loss: 0.4950 - val_acc: 0.7996\nEpoch 193/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.4958 - acc: 0.7954 - val_loss: 0.5106 - val_acc: 0.7854\nEpoch 194/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5045 - acc: 0.7968 - val_loss: 0.5038 - val_acc: 0.7963\nEpoch 195/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5070 - acc: 0.7931 - val_loss: 0.4978 - val_acc: 0.8016\nEpoch 196/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.5053 - acc: 0.7922 - val_loss: 0.5133 - val_acc: 0.7847\nEpoch 197/250\n12096/12096 [==============================] - 1s 86us/sample - loss: 0.4991 - acc: 0.7975 - val_loss: 0.4969 - val_acc: 0.8052\nEpoch 198/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5050 - acc: 0.7939 - val_loss: 0.5054 - val_acc: 0.7890\nEpoch 199/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5009 - acc: 0.7961 - val_loss: 0.5058 - val_acc: 0.7960\nEpoch 200/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5027 - acc: 0.7951 - val_loss: 0.5072 - val_acc: 0.7973\nEpoch 201/250\n12096/12096 [==============================] - 1s 91us/sample - loss: 0.5009 - acc: 0.7964 - val_loss: 0.4986 - val_acc: 0.7999\nEpoch 202/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5020 - acc: 0.7946 - val_loss: 0.5114 - val_acc: 0.7996\nEpoch 203/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.5024 - acc: 0.7932 - val_loss: 0.5130 - val_acc: 0.7910\nEpoch 204/250\n12096/12096 [==============================] - 1s 92us/sample - loss: 0.5030 - acc: 0.7971 - val_loss: 0.5103 - val_acc: 0.7937\nEpoch 205/250\n12096/12096 [==============================] - 1s 92us/sample - loss: 0.4960 - acc: 0.7977 - val_loss: 0.5071 - val_acc: 0.7930\nEpoch 206/250\n12096/12096 [==============================] - 1s 91us/sample - loss: 0.4992 - acc: 0.7972 - val_loss: 0.5069 - val_acc: 0.7903\nEpoch 207/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5055 - acc: 0.7959 - val_loss: 0.5204 - val_acc: 0.7880\nEpoch 208/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.4999 - acc: 0.7957 - val_loss: 0.5015 - val_acc: 0.7946\nEpoch 209/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4922 - acc: 0.7991 - val_loss: 0.5202 - val_acc: 0.7864\nEpoch 210/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4916 - acc: 0.7992 - val_loss: 0.5061 - val_acc: 0.8062\nEpoch 211/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5016 - acc: 0.7956 - val_loss: 0.5079 - val_acc: 0.7950\nEpoch 212/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4997 - acc: 0.7994 - val_loss: 0.5206 - val_acc: 0.7917\nEpoch 213/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4930 - acc: 0.7946 - val_loss: 0.4938 - val_acc: 0.8013\nEpoch 214/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.5062 - acc: 0.7921 - val_loss: 0.5001 - val_acc: 0.7960\nEpoch 215/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.4932 - acc: 0.7951 - val_loss: 0.5027 - val_acc: 0.7956\nEpoch 216/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.5022 - acc: 0.7951 - val_loss: 0.4998 - val_acc: 0.8062\nEpoch 217/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5014 - acc: 0.7993 - val_loss: 0.5051 - val_acc: 0.7956\nEpoch 218/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4966 - acc: 0.7938 - val_loss: 0.5138 - val_acc: 0.8039\nEpoch 219/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4986 - acc: 0.7934 - val_loss: 0.5157 - val_acc: 0.7927\nEpoch 220/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4978 - acc: 0.7975 - val_loss: 0.5097 - val_acc: 0.7917\nEpoch 221/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5040 - acc: 0.7928 - val_loss: 0.5227 - val_acc: 0.7956\nEpoch 222/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4929 - acc: 0.7974 - val_loss: 0.5089 - val_acc: 0.7979\nEpoch 223/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5031 - acc: 0.7937 - val_loss: 0.5026 - val_acc: 0.7986\nEpoch 224/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4969 - acc: 0.7960 - val_loss: 0.5085 - val_acc: 0.7983\nEpoch 225/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4924 - acc: 0.7960 - val_loss: 0.5055 - val_acc: 0.7999\nEpoch 226/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4993 - acc: 0.7939 - val_loss: 0.4956 - val_acc: 0.8036\nEpoch 227/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4933 - acc: 0.8019 - val_loss: 0.5082 - val_acc: 0.7983\nEpoch 228/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4974 - acc: 0.7951 - val_loss: 0.4999 - val_acc: 0.8046\nEpoch 229/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5029 - acc: 0.7969 - val_loss: 0.5017 - val_acc: 0.8013\nEpoch 230/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4919 - acc: 0.8010 - val_loss: 0.4971 - val_acc: 0.8036\nEpoch 231/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4879 - acc: 0.7982 - val_loss: 0.4866 - val_acc: 0.8062\nEpoch 232/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4902 - acc: 0.7989 - val_loss: 0.4932 - val_acc: 0.8009\n","name":"stdout"},{"output_type":"stream","text":"Epoch 233/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5031 - acc: 0.7937 - val_loss: 0.4888 - val_acc: 0.8062\nEpoch 234/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4992 - acc: 0.7944 - val_loss: 0.5077 - val_acc: 0.7900\nEpoch 235/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.4864 - acc: 0.8018 - val_loss: 0.4846 - val_acc: 0.8069\nEpoch 236/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4936 - acc: 0.7996 - val_loss: 0.5020 - val_acc: 0.7943\nEpoch 237/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4931 - acc: 0.7984 - val_loss: 0.5000 - val_acc: 0.7996\nEpoch 238/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.4896 - acc: 0.8032 - val_loss: 0.5191 - val_acc: 0.7923\nEpoch 239/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.5004 - acc: 0.7966 - val_loss: 0.4924 - val_acc: 0.8069\nEpoch 240/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.5002 - acc: 0.7964 - val_loss: 0.5160 - val_acc: 0.7930\nEpoch 241/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4926 - acc: 0.7993 - val_loss: 0.4849 - val_acc: 0.8089\nEpoch 242/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.4931 - acc: 0.8017 - val_loss: 0.5018 - val_acc: 0.7986\nEpoch 243/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4832 - acc: 0.8029 - val_loss: 0.4995 - val_acc: 0.7986\nEpoch 244/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4910 - acc: 0.7951 - val_loss: 0.5163 - val_acc: 0.7973\nEpoch 245/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4909 - acc: 0.8018 - val_loss: 0.4975 - val_acc: 0.8046\nEpoch 246/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.4882 - acc: 0.8022 - val_loss: 0.4865 - val_acc: 0.8069\nEpoch 247/250\n12096/12096 [==============================] - 1s 89us/sample - loss: 0.4879 - acc: 0.8016 - val_loss: 0.4987 - val_acc: 0.7993\nEpoch 248/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4911 - acc: 0.7999 - val_loss: 0.4989 - val_acc: 0.8009\nEpoch 249/250\n12096/12096 [==============================] - 1s 87us/sample - loss: 0.4891 - acc: 0.7929 - val_loss: 0.4969 - val_acc: 0.8072\nEpoch 250/250\n12096/12096 [==============================] - 1s 88us/sample - loss: 0.4901 - acc: 0.8008 - val_loss: 0.4953 - val_acc: 0.7953\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f4767714b00>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict!!\ny_predict = model.predict(x_predict)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n\tprint(y_predict[i], np.argmax(y_predict[i])+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save predictions to a file for submission\n#argmax give us the highest probable label\n# we add one to the predictions to scale from 0..6 to 1..7\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict.argmax(axis=1)+1})\noutput.to_csv('submission.csv', index=False)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}