{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first experiment in this competition. Whereas XGBoost is highly recommended I rather tried to see how far I can go with an NN (using Keras).\n\nThis is the basic model and with 250 epochs has an accuracy of 80% (really poor).\n\nI'll continue for a few days researching how much I can optimize this model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\nprint(os.listdir(\"../working/\"))\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":17,"outputs":[{"output_type":"stream","text":"['learn-together']\n['__notebook_source__.ipynb', 'submission.csv', '.ipynb_checkpoints']\n/kaggle/input/learn-together/sample_submission.csv\n/kaggle/input/learn-together/test.csv\n/kaggle/input/learn-together/train.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load data, I had an issue with the data and hacked the data into the kernel\n#dftrain=pd.read_csv('/kaggle/input/train.csv')\n#dftest=pd.read_csv('/kaggle/input/test.csv')\ndftrain=pd.read_csv('/kaggle/input/learn-together/train.csv')\ndftest=pd.read_csv('/kaggle/input/learn-together/test.csv')","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#file shape\nprint(dftrain.head())\nprint(dftrain.shape[0])\nprint(dftest.head())\nprint(dftest.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features types\nprint(dftrain.dtypes)\nprint(dftest.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate files - nans per feature\nprint(dftrain.isnull().sum(axis = 0))\nprint(dftest.isnull().sum(axis = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate files - no rows with all zeros\nprint(dftrain[dftrain.drop(['Id','Cover_Type'], axis=1).eq(0).all(1)].empty)\nprint(dftest[dftest.drop('Id', axis=1).eq(0).all(1)].empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#force all types to float\nx = x.astype(float)\nx_predict = x_predict.astype(float)","execution_count":20,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize features\n#in the future it can be done more elegantly, for now just using the max min values of the data that we know\n#x['Elevation']=(x['Elevation']-x['Elevation'].min())/(x['Elevation'].max()-x['Elevation'].min())                             \nx['Elevation']=(x['Elevation']-1859)/(3858-1859)                             \nx['Aspect']=x['Aspect']/360                      \nx['Slope']=x['Slope']/66                      \nx['Horizontal_Distance_To_Hydrology']=x['Horizontal_Distance_To_Hydrology']/1397                      \nx['Vertical_Distance_To_Hydrology']=(x['Vertical_Distance_To_Hydrology']+173)/(601+173)                             \nx['Horizontal_Distance_To_Roadways']=x['Horizontal_Distance_To_Roadways']/7117                      \nx['Hillshade_9am']=x['Hillshade_9am']/254                      \nx['Hillshade_Noon']=x['Hillshade_Noon']/254                      \nx['Hillshade_3pm']=x['Hillshade_3pm']/254                      \nx['Horizontal_Distance_To_Fire_Points']=x['Horizontal_Distance_To_Fire_Points']/67173                      \n                                \nx_predict['Elevation']=(x_predict['Elevation']-1859)/(3858-1859)                             \nx_predict['Aspect']=x_predict['Aspect']/360                      \nx_predict['Slope']=x_predict['Slope']/66                      \nx_predict['Horizontal_Distance_To_Hydrology']=x_predict['Horizontal_Distance_To_Hydrology']/1397                      \nx_predict['Vertical_Distance_To_Hydrology']=(x_predict['Vertical_Distance_To_Hydrology']+173)/(601+173)                             \nx_predict['Horizontal_Distance_To_Roadways']=x_predict['Horizontal_Distance_To_Roadways']/7117                      \nx_predict['Hillshade_9am']=x_predict['Hillshade_9am']/254                      \nx_predict['Hillshade_Noon']=x_predict['Hillshade_Noon']/254                      \nx_predict['Hillshade_3pm']=x_predict['Hillshade_3pm']/254                      \nx_predict['Horizontal_Distance_To_Fire_Points']=x_predict['Horizontal_Distance_To_Fire_Points']/67173                      ","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate data - no rows with all zeros\n#x.index[x.eq(0).all(1)]\nprint(x[x.eq(0).all(1)].empty)\nprint(x_predict[x_predict.eq(0).all(1)].empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the label to One Hot Encoding\n#to_categorical requires 0..6 instead of 1..7\ny -=1\ny = y.to_numpy()\n\nnum_classes = 7\n\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y, num_classes)","execution_count":22,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the features dataframes to numpy arrays\nx = x.to_numpy()\nx_predict = x_predict.to_numpy()","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split in train (80%) and test (20%) sets \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here is the NN model\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\nnum_features = 54\n\nmodel = Sequential()\nmodel.add(Dense(units=num_features, activation='relu', kernel_initializer='normal', input_dim=num_features))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*10, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*10, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*10, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*10, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n#              optimizer='Adadelta',\n              optimizer='Adam',\n              metrics=['accuracy'])","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=250)","execution_count":26,"outputs":[{"output_type":"stream","text":"Train on 12096 samples, validate on 3024 samples\nEpoch 1/250\n12096/12096 [==============================] - 4s 314us/sample - loss: 1.0548 - acc: 0.5422 - val_loss: 0.8657 - val_acc: 0.6329\nEpoch 2/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.8762 - acc: 0.6323 - val_loss: 0.8305 - val_acc: 0.6594\nEpoch 3/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.8029 - acc: 0.6648 - val_loss: 0.7871 - val_acc: 0.6587\nEpoch 4/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.7697 - acc: 0.6792 - val_loss: 0.7076 - val_acc: 0.7004\nEpoch 5/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.7486 - acc: 0.6822 - val_loss: 0.6858 - val_acc: 0.7199\nEpoch 6/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.7262 - acc: 0.6939 - val_loss: 0.6559 - val_acc: 0.7189\nEpoch 7/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.7132 - acc: 0.7051 - val_loss: 0.6691 - val_acc: 0.7169\nEpoch 8/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.6984 - acc: 0.7089 - val_loss: 0.6717 - val_acc: 0.7133\nEpoch 9/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.6846 - acc: 0.7125 - val_loss: 0.6956 - val_acc: 0.7060\nEpoch 10/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.6843 - acc: 0.7092 - val_loss: 0.6305 - val_acc: 0.7401\nEpoch 11/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.6730 - acc: 0.7194 - val_loss: 0.6396 - val_acc: 0.7262\nEpoch 12/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.6690 - acc: 0.7159 - val_loss: 0.6216 - val_acc: 0.7477\nEpoch 13/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.6527 - acc: 0.7251 - val_loss: 0.6173 - val_acc: 0.7434\nEpoch 14/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.6560 - acc: 0.7303 - val_loss: 0.6108 - val_acc: 0.7477\nEpoch 15/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.6403 - acc: 0.7335 - val_loss: 0.6375 - val_acc: 0.7381\nEpoch 16/250\n12096/12096 [==============================] - 3s 287us/sample - loss: 0.6484 - acc: 0.7314 - val_loss: 0.6012 - val_acc: 0.7470\nEpoch 17/250\n12096/12096 [==============================] - 4s 303us/sample - loss: 0.6344 - acc: 0.7365 - val_loss: 0.6096 - val_acc: 0.7407\nEpoch 18/250\n12096/12096 [==============================] - 4s 297us/sample - loss: 0.6334 - acc: 0.7364 - val_loss: 0.6023 - val_acc: 0.7450\nEpoch 19/250\n12096/12096 [==============================] - 3s 278us/sample - loss: 0.6286 - acc: 0.7407 - val_loss: 0.6056 - val_acc: 0.7503\nEpoch 20/250\n12096/12096 [==============================] - 3s 277us/sample - loss: 0.6206 - acc: 0.7454 - val_loss: 0.5762 - val_acc: 0.7483\nEpoch 21/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.6166 - acc: 0.7450 - val_loss: 0.6210 - val_acc: 0.7490\nEpoch 22/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.6035 - acc: 0.7481 - val_loss: 0.6081 - val_acc: 0.7470\nEpoch 23/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.6103 - acc: 0.7455 - val_loss: 0.5980 - val_acc: 0.7530\nEpoch 24/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.6145 - acc: 0.7442 - val_loss: 0.6000 - val_acc: 0.7589\nEpoch 25/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.5978 - acc: 0.7531 - val_loss: 0.5762 - val_acc: 0.7655\nEpoch 26/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.5899 - acc: 0.7545 - val_loss: 0.5722 - val_acc: 0.7569\nEpoch 27/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.5935 - acc: 0.7559 - val_loss: 0.5983 - val_acc: 0.7579\nEpoch 28/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.5859 - acc: 0.7559 - val_loss: 0.6012 - val_acc: 0.7457\nEpoch 29/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.5810 - acc: 0.7603 - val_loss: 0.5673 - val_acc: 0.7636\nEpoch 30/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.5813 - acc: 0.7547 - val_loss: 0.5828 - val_acc: 0.7725\nEpoch 31/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.5761 - acc: 0.7599 - val_loss: 0.5779 - val_acc: 0.7692\nEpoch 32/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.5700 - acc: 0.7631 - val_loss: 0.5832 - val_acc: 0.7655\nEpoch 33/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.5800 - acc: 0.7616 - val_loss: 0.5592 - val_acc: 0.7735\nEpoch 34/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.5669 - acc: 0.7630 - val_loss: 0.5626 - val_acc: 0.7808\nEpoch 35/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.5688 - acc: 0.7617 - val_loss: 0.5806 - val_acc: 0.7665\nEpoch 36/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.5491 - acc: 0.7710 - val_loss: 0.5588 - val_acc: 0.7755\nEpoch 37/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.5576 - acc: 0.7709 - val_loss: 0.5626 - val_acc: 0.7794\nEpoch 38/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.5592 - acc: 0.7699 - val_loss: 0.5464 - val_acc: 0.7811\nEpoch 39/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.5515 - acc: 0.7741 - val_loss: 0.5546 - val_acc: 0.7824\nEpoch 40/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.5449 - acc: 0.7717 - val_loss: 0.5508 - val_acc: 0.7765\nEpoch 41/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.5491 - acc: 0.7693 - val_loss: 0.5703 - val_acc: 0.7774\nEpoch 42/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.5425 - acc: 0.7755 - val_loss: 0.5625 - val_acc: 0.7844\nEpoch 43/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.5413 - acc: 0.7755 - val_loss: 0.5448 - val_acc: 0.7887\nEpoch 44/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.5411 - acc: 0.7770 - val_loss: 0.5522 - val_acc: 0.7755\nEpoch 45/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.5423 - acc: 0.7742 - val_loss: 0.5326 - val_acc: 0.7940\nEpoch 46/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.5417 - acc: 0.7773 - val_loss: 0.5326 - val_acc: 0.7960\nEpoch 47/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.5335 - acc: 0.7797 - val_loss: 0.5491 - val_acc: 0.7811\nEpoch 48/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.5278 - acc: 0.7866 - val_loss: 0.5545 - val_acc: 0.7808\nEpoch 49/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.5325 - acc: 0.7801 - val_loss: 0.5335 - val_acc: 0.7851\nEpoch 50/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.5236 - acc: 0.7831 - val_loss: 0.5303 - val_acc: 0.7930\nEpoch 51/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.5173 - acc: 0.7851 - val_loss: 0.5427 - val_acc: 0.7887\nEpoch 52/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.5272 - acc: 0.7841 - val_loss: 0.5152 - val_acc: 0.7979\nEpoch 53/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.5199 - acc: 0.7817 - val_loss: 0.5284 - val_acc: 0.7913\nEpoch 54/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.5243 - acc: 0.7815 - val_loss: 0.5159 - val_acc: 0.8016\nEpoch 55/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.5177 - acc: 0.7863 - val_loss: 0.5377 - val_acc: 0.7946\nEpoch 56/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.5180 - acc: 0.7841 - val_loss: 0.5255 - val_acc: 0.7960\nEpoch 57/250\n12096/12096 [==============================] - 3s 265us/sample - loss: 0.5176 - acc: 0.7867 - val_loss: 0.5328 - val_acc: 0.7887\nEpoch 58/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 3s 267us/sample - loss: 0.5156 - acc: 0.7856 - val_loss: 0.5219 - val_acc: 0.7953\nEpoch 59/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.5168 - acc: 0.7873 - val_loss: 0.5128 - val_acc: 0.8039\nEpoch 60/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.5037 - acc: 0.7917 - val_loss: 0.5258 - val_acc: 0.7907\nEpoch 61/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.5097 - acc: 0.7873 - val_loss: 0.5159 - val_acc: 0.7993\nEpoch 62/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.5005 - acc: 0.7890 - val_loss: 0.5250 - val_acc: 0.7996\nEpoch 63/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.5028 - acc: 0.7937 - val_loss: 0.5270 - val_acc: 0.7927\nEpoch 64/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4990 - acc: 0.7926 - val_loss: 0.5081 - val_acc: 0.8036\nEpoch 65/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4928 - acc: 0.7943 - val_loss: 0.5194 - val_acc: 0.7930\nEpoch 66/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4998 - acc: 0.7937 - val_loss: 0.5035 - val_acc: 0.8069\nEpoch 67/250\n12096/12096 [==============================] - 3s 264us/sample - loss: 0.4935 - acc: 0.8000 - val_loss: 0.4952 - val_acc: 0.8151\nEpoch 68/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4966 - acc: 0.8004 - val_loss: 0.5243 - val_acc: 0.7940\nEpoch 69/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4910 - acc: 0.7981 - val_loss: 0.5066 - val_acc: 0.7983\nEpoch 70/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4902 - acc: 0.7987 - val_loss: 0.5112 - val_acc: 0.8042\nEpoch 71/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4879 - acc: 0.8024 - val_loss: 0.5012 - val_acc: 0.8032\nEpoch 72/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4875 - acc: 0.7988 - val_loss: 0.5067 - val_acc: 0.8072\nEpoch 73/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4848 - acc: 0.7994 - val_loss: 0.5101 - val_acc: 0.8079\nEpoch 74/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4852 - acc: 0.8027 - val_loss: 0.5025 - val_acc: 0.8075\nEpoch 75/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4780 - acc: 0.8004 - val_loss: 0.5026 - val_acc: 0.8075\nEpoch 76/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4788 - acc: 0.8040 - val_loss: 0.5007 - val_acc: 0.8115\nEpoch 77/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4826 - acc: 0.8073 - val_loss: 0.5103 - val_acc: 0.8099\nEpoch 78/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4773 - acc: 0.8021 - val_loss: 0.5012 - val_acc: 0.8046\nEpoch 79/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.4816 - acc: 0.8029 - val_loss: 0.4996 - val_acc: 0.8125\nEpoch 80/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4844 - acc: 0.7997 - val_loss: 0.5045 - val_acc: 0.7986\nEpoch 81/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4737 - acc: 0.8061 - val_loss: 0.4856 - val_acc: 0.8108\nEpoch 82/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4827 - acc: 0.8038 - val_loss: 0.4920 - val_acc: 0.8158\nEpoch 83/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4781 - acc: 0.8042 - val_loss: 0.4959 - val_acc: 0.8138\nEpoch 84/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4650 - acc: 0.8132 - val_loss: 0.4787 - val_acc: 0.8208\nEpoch 85/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4710 - acc: 0.8114 - val_loss: 0.4999 - val_acc: 0.8019\nEpoch 86/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4671 - acc: 0.8053 - val_loss: 0.5135 - val_acc: 0.8009\nEpoch 87/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4789 - acc: 0.8099 - val_loss: 0.4799 - val_acc: 0.8168\nEpoch 88/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4700 - acc: 0.8080 - val_loss: 0.4979 - val_acc: 0.8092\nEpoch 89/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4650 - acc: 0.8076 - val_loss: 0.5301 - val_acc: 0.8022\nEpoch 90/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.4708 - acc: 0.8076 - val_loss: 0.4989 - val_acc: 0.8072\nEpoch 91/250\n12096/12096 [==============================] - 3s 282us/sample - loss: 0.4519 - acc: 0.8127 - val_loss: 0.5014 - val_acc: 0.8046\nEpoch 92/250\n12096/12096 [==============================] - 4s 300us/sample - loss: 0.4661 - acc: 0.8104 - val_loss: 0.4889 - val_acc: 0.8181\nEpoch 93/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.4630 - acc: 0.8080 - val_loss: 0.5046 - val_acc: 0.8082\nEpoch 94/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4705 - acc: 0.8085 - val_loss: 0.4897 - val_acc: 0.8115\nEpoch 95/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4648 - acc: 0.8089 - val_loss: 0.4805 - val_acc: 0.8194\nEpoch 96/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4566 - acc: 0.8104 - val_loss: 0.5124 - val_acc: 0.8069\nEpoch 97/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4590 - acc: 0.8151 - val_loss: 0.4886 - val_acc: 0.8128\nEpoch 98/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4593 - acc: 0.8096 - val_loss: 0.5300 - val_acc: 0.7986\nEpoch 99/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4542 - acc: 0.8171 - val_loss: 0.4952 - val_acc: 0.8082\nEpoch 100/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4582 - acc: 0.8128 - val_loss: 0.4837 - val_acc: 0.8151\nEpoch 101/250\n12096/12096 [==============================] - 3s 265us/sample - loss: 0.4544 - acc: 0.8140 - val_loss: 0.4975 - val_acc: 0.8161\nEpoch 102/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4601 - acc: 0.8123 - val_loss: 0.4999 - val_acc: 0.8072\nEpoch 103/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4497 - acc: 0.8161 - val_loss: 0.4782 - val_acc: 0.8208\nEpoch 104/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4510 - acc: 0.8146 - val_loss: 0.4917 - val_acc: 0.8069\nEpoch 105/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.4463 - acc: 0.8186 - val_loss: 0.4763 - val_acc: 0.8254\nEpoch 106/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4535 - acc: 0.8168 - val_loss: 0.4952 - val_acc: 0.8102\nEpoch 107/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4492 - acc: 0.8180 - val_loss: 0.4845 - val_acc: 0.8079\nEpoch 108/250\n12096/12096 [==============================] - 3s 264us/sample - loss: 0.4487 - acc: 0.8166 - val_loss: 0.4832 - val_acc: 0.8204\nEpoch 109/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4483 - acc: 0.8158 - val_loss: 0.4913 - val_acc: 0.8128\nEpoch 110/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4355 - acc: 0.8221 - val_loss: 0.5067 - val_acc: 0.8165\nEpoch 111/250\n12096/12096 [==============================] - 3s 288us/sample - loss: 0.4588 - acc: 0.8140 - val_loss: 0.5134 - val_acc: 0.8039\nEpoch 112/250\n12096/12096 [==============================] - 4s 294us/sample - loss: 0.4407 - acc: 0.8172 - val_loss: 0.4926 - val_acc: 0.8056\nEpoch 113/250\n12096/12096 [==============================] - 4s 295us/sample - loss: 0.4433 - acc: 0.8198 - val_loss: 0.4733 - val_acc: 0.8244\nEpoch 114/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4364 - acc: 0.8228 - val_loss: 0.4897 - val_acc: 0.8161\nEpoch 115/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 3s 266us/sample - loss: 0.4520 - acc: 0.8131 - val_loss: 0.4830 - val_acc: 0.8171\nEpoch 116/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4397 - acc: 0.8201 - val_loss: 0.5139 - val_acc: 0.8155\nEpoch 117/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4453 - acc: 0.8182 - val_loss: 0.5060 - val_acc: 0.8072\nEpoch 118/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4383 - acc: 0.8179 - val_loss: 0.4788 - val_acc: 0.8221\nEpoch 119/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4429 - acc: 0.8211 - val_loss: 0.4867 - val_acc: 0.8138\nEpoch 120/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4415 - acc: 0.8182 - val_loss: 0.5038 - val_acc: 0.8191\nEpoch 121/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4541 - acc: 0.8146 - val_loss: 0.5040 - val_acc: 0.8013\nEpoch 122/250\n12096/12096 [==============================] - 3s 265us/sample - loss: 0.4434 - acc: 0.8223 - val_loss: 0.5077 - val_acc: 0.8085\nEpoch 123/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4414 - acc: 0.8220 - val_loss: 0.4972 - val_acc: 0.8026\nEpoch 124/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.4338 - acc: 0.8229 - val_loss: 0.5013 - val_acc: 0.8099\nEpoch 125/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4267 - acc: 0.8275 - val_loss: 0.4937 - val_acc: 0.8158\nEpoch 126/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4378 - acc: 0.8205 - val_loss: 0.4864 - val_acc: 0.8105\nEpoch 127/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4363 - acc: 0.8223 - val_loss: 0.4975 - val_acc: 0.8049\nEpoch 128/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4374 - acc: 0.8228 - val_loss: 0.5230 - val_acc: 0.7937\nEpoch 129/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4270 - acc: 0.8275 - val_loss: 0.5019 - val_acc: 0.7979\nEpoch 130/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4344 - acc: 0.8234 - val_loss: 0.4957 - val_acc: 0.8198\nEpoch 131/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4387 - acc: 0.8232 - val_loss: 0.4759 - val_acc: 0.8201\nEpoch 132/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4254 - acc: 0.8245 - val_loss: 0.4964 - val_acc: 0.8148\nEpoch 133/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4309 - acc: 0.8232 - val_loss: 0.4796 - val_acc: 0.8257\nEpoch 134/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4360 - acc: 0.8212 - val_loss: 0.4791 - val_acc: 0.8251\nEpoch 135/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4271 - acc: 0.8261 - val_loss: 0.4981 - val_acc: 0.8175\nEpoch 136/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.4375 - acc: 0.8250 - val_loss: 0.5006 - val_acc: 0.8112\nEpoch 137/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4243 - acc: 0.8290 - val_loss: 0.4780 - val_acc: 0.8099\nEpoch 138/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4270 - acc: 0.8266 - val_loss: 0.4830 - val_acc: 0.8191\nEpoch 139/250\n12096/12096 [==============================] - 3s 264us/sample - loss: 0.4328 - acc: 0.8251 - val_loss: 0.4890 - val_acc: 0.8191\nEpoch 140/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4295 - acc: 0.8247 - val_loss: 0.4839 - val_acc: 0.8218\nEpoch 141/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4292 - acc: 0.8247 - val_loss: 0.4869 - val_acc: 0.8198\nEpoch 142/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4206 - acc: 0.8304 - val_loss: 0.4888 - val_acc: 0.8244\nEpoch 143/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4227 - acc: 0.8273 - val_loss: 0.4841 - val_acc: 0.8181\nEpoch 144/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4245 - acc: 0.8290 - val_loss: 0.4927 - val_acc: 0.8115\nEpoch 145/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4252 - acc: 0.8262 - val_loss: 0.4951 - val_acc: 0.8185\nEpoch 146/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4304 - acc: 0.8273 - val_loss: 0.4897 - val_acc: 0.8125\nEpoch 147/250\n12096/12096 [==============================] - 3s 265us/sample - loss: 0.4299 - acc: 0.8213 - val_loss: 0.4891 - val_acc: 0.8171\nEpoch 148/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4238 - acc: 0.8286 - val_loss: 0.4942 - val_acc: 0.8158\nEpoch 149/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4243 - acc: 0.8294 - val_loss: 0.4877 - val_acc: 0.8201\nEpoch 150/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4273 - acc: 0.8278 - val_loss: 0.4785 - val_acc: 0.8158\nEpoch 151/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4243 - acc: 0.8278 - val_loss: 0.4817 - val_acc: 0.8277\nEpoch 152/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4277 - acc: 0.8254 - val_loss: 0.4808 - val_acc: 0.8218\nEpoch 153/250\n12096/12096 [==============================] - 3s 268us/sample - loss: 0.4167 - acc: 0.8280 - val_loss: 0.4835 - val_acc: 0.8151\nEpoch 154/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4177 - acc: 0.8302 - val_loss: 0.4859 - val_acc: 0.8224\nEpoch 155/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4199 - acc: 0.8316 - val_loss: 0.4939 - val_acc: 0.8161\nEpoch 156/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4110 - acc: 0.8326 - val_loss: 0.4859 - val_acc: 0.8171\nEpoch 157/250\n12096/12096 [==============================] - 3s 265us/sample - loss: 0.4133 - acc: 0.8354 - val_loss: 0.4879 - val_acc: 0.8148\nEpoch 158/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4136 - acc: 0.8333 - val_loss: 0.4882 - val_acc: 0.8171\nEpoch 159/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.4169 - acc: 0.8282 - val_loss: 0.5013 - val_acc: 0.8145\nEpoch 160/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4204 - acc: 0.8296 - val_loss: 0.4804 - val_acc: 0.8264\nEpoch 161/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4112 - acc: 0.8328 - val_loss: 0.4792 - val_acc: 0.8231\nEpoch 162/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4158 - acc: 0.8271 - val_loss: 0.4776 - val_acc: 0.8185\nEpoch 163/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4100 - acc: 0.8360 - val_loss: 0.5061 - val_acc: 0.8155\nEpoch 164/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4359 - acc: 0.8194 - val_loss: 0.5013 - val_acc: 0.8132\nEpoch 165/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.4267 - acc: 0.8273 - val_loss: 0.4826 - val_acc: 0.8204\nEpoch 166/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.4155 - acc: 0.8300 - val_loss: 0.4760 - val_acc: 0.8251\nEpoch 167/250\n12096/12096 [==============================] - 3s 277us/sample - loss: 0.4149 - acc: 0.8309 - val_loss: 0.4765 - val_acc: 0.8257\nEpoch 168/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.4114 - acc: 0.8327 - val_loss: 0.4732 - val_acc: 0.8244\nEpoch 169/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.4024 - acc: 0.8341 - val_loss: 0.4741 - val_acc: 0.8234\nEpoch 170/250\n12096/12096 [==============================] - 3s 278us/sample - loss: 0.4179 - acc: 0.8308 - val_loss: 0.4769 - val_acc: 0.8237\nEpoch 171/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.4036 - acc: 0.8359 - val_loss: 0.4948 - val_acc: 0.8194\nEpoch 172/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 3s 275us/sample - loss: 0.4096 - acc: 0.8366 - val_loss: 0.4836 - val_acc: 0.8284\nEpoch 173/250\n12096/12096 [==============================] - 3s 278us/sample - loss: 0.4182 - acc: 0.8333 - val_loss: 0.5057 - val_acc: 0.8128\nEpoch 174/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.4059 - acc: 0.8346 - val_loss: 0.4829 - val_acc: 0.8280\nEpoch 175/250\n12096/12096 [==============================] - 3s 277us/sample - loss: 0.4088 - acc: 0.8377 - val_loss: 0.5036 - val_acc: 0.8181\nEpoch 176/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4029 - acc: 0.8387 - val_loss: 0.4983 - val_acc: 0.8201\nEpoch 177/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.4054 - acc: 0.8379 - val_loss: 0.4852 - val_acc: 0.8251\nEpoch 178/250\n12096/12096 [==============================] - 3s 281us/sample - loss: 0.4138 - acc: 0.8349 - val_loss: 0.4904 - val_acc: 0.8161\nEpoch 179/250\n12096/12096 [==============================] - 3s 280us/sample - loss: 0.4140 - acc: 0.8343 - val_loss: 0.5140 - val_acc: 0.7986\nEpoch 180/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.4184 - acc: 0.8317 - val_loss: 0.4769 - val_acc: 0.8218\nEpoch 181/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4042 - acc: 0.8375 - val_loss: 0.4818 - val_acc: 0.8264\nEpoch 182/250\n12096/12096 [==============================] - 3s 277us/sample - loss: 0.4135 - acc: 0.8328 - val_loss: 0.4884 - val_acc: 0.8135\nEpoch 183/250\n12096/12096 [==============================] - 3s 282us/sample - loss: 0.4055 - acc: 0.8338 - val_loss: 0.4931 - val_acc: 0.8218\nEpoch 184/250\n12096/12096 [==============================] - 4s 308us/sample - loss: 0.4032 - acc: 0.8406 - val_loss: 0.4689 - val_acc: 0.8271\nEpoch 185/250\n12096/12096 [==============================] - 3s 282us/sample - loss: 0.4185 - acc: 0.8367 - val_loss: 0.4969 - val_acc: 0.8171\nEpoch 186/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.4112 - acc: 0.8326 - val_loss: 0.4815 - val_acc: 0.8175\nEpoch 187/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4153 - acc: 0.8289 - val_loss: 0.4749 - val_acc: 0.8244\nEpoch 188/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.4030 - acc: 0.8374 - val_loss: 0.4919 - val_acc: 0.8254\nEpoch 189/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.3972 - acc: 0.8384 - val_loss: 0.4951 - val_acc: 0.8155\nEpoch 190/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4066 - acc: 0.8364 - val_loss: 0.4837 - val_acc: 0.8241\nEpoch 191/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.4024 - acc: 0.8361 - val_loss: 0.4984 - val_acc: 0.8049\nEpoch 192/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.4003 - acc: 0.8412 - val_loss: 0.4830 - val_acc: 0.8241\nEpoch 193/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4035 - acc: 0.8362 - val_loss: 0.4965 - val_acc: 0.8191\nEpoch 194/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4045 - acc: 0.8347 - val_loss: 0.4878 - val_acc: 0.8201\nEpoch 195/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.4043 - acc: 0.8380 - val_loss: 0.4867 - val_acc: 0.8251\nEpoch 196/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.4079 - acc: 0.8415 - val_loss: 0.4957 - val_acc: 0.8201\nEpoch 197/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4079 - acc: 0.8349 - val_loss: 0.4866 - val_acc: 0.8284\nEpoch 198/250\n12096/12096 [==============================] - 3s 277us/sample - loss: 0.3983 - acc: 0.8375 - val_loss: 0.4818 - val_acc: 0.8218\nEpoch 199/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.3924 - acc: 0.8441 - val_loss: 0.5001 - val_acc: 0.8201\nEpoch 200/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.3935 - acc: 0.8409 - val_loss: 0.4931 - val_acc: 0.8115\nEpoch 201/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.4076 - acc: 0.8391 - val_loss: 0.4994 - val_acc: 0.8135\nEpoch 202/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4020 - acc: 0.8413 - val_loss: 0.4889 - val_acc: 0.8188\nEpoch 203/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3969 - acc: 0.8409 - val_loss: 0.4835 - val_acc: 0.8320\nEpoch 204/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.4035 - acc: 0.8406 - val_loss: 0.4971 - val_acc: 0.8175\nEpoch 205/250\n12096/12096 [==============================] - 3s 285us/sample - loss: 0.3980 - acc: 0.8414 - val_loss: 0.5104 - val_acc: 0.8142\nEpoch 206/250\n12096/12096 [==============================] - 4s 297us/sample - loss: 0.4033 - acc: 0.8403 - val_loss: 0.4902 - val_acc: 0.8188\nEpoch 207/250\n12096/12096 [==============================] - 4s 296us/sample - loss: 0.3947 - acc: 0.8395 - val_loss: 0.4946 - val_acc: 0.8168\nEpoch 208/250\n12096/12096 [==============================] - 3s 283us/sample - loss: 0.4010 - acc: 0.8384 - val_loss: 0.4979 - val_acc: 0.8185\nEpoch 209/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.3973 - acc: 0.8391 - val_loss: 0.4928 - val_acc: 0.8178\nEpoch 210/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3997 - acc: 0.8382 - val_loss: 0.4837 - val_acc: 0.8181\nEpoch 211/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.3969 - acc: 0.8371 - val_loss: 0.4876 - val_acc: 0.8244\nEpoch 212/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.3947 - acc: 0.8416 - val_loss: 0.5085 - val_acc: 0.8161\nEpoch 213/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.3885 - acc: 0.8425 - val_loss: 0.4807 - val_acc: 0.8287\nEpoch 214/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.4025 - acc: 0.8367 - val_loss: 0.4877 - val_acc: 0.8175\nEpoch 215/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3962 - acc: 0.8436 - val_loss: 0.4837 - val_acc: 0.8307\nEpoch 216/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.3895 - acc: 0.8433 - val_loss: 0.5027 - val_acc: 0.8234\nEpoch 217/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.4001 - acc: 0.8453 - val_loss: 0.4869 - val_acc: 0.8244\nEpoch 218/250\n12096/12096 [==============================] - 3s 277us/sample - loss: 0.4034 - acc: 0.8389 - val_loss: 0.4868 - val_acc: 0.8115\nEpoch 219/250\n12096/12096 [==============================] - 3s 277us/sample - loss: 0.3892 - acc: 0.8449 - val_loss: 0.4942 - val_acc: 0.8274\nEpoch 220/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3951 - acc: 0.8428 - val_loss: 0.4912 - val_acc: 0.8231\nEpoch 221/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3866 - acc: 0.8467 - val_loss: 0.4618 - val_acc: 0.8313\nEpoch 222/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4072 - acc: 0.8355 - val_loss: 0.4794 - val_acc: 0.8307\nEpoch 223/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.3958 - acc: 0.8461 - val_loss: 0.4692 - val_acc: 0.8307\nEpoch 224/250\n12096/12096 [==============================] - 3s 271us/sample - loss: 0.3888 - acc: 0.8430 - val_loss: 0.4984 - val_acc: 0.8231\nEpoch 225/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3914 - acc: 0.8450 - val_loss: 0.5259 - val_acc: 0.8125\nEpoch 226/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3958 - acc: 0.8364 - val_loss: 0.4940 - val_acc: 0.8251\nEpoch 227/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.3926 - acc: 0.8423 - val_loss: 0.4866 - val_acc: 0.8228\nEpoch 228/250\n12096/12096 [==============================] - 3s 266us/sample - loss: 0.3916 - acc: 0.8418 - val_loss: 0.4838 - val_acc: 0.8280\nEpoch 229/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 3s 267us/sample - loss: 0.3914 - acc: 0.8449 - val_loss: 0.4823 - val_acc: 0.8261\nEpoch 230/250\n12096/12096 [==============================] - 3s 269us/sample - loss: 0.3952 - acc: 0.8411 - val_loss: 0.4919 - val_acc: 0.8251\nEpoch 231/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3918 - acc: 0.8419 - val_loss: 0.4808 - val_acc: 0.8294\nEpoch 232/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.4005 - acc: 0.8405 - val_loss: 0.5127 - val_acc: 0.8112\nEpoch 233/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.4156 - acc: 0.8352 - val_loss: 0.5008 - val_acc: 0.8118\nEpoch 234/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.4024 - acc: 0.8394 - val_loss: 0.4825 - val_acc: 0.8247\nEpoch 235/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3881 - acc: 0.8444 - val_loss: 0.4892 - val_acc: 0.8307\nEpoch 236/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.3950 - acc: 0.8436 - val_loss: 0.4816 - val_acc: 0.8237\nEpoch 237/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3842 - acc: 0.8453 - val_loss: 0.4782 - val_acc: 0.8284\nEpoch 238/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.3931 - acc: 0.8443 - val_loss: 0.4916 - val_acc: 0.8241\nEpoch 239/250\n12096/12096 [==============================] - 3s 267us/sample - loss: 0.4011 - acc: 0.8369 - val_loss: 0.4915 - val_acc: 0.8191\nEpoch 240/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.3945 - acc: 0.8433 - val_loss: 0.4907 - val_acc: 0.8231\nEpoch 241/250\n12096/12096 [==============================] - 3s 274us/sample - loss: 0.3806 - acc: 0.8442 - val_loss: 0.4792 - val_acc: 0.8294\nEpoch 242/250\n12096/12096 [==============================] - 3s 270us/sample - loss: 0.3877 - acc: 0.8440 - val_loss: 0.5176 - val_acc: 0.8108\nEpoch 243/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.3931 - acc: 0.8423 - val_loss: 0.4799 - val_acc: 0.8261\nEpoch 244/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.3770 - acc: 0.8453 - val_loss: 0.4925 - val_acc: 0.8221\nEpoch 245/250\n12096/12096 [==============================] - 3s 275us/sample - loss: 0.3831 - acc: 0.8456 - val_loss: 0.4883 - val_acc: 0.8208\nEpoch 246/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.3942 - acc: 0.8421 - val_loss: 0.5234 - val_acc: 0.8075\nEpoch 247/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.3839 - acc: 0.8427 - val_loss: 0.5009 - val_acc: 0.8231\nEpoch 248/250\n12096/12096 [==============================] - 3s 273us/sample - loss: 0.3867 - acc: 0.8445 - val_loss: 0.4857 - val_acc: 0.8211\nEpoch 249/250\n12096/12096 [==============================] - 3s 272us/sample - loss: 0.3812 - acc: 0.8476 - val_loss: 0.4795 - val_acc: 0.8244\nEpoch 250/250\n12096/12096 [==============================] - 3s 276us/sample - loss: 0.3853 - acc: 0.8448 - val_loss: 0.4861 - val_acc: 0.8185\n","name":"stdout"},{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f47a44018d0>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict!!\ny_predict = model.predict(x_predict)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n\tprint(y_predict[i], np.argmax(y_predict[i])+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save predictions to a file for submission\n#argmax give us the highest probable label\n# we add one to the predictions to scale from 0..6 to 1..7\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict.argmax(axis=1)+1})\noutput.to_csv('submission.csv', index=False)","execution_count":28,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}