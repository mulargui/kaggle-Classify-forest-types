{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first experiment in this competition. Whereas XGBoost is highly recommended I rather tried to see how far I can go with an NN (using Keras).\n\nThis is the basic model and with 250 epochs has an accuracy of 80% (really poor).\n\nI'll continue for a few days researching how much I can optimize this model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\nprint(os.listdir(\"../working/\"))\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['learn-together']\n['.ipynb_checkpoints', '__notebook_source__.ipynb']\n/kaggle/input/learn-together/test.csv\n/kaggle/input/learn-together/train.csv\n/kaggle/input/learn-together/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load data, I had an issue with the data and hacked the data into the kernel\n#dftrain=pd.read_csv('/kaggle/input/train.csv')\n#dftest=pd.read_csv('/kaggle/input/test.csv')\ndftrain=pd.read_csv('/kaggle/input/learn-together/train.csv')\ndftest=pd.read_csv('/kaggle/input/learn-together/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#file shape\nprint(dftrain.head())\nprint(dftrain.shape[0])\nprint(dftest.head())\nprint(dftest.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features types\nprint(dftrain.dtypes)\nprint(dftest.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate files - nans per feature\nprint(dftrain.isnull().sum(axis = 0))\nprint(dftest.isnull().sum(axis = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate files - no rows with all zeros\nprint(dftrain[dftrain.drop(['Id','Cover_Type'], axis=1).eq(0).all(1)].empty)\nprint(dftest[dftest.drop('Id', axis=1).eq(0).all(1)].empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#force all types to float\nx = x.astype(float)\nx_predict = x_predict.astype(float)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize features\n#in the future it can be done more elegantly, for now just using the max min values of the data that we know\n#x['Elevation']=(x['Elevation']-x['Elevation'].min())/(x['Elevation'].max()-x['Elevation'].min())                             \nx['Elevation']=(x['Elevation']-1859)/(3858-1859)                             \nx['Aspect']=x['Aspect']/360                      \nx['Slope']=x['Slope']/66                      \nx['Horizontal_Distance_To_Hydrology']=x['Horizontal_Distance_To_Hydrology']/1397                      \nx['Vertical_Distance_To_Hydrology']=(x['Vertical_Distance_To_Hydrology']+173)/(601+173)                             \nx['Horizontal_Distance_To_Roadways']=x['Horizontal_Distance_To_Roadways']/7117                      \nx['Hillshade_9am']=x['Hillshade_9am']/254                      \nx['Hillshade_Noon']=x['Hillshade_Noon']/254                      \nx['Hillshade_3pm']=x['Hillshade_3pm']/254                      \nx['Horizontal_Distance_To_Fire_Points']=x['Horizontal_Distance_To_Fire_Points']/67173                      \n                                \nx_predict['Elevation']=(x_predict['Elevation']-1859)/(3858-1859)                             \nx_predict['Aspect']=x_predict['Aspect']/360                      \nx_predict['Slope']=x_predict['Slope']/66                      \nx_predict['Horizontal_Distance_To_Hydrology']=x_predict['Horizontal_Distance_To_Hydrology']/1397                      \nx_predict['Vertical_Distance_To_Hydrology']=(x_predict['Vertical_Distance_To_Hydrology']+173)/(601+173)                             \nx_predict['Horizontal_Distance_To_Roadways']=x_predict['Horizontal_Distance_To_Roadways']/7117                      \nx_predict['Hillshade_9am']=x_predict['Hillshade_9am']/254                      \nx_predict['Hillshade_Noon']=x_predict['Hillshade_Noon']/254                      \nx_predict['Hillshade_3pm']=x_predict['Hillshade_3pm']/254                      \nx_predict['Horizontal_Distance_To_Fire_Points']=x_predict['Horizontal_Distance_To_Fire_Points']/67173                      ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate data - no rows with all zeros\n#x.index[x.eq(0).all(1)]\nprint(x[x.eq(0).all(1)].empty)\nprint(x_predict[x_predict.eq(0).all(1)].empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the label to One Hot Encoding\n#to_categorical requires 0..6 instead of 1..7\ny -=1\ny = y.to_numpy()\n\nnum_classes = 7\n\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y, num_classes)","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the features dataframes to numpy arrays\nx = x.to_numpy()\nx_predict = x_predict.to_numpy()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split in train (80%) and test (20%) sets \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here is the NN model\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\nnum_features = 54\n\nmodel = Sequential()\nmodel.add(Dense(units=num_features, activation='relu', kernel_initializer='normal', input_dim=num_features))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='Adam',\n              metrics=['accuracy'])","execution_count":9,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=250)","execution_count":10,"outputs":[{"output_type":"stream","text":"Train on 12096 samples, validate on 3024 samples\nEpoch 1/250\n12096/12096 [==============================] - 3s 212us/sample - loss: 1.6067 - acc: 0.2730 - val_loss: 1.1574 - val_acc: 0.5241\nEpoch 2/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 1.0566 - acc: 0.5370 - val_loss: 0.9281 - val_acc: 0.5880\nEpoch 3/250\n12096/12096 [==============================] - 2s 150us/sample - loss: 0.9555 - acc: 0.5770 - val_loss: 0.8757 - val_acc: 0.6323\nEpoch 4/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.9231 - acc: 0.5933 - val_loss: 0.8073 - val_acc: 0.6376\nEpoch 5/250\n12096/12096 [==============================] - 2s 150us/sample - loss: 0.8859 - acc: 0.6088 - val_loss: 0.7968 - val_acc: 0.6505\nEpoch 6/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.8823 - acc: 0.6173 - val_loss: 0.8012 - val_acc: 0.6481\nEpoch 7/250\n12096/12096 [==============================] - 2s 150us/sample - loss: 0.8747 - acc: 0.6176 - val_loss: 0.7749 - val_acc: 0.6548\nEpoch 8/250\n12096/12096 [==============================] - 2s 162us/sample - loss: 0.8501 - acc: 0.6209 - val_loss: 0.7806 - val_acc: 0.6538\nEpoch 9/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.8338 - acc: 0.6284 - val_loss: 0.7784 - val_acc: 0.6541\nEpoch 10/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.8324 - acc: 0.6271 - val_loss: 0.7563 - val_acc: 0.6653\nEpoch 11/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.8236 - acc: 0.6336 - val_loss: 0.7408 - val_acc: 0.6766\nEpoch 12/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.8189 - acc: 0.6400 - val_loss: 0.7378 - val_acc: 0.6700\nEpoch 13/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.8211 - acc: 0.6396 - val_loss: 0.7405 - val_acc: 0.6680\nEpoch 14/250\n12096/12096 [==============================] - 2s 167us/sample - loss: 0.8009 - acc: 0.6412 - val_loss: 0.7447 - val_acc: 0.6759\nEpoch 15/250\n12096/12096 [==============================] - 2s 173us/sample - loss: 0.8090 - acc: 0.6345 - val_loss: 0.7315 - val_acc: 0.6733\nEpoch 16/250\n12096/12096 [==============================] - 2s 172us/sample - loss: 0.7936 - acc: 0.6477 - val_loss: 0.7284 - val_acc: 0.6726\nEpoch 17/250\n12096/12096 [==============================] - 2s 166us/sample - loss: 0.8023 - acc: 0.6453 - val_loss: 0.7207 - val_acc: 0.6849\nEpoch 18/250\n12096/12096 [==============================] - 2s 167us/sample - loss: 0.7907 - acc: 0.6448 - val_loss: 0.7816 - val_acc: 0.6415\nEpoch 19/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.8015 - acc: 0.6475 - val_loss: 0.7275 - val_acc: 0.6726\nEpoch 20/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.7784 - acc: 0.6552 - val_loss: 0.6984 - val_acc: 0.6878\nEpoch 21/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7780 - acc: 0.6550 - val_loss: 0.7248 - val_acc: 0.6829\nEpoch 22/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.7950 - acc: 0.6502 - val_loss: 0.7061 - val_acc: 0.6772\nEpoch 23/250\n12096/12096 [==============================] - 2s 165us/sample - loss: 0.7946 - acc: 0.6515 - val_loss: 0.7333 - val_acc: 0.6710\nEpoch 24/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.7795 - acc: 0.6539 - val_loss: 0.7365 - val_acc: 0.6703\nEpoch 25/250\n12096/12096 [==============================] - 2s 166us/sample - loss: 0.7758 - acc: 0.6547 - val_loss: 0.7179 - val_acc: 0.6809\nEpoch 26/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.7959 - acc: 0.6474 - val_loss: 0.7248 - val_acc: 0.6749\nEpoch 27/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.7885 - acc: 0.6527 - val_loss: 0.7069 - val_acc: 0.6809\nEpoch 28/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.7699 - acc: 0.6568 - val_loss: 0.7043 - val_acc: 0.6865\nEpoch 29/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.7687 - acc: 0.6582 - val_loss: 0.7368 - val_acc: 0.6825\nEpoch 30/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7626 - acc: 0.6638 - val_loss: 0.6934 - val_acc: 0.6766\nEpoch 31/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7610 - acc: 0.6548 - val_loss: 0.7034 - val_acc: 0.6862\nEpoch 32/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.7687 - acc: 0.6560 - val_loss: 0.7223 - val_acc: 0.6624\nEpoch 33/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.7738 - acc: 0.6561 - val_loss: 0.7277 - val_acc: 0.6634\nEpoch 34/250\n12096/12096 [==============================] - 2s 146us/sample - loss: 0.7607 - acc: 0.6626 - val_loss: 0.7592 - val_acc: 0.6720\nEpoch 35/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.7623 - acc: 0.6652 - val_loss: 0.6904 - val_acc: 0.6720\nEpoch 36/250\n12096/12096 [==============================] - 2s 165us/sample - loss: 0.7406 - acc: 0.6717 - val_loss: 0.6721 - val_acc: 0.7007\nEpoch 37/250\n12096/12096 [==============================] - 2s 146us/sample - loss: 0.7481 - acc: 0.6654 - val_loss: 0.7416 - val_acc: 0.6825\nEpoch 38/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.7450 - acc: 0.6664 - val_loss: 0.6786 - val_acc: 0.6888\nEpoch 39/250\n12096/12096 [==============================] - 2s 144us/sample - loss: 0.7401 - acc: 0.6679 - val_loss: 0.6830 - val_acc: 0.6892\nEpoch 40/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7448 - acc: 0.6675 - val_loss: 0.6826 - val_acc: 0.6799\nEpoch 41/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.7460 - acc: 0.6660 - val_loss: 0.6931 - val_acc: 0.6849\nEpoch 42/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.7337 - acc: 0.6729 - val_loss: 0.7029 - val_acc: 0.7067\nEpoch 43/250\n12096/12096 [==============================] - 2s 153us/sample - loss: 0.7368 - acc: 0.6743 - val_loss: 0.6889 - val_acc: 0.6925\nEpoch 44/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.7227 - acc: 0.6858 - val_loss: 0.6544 - val_acc: 0.6951\nEpoch 45/250\n12096/12096 [==============================] - 2s 146us/sample - loss: 0.7247 - acc: 0.6861 - val_loss: 0.6759 - val_acc: 0.7047\nEpoch 46/250\n12096/12096 [==============================] - 2s 149us/sample - loss: 0.7206 - acc: 0.6837 - val_loss: 0.6516 - val_acc: 0.7097\nEpoch 47/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7160 - acc: 0.6828 - val_loss: 0.6514 - val_acc: 0.7080\nEpoch 48/250\n12096/12096 [==============================] - 2s 153us/sample - loss: 0.7329 - acc: 0.6826 - val_loss: 0.6563 - val_acc: 0.7331\nEpoch 49/250\n12096/12096 [==============================] - 2s 149us/sample - loss: 0.7347 - acc: 0.6744 - val_loss: 0.6637 - val_acc: 0.7192\nEpoch 50/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.7267 - acc: 0.6807 - val_loss: 0.6843 - val_acc: 0.7070\nEpoch 51/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.7280 - acc: 0.6813 - val_loss: 0.7004 - val_acc: 0.6931\nEpoch 52/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.7244 - acc: 0.6878 - val_loss: 0.6586 - val_acc: 0.7063\nEpoch 53/250\n12096/12096 [==============================] - 2s 150us/sample - loss: 0.7133 - acc: 0.6896 - val_loss: 0.6646 - val_acc: 0.7166\nEpoch 54/250\n12096/12096 [==============================] - 2s 149us/sample - loss: 0.7175 - acc: 0.6901 - val_loss: 0.6949 - val_acc: 0.6898\nEpoch 55/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.7181 - acc: 0.6908 - val_loss: 0.6844 - val_acc: 0.7116\nEpoch 56/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.7198 - acc: 0.6944 - val_loss: 0.6667 - val_acc: 0.7249\nEpoch 57/250\n12096/12096 [==============================] - 2s 171us/sample - loss: 0.7194 - acc: 0.6917 - val_loss: 0.6955 - val_acc: 0.7014\nEpoch 58/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 2s 153us/sample - loss: 0.7110 - acc: 0.6965 - val_loss: 0.7151 - val_acc: 0.6862\nEpoch 59/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.7272 - acc: 0.6844 - val_loss: 0.6549 - val_acc: 0.7153\nEpoch 60/250\n12096/12096 [==============================] - 2s 165us/sample - loss: 0.7138 - acc: 0.6925 - val_loss: 0.6679 - val_acc: 0.7315\nEpoch 61/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.7097 - acc: 0.6968 - val_loss: 0.6929 - val_acc: 0.7037\nEpoch 62/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.7086 - acc: 0.7010 - val_loss: 0.6515 - val_acc: 0.7378\nEpoch 63/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.7071 - acc: 0.7022 - val_loss: 0.6538 - val_acc: 0.7298\nEpoch 64/250\n12096/12096 [==============================] - 2s 168us/sample - loss: 0.7069 - acc: 0.7007 - val_loss: 0.6823 - val_acc: 0.7226\nEpoch 65/250\n12096/12096 [==============================] - 2s 151us/sample - loss: 0.7217 - acc: 0.6945 - val_loss: 0.6883 - val_acc: 0.7163\nEpoch 66/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.7020 - acc: 0.7053 - val_loss: 0.6442 - val_acc: 0.7275\nEpoch 67/250\n12096/12096 [==============================] - 2s 147us/sample - loss: 0.7118 - acc: 0.7025 - val_loss: 0.7508 - val_acc: 0.7004\nEpoch 68/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7203 - acc: 0.7013 - val_loss: 0.6640 - val_acc: 0.7275\nEpoch 69/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.7117 - acc: 0.7023 - val_loss: 0.6893 - val_acc: 0.7047\nEpoch 70/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7133 - acc: 0.7012 - val_loss: 0.6632 - val_acc: 0.7364\nEpoch 71/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7063 - acc: 0.6986 - val_loss: 0.6543 - val_acc: 0.7368\nEpoch 72/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.7117 - acc: 0.7032 - val_loss: 0.6640 - val_acc: 0.7328\nEpoch 73/250\n12096/12096 [==============================] - 2s 153us/sample - loss: 0.7063 - acc: 0.7043 - val_loss: 0.6743 - val_acc: 0.7140\nEpoch 74/250\n12096/12096 [==============================] - 2s 153us/sample - loss: 0.7002 - acc: 0.7112 - val_loss: 0.6804 - val_acc: 0.7295\nEpoch 75/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.7205 - acc: 0.7028 - val_loss: 0.6393 - val_acc: 0.7404\nEpoch 76/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.7072 - acc: 0.7049 - val_loss: 0.6662 - val_acc: 0.7282\nEpoch 77/250\n12096/12096 [==============================] - 2s 151us/sample - loss: 0.6992 - acc: 0.7097 - val_loss: 0.6329 - val_acc: 0.7345\nEpoch 78/250\n12096/12096 [==============================] - 2s 147us/sample - loss: 0.6982 - acc: 0.7045 - val_loss: 0.6385 - val_acc: 0.7424\nEpoch 79/250\n12096/12096 [==============================] - 2s 147us/sample - loss: 0.6918 - acc: 0.7165 - val_loss: 0.6610 - val_acc: 0.7394\nEpoch 80/250\n12096/12096 [==============================] - 2s 149us/sample - loss: 0.6826 - acc: 0.7206 - val_loss: 0.6384 - val_acc: 0.7457\nEpoch 81/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6861 - acc: 0.7160 - val_loss: 0.6320 - val_acc: 0.7497\nEpoch 82/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6810 - acc: 0.7198 - val_loss: 0.6294 - val_acc: 0.7374\nEpoch 83/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.6874 - acc: 0.7150 - val_loss: 0.6543 - val_acc: 0.7384\nEpoch 84/250\n12096/12096 [==============================] - 2s 162us/sample - loss: 0.6906 - acc: 0.7144 - val_loss: 0.6633 - val_acc: 0.7351\nEpoch 85/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6716 - acc: 0.7235 - val_loss: 0.6567 - val_acc: 0.7249\nEpoch 86/250\n12096/12096 [==============================] - 2s 151us/sample - loss: 0.7050 - acc: 0.7095 - val_loss: 0.6465 - val_acc: 0.7374\nEpoch 87/250\n12096/12096 [==============================] - 2s 150us/sample - loss: 0.6973 - acc: 0.7154 - val_loss: 0.6338 - val_acc: 0.7411\nEpoch 88/250\n12096/12096 [==============================] - 2s 164us/sample - loss: 0.6928 - acc: 0.7174 - val_loss: 0.6356 - val_acc: 0.7374\nEpoch 89/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.6787 - acc: 0.7183 - val_loss: 0.6435 - val_acc: 0.7454\nEpoch 90/250\n12096/12096 [==============================] - 2s 150us/sample - loss: 0.6739 - acc: 0.7202 - val_loss: 0.6336 - val_acc: 0.7411\nEpoch 91/250\n12096/12096 [==============================] - 2s 144us/sample - loss: 0.6781 - acc: 0.7178 - val_loss: 0.6330 - val_acc: 0.7437\nEpoch 92/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.6801 - acc: 0.7218 - val_loss: 0.6560 - val_acc: 0.7315\nEpoch 93/250\n12096/12096 [==============================] - 2s 145us/sample - loss: 0.6681 - acc: 0.7252 - val_loss: 0.6779 - val_acc: 0.7318\nEpoch 94/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6726 - acc: 0.7278 - val_loss: 0.6225 - val_acc: 0.7569\nEpoch 95/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6722 - acc: 0.7199 - val_loss: 0.6191 - val_acc: 0.7503\nEpoch 96/250\n12096/12096 [==============================] - 2s 151us/sample - loss: 0.6934 - acc: 0.7116 - val_loss: 0.6115 - val_acc: 0.7450\nEpoch 97/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6815 - acc: 0.7193 - val_loss: 0.6634 - val_acc: 0.7186\nEpoch 98/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6815 - acc: 0.7171 - val_loss: 0.6761 - val_acc: 0.7179\nEpoch 99/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6693 - acc: 0.7258 - val_loss: 0.6731 - val_acc: 0.7235\nEpoch 100/250\n12096/12096 [==============================] - 2s 147us/sample - loss: 0.6808 - acc: 0.7154 - val_loss: 0.6450 - val_acc: 0.7331\nEpoch 101/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.6803 - acc: 0.7238 - val_loss: 0.6265 - val_acc: 0.7599\nEpoch 102/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.6657 - acc: 0.7262 - val_loss: 0.6440 - val_acc: 0.7282\nEpoch 103/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6654 - acc: 0.7273 - val_loss: 0.6616 - val_acc: 0.7262\nEpoch 104/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6748 - acc: 0.7264 - val_loss: 0.6325 - val_acc: 0.7437\nEpoch 105/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6912 - acc: 0.7170 - val_loss: 0.6278 - val_acc: 0.7483\nEpoch 106/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6832 - acc: 0.7185 - val_loss: 0.6555 - val_acc: 0.7421\nEpoch 107/250\n12096/12096 [==============================] - 2s 167us/sample - loss: 0.6692 - acc: 0.7243 - val_loss: 0.6385 - val_acc: 0.7490\nEpoch 108/250\n12096/12096 [==============================] - 2s 165us/sample - loss: 0.6717 - acc: 0.7264 - val_loss: 0.6479 - val_acc: 0.7510\nEpoch 109/250\n12096/12096 [==============================] - 2s 176us/sample - loss: 0.6536 - acc: 0.7358 - val_loss: 0.6463 - val_acc: 0.7457\nEpoch 110/250\n12096/12096 [==============================] - 2s 172us/sample - loss: 0.6673 - acc: 0.7285 - val_loss: 0.6437 - val_acc: 0.7487\nEpoch 111/250\n12096/12096 [==============================] - 2s 175us/sample - loss: 0.6807 - acc: 0.7246 - val_loss: 0.6418 - val_acc: 0.7470\nEpoch 112/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6743 - acc: 0.7218 - val_loss: 0.6492 - val_acc: 0.7331\nEpoch 113/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6762 - acc: 0.7211 - val_loss: 0.6280 - val_acc: 0.7450\nEpoch 114/250\n12096/12096 [==============================] - 2s 173us/sample - loss: 0.6643 - acc: 0.7251 - val_loss: 0.6487 - val_acc: 0.7427\nEpoch 115/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 2s 172us/sample - loss: 0.6473 - acc: 0.7359 - val_loss: 0.6408 - val_acc: 0.7470\nEpoch 116/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6747 - acc: 0.7232 - val_loss: 0.6727 - val_acc: 0.7335\nEpoch 117/250\n12096/12096 [==============================] - 2s 170us/sample - loss: 0.6701 - acc: 0.7228 - val_loss: 0.6714 - val_acc: 0.7285\nEpoch 118/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6653 - acc: 0.7328 - val_loss: 0.6722 - val_acc: 0.7285\nEpoch 119/250\n12096/12096 [==============================] - 2s 174us/sample - loss: 0.6657 - acc: 0.7342 - val_loss: 0.6242 - val_acc: 0.7483\nEpoch 120/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6644 - acc: 0.7318 - val_loss: 0.6443 - val_acc: 0.7371\nEpoch 121/250\n12096/12096 [==============================] - 2s 165us/sample - loss: 0.6636 - acc: 0.7267 - val_loss: 0.6737 - val_acc: 0.7146\nEpoch 122/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6675 - acc: 0.7295 - val_loss: 0.6428 - val_acc: 0.7421\nEpoch 123/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6564 - acc: 0.7302 - val_loss: 0.5999 - val_acc: 0.7553\nEpoch 124/250\n12096/12096 [==============================] - 2s 168us/sample - loss: 0.6474 - acc: 0.7364 - val_loss: 0.6291 - val_acc: 0.7444\nEpoch 125/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6595 - acc: 0.7338 - val_loss: 0.6104 - val_acc: 0.7616\nEpoch 126/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6522 - acc: 0.7360 - val_loss: 0.6543 - val_acc: 0.7440\nEpoch 127/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6611 - acc: 0.7332 - val_loss: 0.6180 - val_acc: 0.7503\nEpoch 128/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6509 - acc: 0.7332 - val_loss: 0.6176 - val_acc: 0.7563\nEpoch 129/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6578 - acc: 0.7320 - val_loss: 0.6272 - val_acc: 0.7517\nEpoch 130/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.6556 - acc: 0.7287 - val_loss: 0.6289 - val_acc: 0.7444\nEpoch 131/250\n12096/12096 [==============================] - 2s 146us/sample - loss: 0.6397 - acc: 0.7380 - val_loss: 0.6524 - val_acc: 0.7325\nEpoch 132/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6582 - acc: 0.7358 - val_loss: 0.6254 - val_acc: 0.7540\nEpoch 133/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6682 - acc: 0.7276 - val_loss: 0.6201 - val_acc: 0.7424\nEpoch 134/250\n12096/12096 [==============================] - 2s 167us/sample - loss: 0.6556 - acc: 0.7334 - val_loss: 0.6228 - val_acc: 0.7437\nEpoch 135/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6570 - acc: 0.7266 - val_loss: 0.6335 - val_acc: 0.7374\nEpoch 136/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6530 - acc: 0.7343 - val_loss: 0.6344 - val_acc: 0.7474\nEpoch 137/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6587 - acc: 0.7327 - val_loss: 0.5991 - val_acc: 0.7490\nEpoch 138/250\n12096/12096 [==============================] - 2s 183us/sample - loss: 0.6662 - acc: 0.7269 - val_loss: 0.6232 - val_acc: 0.7391\nEpoch 139/250\n12096/12096 [==============================] - 2s 179us/sample - loss: 0.6568 - acc: 0.7337 - val_loss: 0.6286 - val_acc: 0.7497\nEpoch 140/250\n12096/12096 [==============================] - 2s 189us/sample - loss: 0.6506 - acc: 0.7402 - val_loss: 0.6196 - val_acc: 0.7407\nEpoch 141/250\n12096/12096 [==============================] - 2s 202us/sample - loss: 0.6470 - acc: 0.7388 - val_loss: 0.6599 - val_acc: 0.7348\nEpoch 142/250\n12096/12096 [==============================] - 2s 177us/sample - loss: 0.6608 - acc: 0.7294 - val_loss: 0.6089 - val_acc: 0.7543\nEpoch 143/250\n12096/12096 [==============================] - 2s 162us/sample - loss: 0.6336 - acc: 0.7423 - val_loss: 0.5987 - val_acc: 0.7543\nEpoch 144/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.6559 - acc: 0.7384 - val_loss: 0.6440 - val_acc: 0.7321\nEpoch 145/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6469 - acc: 0.7345 - val_loss: 0.6329 - val_acc: 0.7417\nEpoch 146/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6543 - acc: 0.7312 - val_loss: 0.6381 - val_acc: 0.7391\nEpoch 147/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6718 - acc: 0.7298 - val_loss: 0.6226 - val_acc: 0.7520\nEpoch 148/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6653 - acc: 0.7340 - val_loss: 0.6628 - val_acc: 0.7354\nEpoch 149/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.6674 - acc: 0.7278 - val_loss: 0.6266 - val_acc: 0.7490\nEpoch 150/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6613 - acc: 0.7328 - val_loss: 0.6453 - val_acc: 0.7417\nEpoch 151/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6607 - acc: 0.7367 - val_loss: 0.6003 - val_acc: 0.7583\nEpoch 152/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6441 - acc: 0.7419 - val_loss: 0.6015 - val_acc: 0.7599\nEpoch 153/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6409 - acc: 0.7403 - val_loss: 0.6335 - val_acc: 0.7477\nEpoch 154/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6481 - acc: 0.7452 - val_loss: 0.6110 - val_acc: 0.7619\nEpoch 155/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6537 - acc: 0.7377 - val_loss: 0.6270 - val_acc: 0.7526\nEpoch 156/250\n12096/12096 [==============================] - 2s 170us/sample - loss: 0.6669 - acc: 0.7342 - val_loss: 0.6355 - val_acc: 0.7407\nEpoch 157/250\n12096/12096 [==============================] - 2s 172us/sample - loss: 0.6691 - acc: 0.7301 - val_loss: 0.6352 - val_acc: 0.7460\nEpoch 158/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.6549 - acc: 0.7364 - val_loss: 0.6232 - val_acc: 0.7632\nEpoch 159/250\n12096/12096 [==============================] - 2s 164us/sample - loss: 0.6491 - acc: 0.7376 - val_loss: 0.6140 - val_acc: 0.7507\nEpoch 160/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6408 - acc: 0.7434 - val_loss: 0.6256 - val_acc: 0.7500\nEpoch 161/250\n12096/12096 [==============================] - 2s 162us/sample - loss: 0.6538 - acc: 0.7345 - val_loss: 0.6353 - val_acc: 0.7348\nEpoch 162/250\n12096/12096 [==============================] - 2s 165us/sample - loss: 0.6524 - acc: 0.7313 - val_loss: 0.6504 - val_acc: 0.7384\nEpoch 163/250\n12096/12096 [==============================] - 2s 167us/sample - loss: 0.6519 - acc: 0.7362 - val_loss: 0.6470 - val_acc: 0.7411\nEpoch 164/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.6639 - acc: 0.7330 - val_loss: 0.6093 - val_acc: 0.7513\nEpoch 165/250\n12096/12096 [==============================] - 2s 172us/sample - loss: 0.6439 - acc: 0.7397 - val_loss: 0.6075 - val_acc: 0.7497\nEpoch 166/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6455 - acc: 0.7327 - val_loss: 0.5836 - val_acc: 0.7603\nEpoch 167/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6367 - acc: 0.7475 - val_loss: 0.6388 - val_acc: 0.7335\nEpoch 168/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6295 - acc: 0.7444 - val_loss: 0.6127 - val_acc: 0.7629\nEpoch 169/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6452 - acc: 0.7354 - val_loss: 0.5959 - val_acc: 0.7619\nEpoch 170/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.6478 - acc: 0.7383 - val_loss: 0.5971 - val_acc: 0.7612\nEpoch 171/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6369 - acc: 0.7447 - val_loss: 0.6086 - val_acc: 0.7652\nEpoch 172/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 2s 164us/sample - loss: 0.6422 - acc: 0.7434 - val_loss: 0.6482 - val_acc: 0.7493\nEpoch 173/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6383 - acc: 0.7427 - val_loss: 0.6721 - val_acc: 0.7298\nEpoch 174/250\n12096/12096 [==============================] - 2s 167us/sample - loss: 0.6464 - acc: 0.7438 - val_loss: 0.6349 - val_acc: 0.7361\nEpoch 175/250\n12096/12096 [==============================] - 2s 168us/sample - loss: 0.6472 - acc: 0.7378 - val_loss: 0.6048 - val_acc: 0.7513\nEpoch 176/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6330 - acc: 0.7430 - val_loss: 0.6185 - val_acc: 0.7480\nEpoch 177/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6369 - acc: 0.7439 - val_loss: 0.6263 - val_acc: 0.7543\nEpoch 178/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6452 - acc: 0.7418 - val_loss: 0.6283 - val_acc: 0.7526\nEpoch 179/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6445 - acc: 0.7400 - val_loss: 0.6222 - val_acc: 0.7576\nEpoch 180/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.6277 - acc: 0.7447 - val_loss: 0.6063 - val_acc: 0.7616\nEpoch 181/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6266 - acc: 0.7469 - val_loss: 0.6271 - val_acc: 0.7460\nEpoch 182/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.6331 - acc: 0.7470 - val_loss: 0.6487 - val_acc: 0.7447\nEpoch 183/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6541 - acc: 0.7407 - val_loss: 0.6598 - val_acc: 0.7222\nEpoch 184/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.6593 - acc: 0.7348 - val_loss: 0.6513 - val_acc: 0.7397\nEpoch 185/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6595 - acc: 0.7377 - val_loss: 0.6155 - val_acc: 0.7556\nEpoch 186/250\n12096/12096 [==============================] - 2s 167us/sample - loss: 0.6353 - acc: 0.7476 - val_loss: 0.6478 - val_acc: 0.7285\nEpoch 187/250\n12096/12096 [==============================] - 2s 166us/sample - loss: 0.6292 - acc: 0.7545 - val_loss: 0.6016 - val_acc: 0.7619\nEpoch 188/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.6467 - acc: 0.7377 - val_loss: 0.6234 - val_acc: 0.7573\nEpoch 189/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.6316 - acc: 0.7414 - val_loss: 0.6059 - val_acc: 0.7470\nEpoch 190/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.6195 - acc: 0.7521 - val_loss: 0.6291 - val_acc: 0.7378\nEpoch 191/250\n12096/12096 [==============================] - 2s 144us/sample - loss: 0.6250 - acc: 0.7520 - val_loss: 0.6169 - val_acc: 0.7543\nEpoch 192/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6260 - acc: 0.7490 - val_loss: 0.6510 - val_acc: 0.7424\nEpoch 193/250\n12096/12096 [==============================] - 2s 153us/sample - loss: 0.6350 - acc: 0.7417 - val_loss: 0.5969 - val_acc: 0.7606\nEpoch 194/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6646 - acc: 0.7339 - val_loss: 0.6660 - val_acc: 0.7368\nEpoch 195/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.6408 - acc: 0.7414 - val_loss: 0.6219 - val_acc: 0.7467\nEpoch 196/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6294 - acc: 0.7443 - val_loss: 0.6099 - val_acc: 0.7573\nEpoch 197/250\n12096/12096 [==============================] - 2s 149us/sample - loss: 0.6295 - acc: 0.7422 - val_loss: 0.6092 - val_acc: 0.7662\nEpoch 198/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6305 - acc: 0.7529 - val_loss: 0.6122 - val_acc: 0.7583\nEpoch 199/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.6298 - acc: 0.7452 - val_loss: 0.6232 - val_acc: 0.7464\nEpoch 200/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.6385 - acc: 0.7423 - val_loss: 0.6278 - val_acc: 0.7440\nEpoch 201/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6294 - acc: 0.7395 - val_loss: 0.6362 - val_acc: 0.7348\nEpoch 202/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6254 - acc: 0.7483 - val_loss: 0.5980 - val_acc: 0.7503\nEpoch 203/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6193 - acc: 0.7479 - val_loss: 0.6102 - val_acc: 0.7550\nEpoch 204/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6225 - acc: 0.7464 - val_loss: 0.6416 - val_acc: 0.7308\nEpoch 205/250\n12096/12096 [==============================] - 2s 147us/sample - loss: 0.6262 - acc: 0.7455 - val_loss: 0.6210 - val_acc: 0.7364\nEpoch 206/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.6365 - acc: 0.7374 - val_loss: 0.6442 - val_acc: 0.7235\nEpoch 207/250\n12096/12096 [==============================] - 2s 162us/sample - loss: 0.6449 - acc: 0.7365 - val_loss: 0.6385 - val_acc: 0.7374\nEpoch 208/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6407 - acc: 0.7407 - val_loss: 0.5925 - val_acc: 0.7573\nEpoch 209/250\n12096/12096 [==============================] - 2s 153us/sample - loss: 0.6281 - acc: 0.7489 - val_loss: 0.6189 - val_acc: 0.7447\nEpoch 210/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6283 - acc: 0.7473 - val_loss: 0.6176 - val_acc: 0.7553\nEpoch 211/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6211 - acc: 0.7493 - val_loss: 0.6051 - val_acc: 0.7447\nEpoch 212/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6222 - acc: 0.7498 - val_loss: 0.6029 - val_acc: 0.7487\nEpoch 213/250\n12096/12096 [==============================] - 2s 173us/sample - loss: 0.6270 - acc: 0.7479 - val_loss: 0.6416 - val_acc: 0.7354\nEpoch 214/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6243 - acc: 0.7487 - val_loss: 0.6037 - val_acc: 0.7490\nEpoch 215/250\n12096/12096 [==============================] - 2s 162us/sample - loss: 0.6216 - acc: 0.7501 - val_loss: 0.6307 - val_acc: 0.7424\nEpoch 216/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6327 - acc: 0.7446 - val_loss: 0.6231 - val_acc: 0.7388\nEpoch 217/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6426 - acc: 0.7406 - val_loss: 0.6227 - val_acc: 0.7583\nEpoch 218/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6370 - acc: 0.7460 - val_loss: 0.6249 - val_acc: 0.7500\nEpoch 219/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6217 - acc: 0.7493 - val_loss: 0.5960 - val_acc: 0.7583\nEpoch 220/250\n12096/12096 [==============================] - 2s 151us/sample - loss: 0.6217 - acc: 0.7516 - val_loss: 0.6319 - val_acc: 0.7341\nEpoch 221/250\n12096/12096 [==============================] - 2s 157us/sample - loss: 0.6305 - acc: 0.7480 - val_loss: 0.6315 - val_acc: 0.7427\nEpoch 222/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6400 - acc: 0.7391 - val_loss: 0.6348 - val_acc: 0.7427\nEpoch 223/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6317 - acc: 0.7459 - val_loss: 0.5795 - val_acc: 0.7652\nEpoch 224/250\n12096/12096 [==============================] - 2s 162us/sample - loss: 0.6297 - acc: 0.7479 - val_loss: 0.6239 - val_acc: 0.7364\nEpoch 225/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.6173 - acc: 0.7519 - val_loss: 0.6000 - val_acc: 0.7517\nEpoch 226/250\n12096/12096 [==============================] - 2s 162us/sample - loss: 0.6328 - acc: 0.7478 - val_loss: 0.6267 - val_acc: 0.7292\nEpoch 227/250\n12096/12096 [==============================] - 2s 164us/sample - loss: 0.6312 - acc: 0.7473 - val_loss: 0.6016 - val_acc: 0.7569\nEpoch 228/250\n12096/12096 [==============================] - 2s 154us/sample - loss: 0.6239 - acc: 0.7528 - val_loss: 0.6454 - val_acc: 0.7302\nEpoch 229/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 2s 149us/sample - loss: 0.6363 - acc: 0.7426 - val_loss: 0.5958 - val_acc: 0.7536\nEpoch 230/250\n12096/12096 [==============================] - 2s 148us/sample - loss: 0.6068 - acc: 0.7547 - val_loss: 0.6218 - val_acc: 0.7467\nEpoch 231/250\n12096/12096 [==============================] - 2s 151us/sample - loss: 0.6226 - acc: 0.7470 - val_loss: 0.6012 - val_acc: 0.7622\nEpoch 232/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6206 - acc: 0.7503 - val_loss: 0.6083 - val_acc: 0.7507\nEpoch 233/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6235 - acc: 0.7479 - val_loss: 0.5998 - val_acc: 0.7576\nEpoch 234/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6284 - acc: 0.7474 - val_loss: 0.6053 - val_acc: 0.7652\nEpoch 235/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6324 - acc: 0.7402 - val_loss: 0.5935 - val_acc: 0.7530\nEpoch 236/250\n12096/12096 [==============================] - 2s 158us/sample - loss: 0.6123 - acc: 0.7545 - val_loss: 0.5857 - val_acc: 0.7646\nEpoch 237/250\n12096/12096 [==============================] - 2s 175us/sample - loss: 0.6212 - acc: 0.7503 - val_loss: 0.6028 - val_acc: 0.7626\nEpoch 238/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6255 - acc: 0.7492 - val_loss: 0.6056 - val_acc: 0.7573\nEpoch 239/250\n12096/12096 [==============================] - 2s 160us/sample - loss: 0.6297 - acc: 0.7512 - val_loss: 0.6174 - val_acc: 0.7536\nEpoch 240/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6256 - acc: 0.7478 - val_loss: 0.5994 - val_acc: 0.7606\nEpoch 241/250\n12096/12096 [==============================] - 2s 161us/sample - loss: 0.6343 - acc: 0.7460 - val_loss: 0.6089 - val_acc: 0.7563\nEpoch 242/250\n12096/12096 [==============================] - 2s 163us/sample - loss: 0.6224 - acc: 0.7521 - val_loss: 0.6008 - val_acc: 0.7526\nEpoch 243/250\n12096/12096 [==============================] - 2s 169us/sample - loss: 0.6279 - acc: 0.7473 - val_loss: 0.6202 - val_acc: 0.7523\nEpoch 244/250\n12096/12096 [==============================] - 2s 153us/sample - loss: 0.6311 - acc: 0.7446 - val_loss: 0.5992 - val_acc: 0.7543\nEpoch 245/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6178 - acc: 0.7522 - val_loss: 0.6094 - val_acc: 0.7490\nEpoch 246/250\n12096/12096 [==============================] - 2s 159us/sample - loss: 0.6223 - acc: 0.7450 - val_loss: 0.6014 - val_acc: 0.7622\nEpoch 247/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.6160 - acc: 0.7550 - val_loss: 0.6206 - val_acc: 0.7510\nEpoch 248/250\n12096/12096 [==============================] - 2s 152us/sample - loss: 0.6386 - acc: 0.7427 - val_loss: 0.6018 - val_acc: 0.7510\nEpoch 249/250\n12096/12096 [==============================] - 2s 153us/sample - loss: 0.6223 - acc: 0.7455 - val_loss: 0.6156 - val_acc: 0.7560\nEpoch 250/250\n12096/12096 [==============================] - 2s 156us/sample - loss: 0.6217 - acc: 0.7475 - val_loss: 0.5988 - val_acc: 0.7735\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7ff065bfe438>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict!!\ny_predict = model.predict(x_predict)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n\tprint(y_predict[i], np.argmax(y_predict[i])+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save predictions to a file for submission\n#argmax give us the highest probable label\n# we add one to the predictions to scale from 0..6 to 1..7\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict.argmax(axis=1)+1})\noutput.to_csv('submission.csv', index=False)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}