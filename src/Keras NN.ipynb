{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first experiment in this competition. Whereas XGBoost is highly recommended I rather tried to see how far I can go with an NN (using Keras).\nThis is the basic model and with 250 epochs has an accuracy of 80% (really poor).\nI'll continue for a few days researching how much I can optimize this model.\n\nNow switching to using forests in this new kernel https://www.kaggle.com/mulargui/xgboost\nYou can find all my notes and versions at https://github.com/mulargui/kaggle-Classify-forest-types"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n#load data\ndftrain=pd.read_csv('/kaggle/input/learn-together/train.csv')\ndftest=pd.read_csv('/kaggle/input/learn-together/test.csv')","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####### FEATURE ENGINEERING #####\n#taking most of this from my work in feature engineering at https://www.kaggle.com/mulargui/xgboost\n#https://www.kaggle.com/arateris/2-layer-k-fold-learning-forest-cover \n#Fixing Hillshade_3pm\n#replacing the zeros for better guess, mainly to avoid zeros in the feature engineering and fake outliers. \nnum_train = len(dftrain)\ntmp = dftrain.drop('Cover_Type', axis = 1)\nall_data = tmp.append(dftest)\n\ncols_for_HS = ['Aspect','Slope', 'Hillshade_9am','Hillshade_Noon']\nHS_zero = all_data[all_data.Hillshade_3pm==0]\nHS_train = all_data[all_data.Hillshade_3pm!=0]\n\nfrom sklearn.ensemble import RandomForestRegressor\nrf_hs = RandomForestRegressor(n_estimators=100).fit(HS_train[cols_for_HS], HS_train.Hillshade_3pm)\nout = rf_hs.predict(HS_zero[cols_for_HS]).astype(int)\n\n#I couldn't make this line work, feature not used\n#all_data.loc[HS_zero.index,'Hillshade_3pm'] = out\n#dftrain['Hillshade_3pm']= all_data.loc[:num_train,'Hillshade_3pm']\n#dftest['Hillshade_3pm']= all_data.loc[num_train:,'Hillshade_3pm']\n\n# Add PCA features\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=0.99).fit(all_data)\ntrans = pca.transform(all_data)\n\nfor i in range(trans.shape[1]):\n    col_name= 'pca'+str(i+1)\n    dftrain[col_name] = trans[:num_train, i]\n    dftest[col_name] = trans[num_train:, i]\n\n#https://www.kaggle.com/evimarp/top-6-roosevelt-national-forest-competition\ndef euclidean(df):\n    df['Euclidean_distance_to_hydro'] = (df.Vertical_Distance_To_Hydrology**2 \n                                         + df.Horizontal_Distance_To_Hydrology**2)**.5\n    return df\n\ndftrain = euclidean(dftrain)\ndftest = euclidean(dftest)\n\nfrom itertools import combinations\ndef distances(df):\n    cols = [\n        'Horizontal_Distance_To_Roadways',\n        'Horizontal_Distance_To_Fire_Points',\n        'Horizontal_Distance_To_Hydrology',\n    ]\n    df['distance_mean'] = df[cols].mean(axis=1)\n    df['distance_sum'] = df[cols].sum(axis=1)\n    df['distance_road_fire'] = df[cols[:2]].mean(axis=1)\n    df['distance_hydro_fire'] = df[cols[1:]].mean(axis=1)\n    df['distance_road_hydro'] = df[[cols[0], cols[2]]].mean(axis=1)\n    \n    df['distance_sum_road_fire'] = df[cols[:2]].sum(axis=1)\n    df['distance_sum_hydro_fire'] = df[cols[1:]].sum(axis=1)\n    df['distance_sum_road_hydro'] = df[[cols[0], cols[2]]].sum(axis=1)\n    \n    df['distance_dif_road_fire'] = df[cols[0]] - df[cols[1]]\n    df['distance_dif_hydro_road'] = df[cols[2]] - df[cols[0]]\n    df['distance_dif_hydro_fire'] = df[cols[2]] - df[cols[1]]\n    \n    # Vertical distances measures\n    colv = ['Elevation', 'Vertical_Distance_To_Hydrology']\n    df['Vertical_dif'] = df[colv[0]] - df[colv[1]]\n    df['Vertical_sum'] = df[colv].sum(axis=1)\n    \n    return df\n  \ndftrain = distances(dftrain)\ndftest = distances(dftest)\n    \ndef shade(df):\n    SHADES = ['Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n    \n    df['shade_noon_diff'] = df['Hillshade_9am'] - df['Hillshade_Noon']\n    df['shade_3pm_diff'] = df['Hillshade_Noon'] - df['Hillshade_3pm']\n    df['shade_all_diff'] = df['Hillshade_9am'] - df['Hillshade_3pm']\n    df['shade_sum'] = df[SHADES].sum(axis=1)\n    df['shade_mean'] = df[SHADES].mean(axis=1)\n    \n    return df\n\ndftrain = shade(dftrain)\ndftest = shade(dftest)\n\ndef elevation(df):\n    df['ElevationHydro'] = df['Elevation'] - 0.25 * df['Euclidean_distance_to_hydro']\n    return df\n\ndftrain = elevation(dftrain)\ndftest = elevation(dftest)\n\ndef elevationV(df):\n    df['ElevationV'] = df['Elevation'] - df['Vertical_Distance_To_Hydrology']\n    return df\n\ndftrain = elevationV(dftrain)\ndftest = elevationV(dftest)\n\ndef elevationH(df):\n    df['ElevationH'] = df['Elevation'] - 0.19 * df['Horizontal_Distance_To_Hydrology']\n    return df\n\ndftrain = elevationH(dftrain)\ndftest = elevationH(dftest)\n\ndef kernel_features(df):\n    df['Elevation2'] = df['Elevation']**2\n    df['ElevationLog'] = np.log1p(df['Elevation'])\n    return df\n\ndftrain = kernel_features(dftrain)\ndftest = kernel_features(dftest)\n\ndef degree(df):\n    df['Aspect_cos'] = np.cos(np.radians(df.Aspect))\n    df['Aspect_sin'] = np.sin(np.radians(df.Aspect))\n    #df['Slope_sin'] = np.sin(np.radians(df.Slope))\n    df['Aspectcos_Slope'] = df.Slope * df.Aspect_cos\n    #df['Aspectsin_Slope'] = df.Slope * df.Aspect_sin\n    \n    return df\n\ndftrain = degree(dftrain)\ndftest = degree(dftest)\n\nfrom bisect import bisect\ncardinals = [i for i in range(45, 361, 90)]\npoints = ['N', 'E', 'S', 'W']\n\ndef cardinal(df):\n    df['Cardinal'] = df.Aspect.apply(lambda x: points[bisect(cardinals, x) % 4])\n    return df\n\ndftrain = cardinal(dftrain)\ndftest = cardinal(dftest)\n\ndef cardinal_num(df):\n    d = {'N': 0, 'E': 1, 'S': 0, 'W':-1}\n    df['Cardinal'] = df.Cardinal.apply(lambda x: d[x])\n    return df\n\ndftrain = cardinal_num(dftrain)\ndftest = cardinal_num(dftest)\n\n#adding features based on https://douglas-fraser.com/forest_cover_management.pdf pages 21,22\n#note: not all climatic and geologic codes have a soil type\n\ndef Climatic2(row): \n    if (row['Soil_Type1'] == 1) or (row['Soil_Type2'] == 1) or (row['Soil_Type3'] == 1) or (row['Soil_Type4'] == 1) \\\n        or (row['Soil_Type5'] == 1) or (row['Soil_Type6'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic2'] = dftrain.apply (lambda row: Climatic2(row), axis=1)\ndftest['Climatic2'] = dftest.apply (lambda row: Climatic2(row), axis=1)\n\ndef Climatic3(row): \n    if (row['Soil_Type7'] == 1) or (row['Soil_Type8'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic3'] = dftrain.apply (lambda row: Climatic3(row), axis=1)\ndftest['Climatic3'] = dftest.apply (lambda row: Climatic3(row), axis=1)\n\ndef Climatic4(row): \n    if (row['Soil_Type9'] == 1) or (row['Soil_Type10'] == 1) or (row['Soil_Type11'] == 1) or (row['Soil_Type12'] == 1) \\\n        or (row['Soil_Type13'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic4'] = dftrain.apply (lambda row: Climatic4(row), axis=1)\ndftest['Climatic4'] = dftest.apply (lambda row: Climatic4(row), axis=1)\n\ndef Climatic5(row): \n    if (row['Soil_Type14'] == 1) or (row['Soil_Type15'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic5'] = dftrain.apply (lambda row: Climatic5(row), axis=1)\ndftest['Climatic5'] = dftest.apply (lambda row: Climatic5(row), axis=1)\n\ndef Climatic6(row): \n    if (row['Soil_Type16'] == 1) or (row['Soil_Type17'] == 1) or (row['Soil_Type18'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic6'] = dftrain.apply (lambda row: Climatic6(row), axis=1)\ndftest['Climatic6'] = dftest.apply (lambda row: Climatic6(row), axis=1)\n\ndef Climatic7(row): \n    if (row['Soil_Type19'] == 1) or (row['Soil_Type20'] == 1) or (row['Soil_Type21'] == 1) or (row['Soil_Type22'] == 1) \\\n        or (row['Soil_Type23'] == 1) or (row['Soil_Type24'] == 1) or (row['Soil_Type25'] == 1) or (row['Soil_Type26'] == 1) \\\n        or (row['Soil_Type27'] == 1) or (row['Soil_Type28'] == 1) or (row['Soil_Type29'] == 1) or (row['Soil_Type30'] == 1) \\\n        or (row['Soil_Type31'] == 1) or (row['Soil_Type32'] == 1) or (row['Soil_Type33'] == 1) or (row['Soil_Type34'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic7'] = dftrain.apply (lambda row: Climatic7(row), axis=1)\ndftest['Climatic7'] = dftest.apply (lambda row: Climatic7(row), axis=1)\n\ndef Climatic8(row): \n    if (row['Soil_Type35'] == 1) or (row['Soil_Type36'] == 1) or (row['Soil_Type37'] == 1) or (row['Soil_Type38'] == 1) \\\n        or (row['Soil_Type39'] == 1) or (row['Soil_Type40'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Climatic8'] = dftrain.apply (lambda row: Climatic8(row), axis=1)\ndftest['Climatic8'] = dftest.apply (lambda row: Climatic8(row), axis=1)\n\ndef Geologic1(row): \n    if (row['Soil_Type14'] == 1) or (row['Soil_Type15'] == 1) or (row['Soil_Type16'] == 1) or (row['Soil_Type17'] == 1) \\\n        or (row['Soil_Type19'] == 1) or (row['Soil_Type20'] == 1) or (row['Soil_Type21'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic1'] = dftrain.apply (lambda row: Geologic1(row), axis=1)\ndftest['Geologic1'] = dftest.apply (lambda row: Geologic1(row), axis=1)\n\ndef Geologic2(row): \n    if (row['Soil_Type9'] == 1) or (row['Soil_Type22'] == 1) or (row['Soil_Type23'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic2'] = dftrain.apply (lambda row: Geologic2(row), axis=1)\ndftest['Geologic2'] = dftest.apply (lambda row: Geologic2(row), axis=1)\n\ndef Geologic5(row): \n    if (row['Soil_Type7'] == 1) or (row['Soil_Type8'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic5'] = dftrain.apply (lambda row: Geologic5(row), axis=1)\ndftest['Geologic5'] = dftest.apply (lambda row: Geologic5(row), axis=1)\n\ndef Geologic7(row): \n    if (row['Soil_Type1'] == 1) or (row['Soil_Type2'] == 1) or (row['Soil_Type3'] == 1) or (row['Soil_Type4'] == 1) \\\n        or (row['Soil_Type5'] == 1) or (row['Soil_Type6'] == 1) or (row['Soil_Type10'] == 1) \\\n        or (row['Soil_Type11'] == 1) or (row['Soil_Type12'] == 1) or (row['Soil_Type13'] == 1) or (row['Soil_Type18'] == 1) \\\n        or (row['Soil_Type24'] == 1) or (row['Soil_Type25'] == 1) or (row['Soil_Type26'] == 1) or (row['Soil_Type27'] == 1) \\\n        or (row['Soil_Type28'] == 1) or (row['Soil_Type29'] == 1) or (row['Soil_Type30'] == 1) or (row['Soil_Type31'] == 1) \\\n        or (row['Soil_Type32'] == 1) or (row['Soil_Type33'] == 1) or (row['Soil_Type34'] == 1) or (row['Soil_Type35'] == 1) \\\n        or (row['Soil_Type36'] == 1) or (row['Soil_Type37'] == 1) or (row['Soil_Type38'] == 1) or (row['Soil_Type39'] == 1) \\\n        or (row['Soil_Type40'] == 1) :\n        return 1 \n    return 0\n\ndftrain['Geologic7'] = dftrain.apply (lambda row: Geologic7(row), axis=1)\ndftest['Geologic7'] = dftest.apply (lambda row: Geologic7(row), axis=1)","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"####### DATA PREPARATION #####\n#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)\n\n#force all types to float\nx = x.astype(float)\nx_predict = x_predict.astype(float)\n\n#normalize features\ndef normalize(feature):\n    min=x[feature].min()\n    min2=x_predict[feature].min()\n    if (min2 < min):\n        min=min2\n\n    max=x[feature].max()\n    max2=x_predict[feature].max()\n    if (max2 > max):\n        max=max2\n        \n    x[feature]=(x[feature]-min)/(max-min)                             \n    x_predict[feature]=(x_predict[feature]-min)/(max-min)  \n    \n    return                                \n\nnormalize(\"Elevation\")\nnormalize(\"Aspect\")\nnormalize(\"Slope\")\nnormalize(\"Horizontal_Distance_To_Hydrology\")\nnormalize(\"Vertical_Distance_To_Hydrology\")\nnormalize(\"Horizontal_Distance_To_Roadways\")\nnormalize(\"Hillshade_9am\")\nnormalize(\"Hillshade_Noon\")\nnormalize(\"Hillshade_3pm\")\nnormalize(\"Horizontal_Distance_To_Fire_Points\")\nnormalize(\"pca1\")\nnormalize(\"Euclidean_distance_to_hydro\")\nnormalize(\"distance_mean\")\nnormalize(\"distance_sum\")\nnormalize(\"distance_road_fire\")\nnormalize(\"distance_hydro_fire\")\nnormalize(\"distance_road_hydro\")\nnormalize(\"distance_sum_road_fire\")\nnormalize(\"distance_sum_hydro_fire\")\nnormalize(\"distance_sum_road_hydro\")\nnormalize(\"distance_dif_road_fire\")\nnormalize(\"distance_dif_hydro_road\")\nnormalize(\"distance_dif_hydro_fire\")\nnormalize(\"Vertical_dif\")\nnormalize(\"Vertical_sum\")\nnormalize(\"shade_noon_diff\")\nnormalize(\"shade_3pm_diff\")\nnormalize(\"shade_all_diff\")\nnormalize(\"shade_sum\")\nnormalize(\"shade_mean\")\nnormalize(\"ElevationHydro\")\nnormalize(\"ElevationV\")\nnormalize(\"ElevationH\")\nnormalize(\"Elevation2\")\nnormalize(\"ElevationLog\")\nnormalize(\"Aspect_cos\")\nnormalize(\"Aspect_sin\")\nnormalize(\"Aspectcos_Slope\")","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the label to One Hot Encoding\nnum_classes = 7\n\n#to_categorical requires 0..6 instead of 1..7\ny -=1\ny = y.to_numpy()\n\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y, num_classes)\n\n#validate data - no rows with all zeros\n#x.index[x.eq(0).all(1)]\nprint(x[x.eq(0).all(1)].empty)\nprint(x_predict[x_predict.eq(0).all(1)].empty)\n\n#convert the features dataframes to numpy arrays\nx = x.to_numpy()\nx_predict = x_predict.to_numpy()\n\n#split in train (80%) and test (20%) sets \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2, stratify=y)","execution_count":32,"outputs":[{"output_type":"stream","text":"True\nTrue\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here is the NN model\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\n\nnum_features = x_train.shape[1]\n\nmodel = Sequential()\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal', input_dim=num_features))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='Adam',\n              metrics=['accuracy'])\n\n#train the model\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=250)\n\n# Predict!!\ny_predict = model.predict(x_predict)","execution_count":33,"outputs":[{"output_type":"stream","text":"Train on 12096 samples, validate on 3024 samples\nEpoch 1/250\n12096/12096 [==============================] - 1s 121us/sample - loss: 0.9482 - acc: 0.6128 - val_loss: 0.7841 - val_acc: 0.6693\nEpoch 2/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.7614 - acc: 0.6771 - val_loss: 0.7179 - val_acc: 0.6852\nEpoch 3/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.7077 - acc: 0.6954 - val_loss: 0.7062 - val_acc: 0.6954\nEpoch 4/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.6900 - acc: 0.7053 - val_loss: 0.6611 - val_acc: 0.7090\nEpoch 5/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.6640 - acc: 0.7150 - val_loss: 0.6519 - val_acc: 0.7183\nEpoch 6/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.6466 - acc: 0.7242 - val_loss: 0.6301 - val_acc: 0.7239\nEpoch 7/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.6277 - acc: 0.7297 - val_loss: 0.6320 - val_acc: 0.7212\nEpoch 8/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.6145 - acc: 0.7363 - val_loss: 0.6017 - val_acc: 0.7427\nEpoch 9/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.5999 - acc: 0.7460 - val_loss: 0.5935 - val_acc: 0.7530\nEpoch 10/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.5922 - acc: 0.7427 - val_loss: 0.5831 - val_acc: 0.7437\nEpoch 11/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.5798 - acc: 0.7508 - val_loss: 0.5728 - val_acc: 0.7616\nEpoch 12/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.5685 - acc: 0.7583 - val_loss: 0.5485 - val_acc: 0.7583\nEpoch 13/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.5524 - acc: 0.7682 - val_loss: 0.5905 - val_acc: 0.7440\nEpoch 14/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.5478 - acc: 0.7667 - val_loss: 0.5513 - val_acc: 0.7583\nEpoch 15/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.5413 - acc: 0.7698 - val_loss: 0.5850 - val_acc: 0.7513\nEpoch 16/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.5284 - acc: 0.7768 - val_loss: 0.5166 - val_acc: 0.7817\nEpoch 17/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.5227 - acc: 0.7785 - val_loss: 0.5133 - val_acc: 0.7781\nEpoch 18/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.5147 - acc: 0.7870 - val_loss: 0.5178 - val_acc: 0.7771\nEpoch 19/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.5030 - acc: 0.7889 - val_loss: 0.5236 - val_acc: 0.7801\nEpoch 20/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.4956 - acc: 0.7925 - val_loss: 0.5107 - val_acc: 0.7877\nEpoch 21/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.4914 - acc: 0.7940 - val_loss: 0.5068 - val_acc: 0.7824\nEpoch 22/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.4873 - acc: 0.7961 - val_loss: 0.5188 - val_acc: 0.7817\nEpoch 23/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.4815 - acc: 0.8013 - val_loss: 0.4919 - val_acc: 0.7963\nEpoch 24/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.4738 - acc: 0.8018 - val_loss: 0.4968 - val_acc: 0.7963\nEpoch 25/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.4654 - acc: 0.8079 - val_loss: 0.5423 - val_acc: 0.7748\nEpoch 26/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.4638 - acc: 0.8087 - val_loss: 0.4729 - val_acc: 0.8046\nEpoch 27/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.4570 - acc: 0.8104 - val_loss: 0.4871 - val_acc: 0.7937\nEpoch 28/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.4565 - acc: 0.8090 - val_loss: 0.4743 - val_acc: 0.8049\nEpoch 29/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.4455 - acc: 0.8171 - val_loss: 0.4739 - val_acc: 0.8032\nEpoch 30/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.4454 - acc: 0.8185 - val_loss: 0.4659 - val_acc: 0.8036\nEpoch 31/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.4372 - acc: 0.8200 - val_loss: 0.4632 - val_acc: 0.7999\nEpoch 32/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.4309 - acc: 0.8240 - val_loss: 0.4567 - val_acc: 0.8099\nEpoch 33/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.4253 - acc: 0.8211 - val_loss: 0.4800 - val_acc: 0.8036\nEpoch 34/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.4228 - acc: 0.8256 - val_loss: 0.4439 - val_acc: 0.8138\nEpoch 35/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.4171 - acc: 0.8286 - val_loss: 0.4456 - val_acc: 0.8168\nEpoch 36/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.4150 - acc: 0.8305 - val_loss: 0.4359 - val_acc: 0.8214\nEpoch 37/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.4117 - acc: 0.8316 - val_loss: 0.4553 - val_acc: 0.8115\nEpoch 38/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.4068 - acc: 0.8355 - val_loss: 0.4632 - val_acc: 0.8036\nEpoch 39/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.4012 - acc: 0.8376 - val_loss: 0.4491 - val_acc: 0.8191\nEpoch 40/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.3946 - acc: 0.8399 - val_loss: 0.4520 - val_acc: 0.8171\nEpoch 41/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.3944 - acc: 0.8355 - val_loss: 0.4406 - val_acc: 0.8307\nEpoch 42/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.3985 - acc: 0.8379 - val_loss: 0.4439 - val_acc: 0.8287\nEpoch 43/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.3877 - acc: 0.8426 - val_loss: 0.4341 - val_acc: 0.8271\nEpoch 44/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.3864 - acc: 0.8432 - val_loss: 0.4470 - val_acc: 0.8241\nEpoch 45/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.3779 - acc: 0.8477 - val_loss: 0.4331 - val_acc: 0.8280\nEpoch 46/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.3780 - acc: 0.8474 - val_loss: 0.4430 - val_acc: 0.8254\nEpoch 47/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.3754 - acc: 0.8509 - val_loss: 0.4371 - val_acc: 0.8231\nEpoch 48/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.3749 - acc: 0.8494 - val_loss: 0.4405 - val_acc: 0.8211\nEpoch 49/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.3656 - acc: 0.8514 - val_loss: 0.4233 - val_acc: 0.8310\nEpoch 50/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.3685 - acc: 0.8484 - val_loss: 0.4202 - val_acc: 0.8323\nEpoch 51/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.3602 - acc: 0.8518 - val_loss: 0.4299 - val_acc: 0.8290\nEpoch 52/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.3591 - acc: 0.8522 - val_loss: 0.4215 - val_acc: 0.8313\nEpoch 53/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.3561 - acc: 0.8555 - val_loss: 0.4370 - val_acc: 0.8297\nEpoch 54/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.3519 - acc: 0.8570 - val_loss: 0.4179 - val_acc: 0.8376\nEpoch 55/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.3494 - acc: 0.8575 - val_loss: 0.4603 - val_acc: 0.8224\nEpoch 56/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.3539 - acc: 0.8558 - val_loss: 0.4215 - val_acc: 0.8373\nEpoch 57/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.3476 - acc: 0.8579 - val_loss: 0.4199 - val_acc: 0.8347\nEpoch 58/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 1s 98us/sample - loss: 0.3433 - acc: 0.8568 - val_loss: 0.4223 - val_acc: 0.8366\nEpoch 59/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.3441 - acc: 0.8597 - val_loss: 0.4333 - val_acc: 0.8380\nEpoch 60/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3405 - acc: 0.8623 - val_loss: 0.4134 - val_acc: 0.8386\nEpoch 61/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.3305 - acc: 0.8676 - val_loss: 0.4166 - val_acc: 0.8356\nEpoch 62/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.3379 - acc: 0.8614 - val_loss: 0.4125 - val_acc: 0.8390\nEpoch 63/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.3245 - acc: 0.8671 - val_loss: 0.4076 - val_acc: 0.8406\nEpoch 64/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.3260 - acc: 0.8646 - val_loss: 0.4118 - val_acc: 0.8466\nEpoch 65/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.3201 - acc: 0.8706 - val_loss: 0.4451 - val_acc: 0.8350\nEpoch 66/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3252 - acc: 0.8682 - val_loss: 0.4174 - val_acc: 0.8360\nEpoch 67/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.3249 - acc: 0.8683 - val_loss: 0.4044 - val_acc: 0.8439\nEpoch 68/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.3248 - acc: 0.8667 - val_loss: 0.4134 - val_acc: 0.8383\nEpoch 69/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.3209 - acc: 0.8688 - val_loss: 0.4205 - val_acc: 0.8452\nEpoch 70/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.3183 - acc: 0.8730 - val_loss: 0.4199 - val_acc: 0.8390\nEpoch 71/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.3149 - acc: 0.8709 - val_loss: 0.4119 - val_acc: 0.8446\nEpoch 72/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.3102 - acc: 0.8730 - val_loss: 0.3958 - val_acc: 0.8558\nEpoch 73/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.3086 - acc: 0.8752 - val_loss: 0.4095 - val_acc: 0.8469\nEpoch 74/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3075 - acc: 0.8754 - val_loss: 0.4156 - val_acc: 0.8426\nEpoch 75/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.3076 - acc: 0.8748 - val_loss: 0.4248 - val_acc: 0.8433\nEpoch 76/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3095 - acc: 0.8755 - val_loss: 0.4055 - val_acc: 0.8522\nEpoch 77/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2995 - acc: 0.8790 - val_loss: 0.4212 - val_acc: 0.8409\nEpoch 78/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2976 - acc: 0.8823 - val_loss: 0.4197 - val_acc: 0.8442\nEpoch 79/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2982 - acc: 0.8795 - val_loss: 0.4098 - val_acc: 0.8456\nEpoch 80/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2998 - acc: 0.8798 - val_loss: 0.4212 - val_acc: 0.8433\nEpoch 81/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2953 - acc: 0.8791 - val_loss: 0.4177 - val_acc: 0.8466\nEpoch 82/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2964 - acc: 0.8791 - val_loss: 0.4158 - val_acc: 0.8515\nEpoch 83/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2912 - acc: 0.8839 - val_loss: 0.4121 - val_acc: 0.8476\nEpoch 84/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2934 - acc: 0.8819 - val_loss: 0.4220 - val_acc: 0.8462\nEpoch 85/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2929 - acc: 0.8810 - val_loss: 0.4107 - val_acc: 0.8476\nEpoch 86/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2932 - acc: 0.8833 - val_loss: 0.4019 - val_acc: 0.8538\nEpoch 87/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2922 - acc: 0.8825 - val_loss: 0.4313 - val_acc: 0.8446\nEpoch 88/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2868 - acc: 0.8839 - val_loss: 0.4106 - val_acc: 0.8456\nEpoch 89/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2848 - acc: 0.8854 - val_loss: 0.4029 - val_acc: 0.8585\nEpoch 90/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2835 - acc: 0.8841 - val_loss: 0.4097 - val_acc: 0.8499\nEpoch 91/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2772 - acc: 0.8893 - val_loss: 0.4098 - val_acc: 0.8555\nEpoch 92/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2746 - acc: 0.8900 - val_loss: 0.4214 - val_acc: 0.8512\nEpoch 93/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2774 - acc: 0.8892 - val_loss: 0.4198 - val_acc: 0.8413\nEpoch 94/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2743 - acc: 0.8888 - val_loss: 0.4010 - val_acc: 0.8528\nEpoch 95/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2768 - acc: 0.8891 - val_loss: 0.4075 - val_acc: 0.8512\nEpoch 96/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2717 - acc: 0.8893 - val_loss: 0.4126 - val_acc: 0.8456\nEpoch 97/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.2700 - acc: 0.8887 - val_loss: 0.4028 - val_acc: 0.8565\nEpoch 98/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2700 - acc: 0.8928 - val_loss: 0.4054 - val_acc: 0.8548\nEpoch 99/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2722 - acc: 0.8910 - val_loss: 0.4089 - val_acc: 0.8558\nEpoch 100/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2708 - acc: 0.8889 - val_loss: 0.4180 - val_acc: 0.8505\nEpoch 101/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2654 - acc: 0.8924 - val_loss: 0.4202 - val_acc: 0.8476\nEpoch 102/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2689 - acc: 0.8950 - val_loss: 0.4074 - val_acc: 0.8535\nEpoch 103/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2665 - acc: 0.8914 - val_loss: 0.4182 - val_acc: 0.8502\nEpoch 104/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2638 - acc: 0.8907 - val_loss: 0.4010 - val_acc: 0.8595\nEpoch 105/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2619 - acc: 0.8979 - val_loss: 0.4120 - val_acc: 0.8538\nEpoch 106/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2569 - acc: 0.8973 - val_loss: 0.4045 - val_acc: 0.8571\nEpoch 107/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2614 - acc: 0.8967 - val_loss: 0.4135 - val_acc: 0.8519\nEpoch 108/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2589 - acc: 0.8950 - val_loss: 0.4237 - val_acc: 0.8495\nEpoch 109/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2593 - acc: 0.8960 - val_loss: 0.4112 - val_acc: 0.8578\nEpoch 110/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2584 - acc: 0.8952 - val_loss: 0.4041 - val_acc: 0.8525\nEpoch 111/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2550 - acc: 0.8948 - val_loss: 0.4080 - val_acc: 0.8575\nEpoch 112/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2477 - acc: 0.9000 - val_loss: 0.4086 - val_acc: 0.8578\nEpoch 113/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2574 - acc: 0.8953 - val_loss: 0.4096 - val_acc: 0.8598\nEpoch 114/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2515 - acc: 0.8991 - val_loss: 0.4151 - val_acc: 0.8532\nEpoch 115/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.2504 - acc: 0.8983 - val_loss: 0.4160 - val_acc: 0.8495\n","name":"stdout"},{"output_type":"stream","text":"Epoch 116/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2528 - acc: 0.9001 - val_loss: 0.4129 - val_acc: 0.8528\nEpoch 117/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2500 - acc: 0.8971 - val_loss: 0.4071 - val_acc: 0.8657\nEpoch 118/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2502 - acc: 0.8985 - val_loss: 0.4226 - val_acc: 0.8568\nEpoch 119/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.2484 - acc: 0.9018 - val_loss: 0.3969 - val_acc: 0.8578\nEpoch 120/250\n12096/12096 [==============================] - 1s 111us/sample - loss: 0.2445 - acc: 0.9020 - val_loss: 0.4229 - val_acc: 0.8522\nEpoch 121/250\n12096/12096 [==============================] - 1s 111us/sample - loss: 0.2446 - acc: 0.9020 - val_loss: 0.4164 - val_acc: 0.8532\nEpoch 122/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.2401 - acc: 0.9033 - val_loss: 0.4157 - val_acc: 0.8558\nEpoch 123/250\n12096/12096 [==============================] - 1s 112us/sample - loss: 0.2461 - acc: 0.9014 - val_loss: 0.4210 - val_acc: 0.8591\nEpoch 124/250\n12096/12096 [==============================] - 1s 112us/sample - loss: 0.2397 - acc: 0.9030 - val_loss: 0.4124 - val_acc: 0.8588\nEpoch 125/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.2434 - acc: 0.9023 - val_loss: 0.4050 - val_acc: 0.8519\nEpoch 126/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.2384 - acc: 0.9035 - val_loss: 0.4156 - val_acc: 0.8585\nEpoch 127/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2373 - acc: 0.9039 - val_loss: 0.4117 - val_acc: 0.8591\nEpoch 128/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2355 - acc: 0.9057 - val_loss: 0.4227 - val_acc: 0.8611\nEpoch 129/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2430 - acc: 0.9002 - val_loss: 0.4224 - val_acc: 0.8499\nEpoch 130/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2344 - acc: 0.9061 - val_loss: 0.4038 - val_acc: 0.8624\nEpoch 131/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2305 - acc: 0.9051 - val_loss: 0.4270 - val_acc: 0.8512\nEpoch 132/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2367 - acc: 0.9062 - val_loss: 0.4126 - val_acc: 0.8588\nEpoch 133/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2333 - acc: 0.9062 - val_loss: 0.4118 - val_acc: 0.8578\nEpoch 134/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2263 - acc: 0.9073 - val_loss: 0.4143 - val_acc: 0.8591\nEpoch 135/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2294 - acc: 0.9061 - val_loss: 0.4255 - val_acc: 0.8595\nEpoch 136/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2324 - acc: 0.9048 - val_loss: 0.4105 - val_acc: 0.8608\nEpoch 137/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2272 - acc: 0.9091 - val_loss: 0.4157 - val_acc: 0.8647\nEpoch 138/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2274 - acc: 0.9117 - val_loss: 0.4039 - val_acc: 0.8595\nEpoch 139/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2250 - acc: 0.9126 - val_loss: 0.4199 - val_acc: 0.8552\nEpoch 140/250\n12096/12096 [==============================] - 1s 122us/sample - loss: 0.2245 - acc: 0.9119 - val_loss: 0.4071 - val_acc: 0.8578\nEpoch 141/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2308 - acc: 0.9094 - val_loss: 0.4265 - val_acc: 0.8611\nEpoch 142/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2217 - acc: 0.9123 - val_loss: 0.4209 - val_acc: 0.8614\nEpoch 143/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2189 - acc: 0.9128 - val_loss: 0.4071 - val_acc: 0.8624\nEpoch 144/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.2225 - acc: 0.9118 - val_loss: 0.4304 - val_acc: 0.8624\nEpoch 145/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2246 - acc: 0.9101 - val_loss: 0.4280 - val_acc: 0.8621\nEpoch 146/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2188 - acc: 0.9105 - val_loss: 0.4182 - val_acc: 0.8631\nEpoch 147/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.2185 - acc: 0.9142 - val_loss: 0.4258 - val_acc: 0.8614\nEpoch 148/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2209 - acc: 0.9126 - val_loss: 0.4304 - val_acc: 0.8611\nEpoch 149/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2176 - acc: 0.9137 - val_loss: 0.4329 - val_acc: 0.8634\nEpoch 150/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2175 - acc: 0.9112 - val_loss: 0.4308 - val_acc: 0.8532\nEpoch 151/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2191 - acc: 0.9110 - val_loss: 0.4401 - val_acc: 0.8604\nEpoch 152/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2171 - acc: 0.9135 - val_loss: 0.4391 - val_acc: 0.8515\nEpoch 153/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2193 - acc: 0.9120 - val_loss: 0.4253 - val_acc: 0.8634\nEpoch 154/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2134 - acc: 0.9167 - val_loss: 0.4451 - val_acc: 0.8631\nEpoch 155/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2112 - acc: 0.9160 - val_loss: 0.4302 - val_acc: 0.8661\nEpoch 156/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2128 - acc: 0.9151 - val_loss: 0.4202 - val_acc: 0.8638\nEpoch 157/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2101 - acc: 0.9152 - val_loss: 0.4491 - val_acc: 0.8588\nEpoch 158/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2086 - acc: 0.9182 - val_loss: 0.4303 - val_acc: 0.8571\nEpoch 159/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2141 - acc: 0.9167 - val_loss: 0.4340 - val_acc: 0.8562\nEpoch 160/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.2093 - acc: 0.9180 - val_loss: 0.4318 - val_acc: 0.8595\nEpoch 161/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2058 - acc: 0.9161 - val_loss: 0.4342 - val_acc: 0.8651\nEpoch 162/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2098 - acc: 0.9160 - val_loss: 0.4155 - val_acc: 0.8631\nEpoch 163/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2090 - acc: 0.9162 - val_loss: 0.4254 - val_acc: 0.8641\nEpoch 164/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2021 - acc: 0.9185 - val_loss: 0.4415 - val_acc: 0.8581\nEpoch 165/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2067 - acc: 0.9167 - val_loss: 0.4403 - val_acc: 0.8618\nEpoch 166/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2054 - acc: 0.9172 - val_loss: 0.4250 - val_acc: 0.8644\nEpoch 167/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2061 - acc: 0.9167 - val_loss: 0.4382 - val_acc: 0.8647\nEpoch 168/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.2080 - acc: 0.9158 - val_loss: 0.4288 - val_acc: 0.8598\nEpoch 169/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.2012 - acc: 0.9189 - val_loss: 0.4285 - val_acc: 0.8618\nEpoch 170/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2093 - acc: 0.9193 - val_loss: 0.4197 - val_acc: 0.8618\nEpoch 171/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2072 - acc: 0.9159 - val_loss: 0.4324 - val_acc: 0.8657\nEpoch 172/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2020 - acc: 0.9179 - val_loss: 0.4268 - val_acc: 0.8581\nEpoch 173/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 1s 98us/sample - loss: 0.2016 - acc: 0.9181 - val_loss: 0.4333 - val_acc: 0.8585\nEpoch 174/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1950 - acc: 0.9210 - val_loss: 0.4465 - val_acc: 0.8578\nEpoch 175/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1958 - acc: 0.9193 - val_loss: 0.4339 - val_acc: 0.8611\nEpoch 176/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2005 - acc: 0.9210 - val_loss: 0.4513 - val_acc: 0.8578\nEpoch 177/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.2041 - acc: 0.9203 - val_loss: 0.4531 - val_acc: 0.8552\nEpoch 178/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1907 - acc: 0.9242 - val_loss: 0.4423 - val_acc: 0.8585\nEpoch 179/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1919 - acc: 0.9215 - val_loss: 0.4419 - val_acc: 0.8694\nEpoch 180/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1941 - acc: 0.9242 - val_loss: 0.4410 - val_acc: 0.8664\nEpoch 181/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1982 - acc: 0.9196 - val_loss: 0.4444 - val_acc: 0.8558\nEpoch 182/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.1890 - acc: 0.9245 - val_loss: 0.4392 - val_acc: 0.8638\nEpoch 183/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1916 - acc: 0.9221 - val_loss: 0.4348 - val_acc: 0.8707\nEpoch 184/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1956 - acc: 0.9227 - val_loss: 0.4447 - val_acc: 0.8687\nEpoch 185/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1926 - acc: 0.9234 - val_loss: 0.4463 - val_acc: 0.8687\nEpoch 186/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1905 - acc: 0.9265 - val_loss: 0.4469 - val_acc: 0.8601\nEpoch 187/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1936 - acc: 0.9224 - val_loss: 0.4461 - val_acc: 0.8585\nEpoch 188/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1899 - acc: 0.9243 - val_loss: 0.4430 - val_acc: 0.8624\nEpoch 189/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.1881 - acc: 0.9236 - val_loss: 0.4541 - val_acc: 0.8644\nEpoch 190/250\n12096/12096 [==============================] - 1s 115us/sample - loss: 0.1949 - acc: 0.9208 - val_loss: 0.4539 - val_acc: 0.8608\nEpoch 191/250\n12096/12096 [==============================] - 1s 113us/sample - loss: 0.1888 - acc: 0.9253 - val_loss: 0.4479 - val_acc: 0.8661\nEpoch 192/250\n12096/12096 [==============================] - 1s 111us/sample - loss: 0.1883 - acc: 0.9239 - val_loss: 0.4397 - val_acc: 0.8661\nEpoch 193/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.1909 - acc: 0.9240 - val_loss: 0.4568 - val_acc: 0.8608\nEpoch 194/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1838 - acc: 0.9272 - val_loss: 0.4624 - val_acc: 0.8578\nEpoch 195/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1928 - acc: 0.9220 - val_loss: 0.4559 - val_acc: 0.8621\nEpoch 196/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1962 - acc: 0.9215 - val_loss: 0.4648 - val_acc: 0.8611\nEpoch 197/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1858 - acc: 0.9264 - val_loss: 0.4534 - val_acc: 0.8684\nEpoch 198/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1852 - acc: 0.9260 - val_loss: 0.4527 - val_acc: 0.8621\nEpoch 199/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1798 - acc: 0.9289 - val_loss: 0.4450 - val_acc: 0.8634\nEpoch 200/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.1857 - acc: 0.9250 - val_loss: 0.4619 - val_acc: 0.8608\nEpoch 201/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.1849 - acc: 0.9258 - val_loss: 0.4594 - val_acc: 0.8575\nEpoch 202/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1819 - acc: 0.9286 - val_loss: 0.4570 - val_acc: 0.8634\nEpoch 203/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1829 - acc: 0.9248 - val_loss: 0.4532 - val_acc: 0.8671\nEpoch 204/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1859 - acc: 0.9266 - val_loss: 0.4785 - val_acc: 0.8595\nEpoch 205/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1820 - acc: 0.9287 - val_loss: 0.4531 - val_acc: 0.8614\nEpoch 206/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1843 - acc: 0.9235 - val_loss: 0.4690 - val_acc: 0.8674\nEpoch 207/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1762 - acc: 0.9301 - val_loss: 0.4536 - val_acc: 0.8595\nEpoch 208/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.1870 - acc: 0.9261 - val_loss: 0.4709 - val_acc: 0.8621\nEpoch 209/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1855 - acc: 0.9280 - val_loss: 0.4518 - val_acc: 0.8628\nEpoch 210/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1780 - acc: 0.9302 - val_loss: 0.4682 - val_acc: 0.8638\nEpoch 211/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1835 - acc: 0.9283 - val_loss: 0.4609 - val_acc: 0.8667\nEpoch 212/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1759 - acc: 0.9287 - val_loss: 0.4580 - val_acc: 0.8717\nEpoch 213/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1776 - acc: 0.9301 - val_loss: 0.4690 - val_acc: 0.8624\nEpoch 214/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1734 - acc: 0.9305 - val_loss: 0.4580 - val_acc: 0.8654\nEpoch 215/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1806 - acc: 0.9284 - val_loss: 0.4673 - val_acc: 0.8687\nEpoch 216/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1763 - acc: 0.9303 - val_loss: 0.4731 - val_acc: 0.8651\nEpoch 217/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1788 - acc: 0.9277 - val_loss: 0.4467 - val_acc: 0.8657\nEpoch 218/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1794 - acc: 0.9302 - val_loss: 0.4729 - val_acc: 0.8674\nEpoch 219/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1747 - acc: 0.9325 - val_loss: 0.4590 - val_acc: 0.8631\nEpoch 220/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1831 - acc: 0.9258 - val_loss: 0.4450 - val_acc: 0.8681\nEpoch 221/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.1759 - acc: 0.9283 - val_loss: 0.4536 - val_acc: 0.8651\nEpoch 222/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1666 - acc: 0.9338 - val_loss: 0.4656 - val_acc: 0.8740\nEpoch 223/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1775 - acc: 0.9301 - val_loss: 0.4584 - val_acc: 0.8707\nEpoch 224/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1799 - acc: 0.9303 - val_loss: 0.4573 - val_acc: 0.8704\nEpoch 225/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1666 - acc: 0.9347 - val_loss: 0.4649 - val_acc: 0.8704\nEpoch 226/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1814 - acc: 0.9296 - val_loss: 0.4840 - val_acc: 0.8562\nEpoch 227/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1791 - acc: 0.9274 - val_loss: 0.4705 - val_acc: 0.8651\nEpoch 228/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1713 - acc: 0.9349 - val_loss: 0.4856 - val_acc: 0.8657\nEpoch 229/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1698 - acc: 0.9341 - val_loss: 0.4598 - val_acc: 0.8677\nEpoch 230/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1755 - acc: 0.9291 - val_loss: 0.4666 - val_acc: 0.8684\n","name":"stdout"},{"output_type":"stream","text":"Epoch 231/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1689 - acc: 0.9314 - val_loss: 0.4661 - val_acc: 0.8707\nEpoch 232/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1672 - acc: 0.9385 - val_loss: 0.4668 - val_acc: 0.8667\nEpoch 233/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1635 - acc: 0.9332 - val_loss: 0.4700 - val_acc: 0.8684\nEpoch 234/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1677 - acc: 0.9348 - val_loss: 0.4859 - val_acc: 0.8661\nEpoch 235/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1719 - acc: 0.9324 - val_loss: 0.4896 - val_acc: 0.8667\nEpoch 236/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1668 - acc: 0.9325 - val_loss: 0.4805 - val_acc: 0.8684\nEpoch 237/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1671 - acc: 0.9346 - val_loss: 0.4645 - val_acc: 0.8647\nEpoch 238/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1709 - acc: 0.9332 - val_loss: 0.4874 - val_acc: 0.8571\nEpoch 239/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1720 - acc: 0.9322 - val_loss: 0.4762 - val_acc: 0.8664\nEpoch 240/250\n12096/12096 [==============================] - ETA: 0s - loss: 0.1683 - acc: 0.934 - 1s 102us/sample - loss: 0.1684 - acc: 0.9342 - val_loss: 0.4850 - val_acc: 0.8647\nEpoch 241/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1684 - acc: 0.9333 - val_loss: 0.4745 - val_acc: 0.8684\nEpoch 242/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1587 - acc: 0.9373 - val_loss: 0.4919 - val_acc: 0.8737\nEpoch 243/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1659 - acc: 0.9348 - val_loss: 0.5111 - val_acc: 0.8608\nEpoch 244/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1622 - acc: 0.9362 - val_loss: 0.4824 - val_acc: 0.8644\nEpoch 245/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1693 - acc: 0.9310 - val_loss: 0.4987 - val_acc: 0.8674\nEpoch 246/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1642 - acc: 0.9344 - val_loss: 0.4873 - val_acc: 0.8733\nEpoch 247/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1669 - acc: 0.9328 - val_loss: 0.4968 - val_acc: 0.8654\nEpoch 248/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1629 - acc: 0.9360 - val_loss: 0.4875 - val_acc: 0.8654\nEpoch 249/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1663 - acc: 0.9334 - val_loss: 0.4890 - val_acc: 0.8671\nEpoch 250/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1600 - acc: 0.9382 - val_loss: 0.4902 - val_acc: 0.8657\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n\tprint(y_predict[i], np.argmax(y_predict[i])+1)","execution_count":34,"outputs":[{"output_type":"stream","text":"[3.2264892e-02 9.6772856e-01 1.2379054e-13 3.2390021e-15 6.6117918e-06\n 2.5086948e-34 0.0000000e+00] 2\n[9.5439595e-01 4.5600817e-02 1.6316273e-12 2.8679335e-17 3.2157147e-06\n 2.0724690e-38 0.0000000e+00] 1\n[9.3185419e-01 6.8143897e-02 3.3286869e-12 4.3126307e-16 1.8717188e-06\n 4.5127945e-38 8.4102675e-36] 1\n[8.83472085e-01 1.16524093e-01 5.28255217e-12 1.54925301e-15\n 3.81575637e-06 9.50608592e-38 1.02279605e-36] 1\n[8.2470053e-01 1.7529199e-01 7.7335126e-12 4.0034034e-15 7.5105409e-06\n 2.0462063e-37 1.6086363e-37] 1\n[9.6126401e-01 3.8733765e-02 1.8594516e-12 3.2274433e-16 2.1702303e-06\n 1.6154307e-38 2.4310878e-38] 1\n[9.7336459e-01 2.6633805e-02 1.3550063e-12 9.6200219e-17 1.6466641e-06\n 0.0000000e+00 5.2041282e-38] 1\n[9.7652721e-01 2.3471422e-02 1.0288696e-12 4.9182710e-17 1.4202045e-06\n 0.0000000e+00 5.4119402e-38] 1\n[9.3059081e-01 6.9406077e-02 2.5706438e-12 1.4416841e-16 3.0576716e-06\n 0.0000000e+00 2.8563250e-38] 1\n[9.5264977e-01 4.7347430e-02 1.7913663e-12 7.7697234e-17 2.8940665e-06\n 0.0000000e+00 1.2480711e-38] 1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save predictions to a file for submission\n#argmax give us the highest probable label\n# we add one to the predictions to scale from 0..6 to 1..7\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict.argmax(axis=1)+1})\noutput.to_csv('submission.csv', index=False)\n\n#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}