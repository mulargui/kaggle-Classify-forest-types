{"cells":[{"metadata":{},"cell_type":"markdown","source":"This is my first experiment in this competition. Whereas XGBoost is highly recommended I rather tried to see how far I can go with an NN (using Keras).\n\nThis is the basic model and with 250 epochs has an accuracy of 80% (really poor).\n\nI'll continue for a few days researching how much I can optimize this model."},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(os.listdir(\"../input/\"))\nprint(os.listdir(\"../working/\"))\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['learn-together']\n['.ipynb_checkpoints', '__notebook_source__.ipynb']\n/kaggle/input/learn-together/test.csv\n/kaggle/input/learn-together/train.csv\n/kaggle/input/learn-together/sample_submission.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#load data, I had an issue with the data and hacked the data into the kernel\n#dftrain=pd.read_csv('/kaggle/input/train.csv')\n#dftest=pd.read_csv('/kaggle/input/test.csv')\ndftrain=pd.read_csv('/kaggle/input/learn-together/train.csv')\ndftest=pd.read_csv('/kaggle/input/learn-together/test.csv')","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#file shape\nprint(dftrain.head())\nprint(dftrain.shape[0])\nprint(dftest.head())\nprint(dftest.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#features types\nprint(dftrain.dtypes)\nprint(dftest.dtypes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate files - nans per feature\nprint(dftrain.isnull().sum(axis = 0))\nprint(dftest.isnull().sum(axis = 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate files - no rows with all zeros\nprint(dftrain[dftrain.drop(['Id','Cover_Type'], axis=1).eq(0).all(1)].empty)\nprint(dftest[dftest.drop('Id', axis=1).eq(0).all(1)].empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split train data in features and labels\ny = dftrain.Cover_Type\nx = dftrain.drop(['Id','Cover_Type'], axis=1)\n\n# split test data in features and Ids\nIds = dftest.Id\nx_predict = dftest.drop('Id', axis=1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#force all types to float\nx = x.astype(float)\nx_predict = x_predict.astype(float)","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#normalize features\n#in the future it can be done more elegantly, for now just using the max min values of the data that we know\n#x['Elevation']=(x['Elevation']-x['Elevation'].min())/(x['Elevation'].max()-x['Elevation'].min())                             \nx['Elevation']=(x['Elevation']-1859)/(3858-1859)                             \nx['Aspect']=x['Aspect']/360                      \nx['Slope']=x['Slope']/66                      \nx['Horizontal_Distance_To_Hydrology']=x['Horizontal_Distance_To_Hydrology']/1397                      \nx['Vertical_Distance_To_Hydrology']=(x['Vertical_Distance_To_Hydrology']+173)/(601+173)                             \nx['Horizontal_Distance_To_Roadways']=x['Horizontal_Distance_To_Roadways']/7117                      \nx['Hillshade_9am']=x['Hillshade_9am']/254                      \nx['Hillshade_Noon']=x['Hillshade_Noon']/254                      \nx['Hillshade_3pm']=x['Hillshade_3pm']/254                      \nx['Horizontal_Distance_To_Fire_Points']=x['Horizontal_Distance_To_Fire_Points']/67173                      \n                                \nx_predict['Elevation']=(x_predict['Elevation']-1859)/(3858-1859)                             \nx_predict['Aspect']=x_predict['Aspect']/360                      \nx_predict['Slope']=x_predict['Slope']/66                      \nx_predict['Horizontal_Distance_To_Hydrology']=x_predict['Horizontal_Distance_To_Hydrology']/1397                      \nx_predict['Vertical_Distance_To_Hydrology']=(x_predict['Vertical_Distance_To_Hydrology']+173)/(601+173)                             \nx_predict['Horizontal_Distance_To_Roadways']=x_predict['Horizontal_Distance_To_Roadways']/7117                      \nx_predict['Hillshade_9am']=x_predict['Hillshade_9am']/254                      \nx_predict['Hillshade_Noon']=x_predict['Hillshade_Noon']/254                      \nx_predict['Hillshade_3pm']=x_predict['Hillshade_3pm']/254                      \nx_predict['Horizontal_Distance_To_Fire_Points']=x_predict['Horizontal_Distance_To_Fire_Points']/67173                      ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#validate data - no rows with all zeros\n#x.index[x.eq(0).all(1)]\nprint(x[x.eq(0).all(1)].empty)\nprint(x_predict[x_predict.eq(0).all(1)].empty)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert the label to One Hot Encoding\n#to_categorical requires 0..6 instead of 1..7\ny -=1\ny = y.to_numpy()\n\nnum_classes = 7\n\nfrom tensorflow.keras.utils import to_categorical\ny = to_categorical(y, num_classes)","execution_count":6,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#convert the features dataframes to numpy arrays\nx = x.to_numpy()\nx_predict = x_predict.to_numpy()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#split in train (80%) and test (20%) sets \nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size=0.2)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#here is the NN model\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nnum_features = 54\n\nmodel = Sequential()\nmodel.add(Dense(units=num_features, activation='relu', kernel_initializer='normal', input_dim=num_features))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dense(units=num_features*2, activation='relu', kernel_initializer='normal'))\nmodel.add(Dense(num_classes, activation='softmax'))\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n#              optimizer='Adadelta',\n              optimizer='Adam',\n              metrics=['accuracy'])","execution_count":9,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train the model\nmodel.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=250)","execution_count":10,"outputs":[{"output_type":"stream","text":"Train on 12096 samples, validate on 3024 samples\nEpoch 1/250\n12096/12096 [==============================] - 2s 126us/sample - loss: 1.1120 - acc: 0.5148 - val_loss: 0.9199 - val_acc: 0.6055\nEpoch 2/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.8479 - acc: 0.6327 - val_loss: 0.8250 - val_acc: 0.6577\nEpoch 3/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.7822 - acc: 0.6597 - val_loss: 0.8108 - val_acc: 0.6511\nEpoch 4/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.7395 - acc: 0.6768 - val_loss: 0.7276 - val_acc: 0.6772\nEpoch 5/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.7174 - acc: 0.6856 - val_loss: 0.7203 - val_acc: 0.6812\nEpoch 6/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.7025 - acc: 0.6916 - val_loss: 0.7015 - val_acc: 0.6915\nEpoch 7/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.6884 - acc: 0.7032 - val_loss: 0.7356 - val_acc: 0.6749\nEpoch 8/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.6762 - acc: 0.7072 - val_loss: 0.6766 - val_acc: 0.7146\nEpoch 9/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.6541 - acc: 0.7251 - val_loss: 0.7479 - val_acc: 0.6961\nEpoch 10/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.6527 - acc: 0.7269 - val_loss: 0.6895 - val_acc: 0.7011\nEpoch 11/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.6387 - acc: 0.7292 - val_loss: 0.6659 - val_acc: 0.7116\nEpoch 12/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.6297 - acc: 0.7349 - val_loss: 0.6524 - val_acc: 0.7262\nEpoch 13/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.6188 - acc: 0.7409 - val_loss: 0.6488 - val_acc: 0.7242\nEpoch 14/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.6104 - acc: 0.7407 - val_loss: 0.6602 - val_acc: 0.7183\nEpoch 15/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.5999 - acc: 0.7447 - val_loss: 0.6150 - val_acc: 0.7295\nEpoch 16/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.5989 - acc: 0.7454 - val_loss: 0.6165 - val_acc: 0.7378\nEpoch 17/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.5900 - acc: 0.7531 - val_loss: 0.6194 - val_acc: 0.7381\nEpoch 18/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.5867 - acc: 0.7517 - val_loss: 0.6366 - val_acc: 0.7394\nEpoch 19/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.5760 - acc: 0.7596 - val_loss: 0.6201 - val_acc: 0.7450\nEpoch 20/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.5735 - acc: 0.7585 - val_loss: 0.6404 - val_acc: 0.7384\nEpoch 21/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.5597 - acc: 0.7652 - val_loss: 0.6109 - val_acc: 0.7401\nEpoch 22/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.5613 - acc: 0.7653 - val_loss: 0.6352 - val_acc: 0.7474\nEpoch 23/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.5471 - acc: 0.7691 - val_loss: 0.6108 - val_acc: 0.7474\nEpoch 24/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.5422 - acc: 0.7663 - val_loss: 0.5970 - val_acc: 0.7619\nEpoch 25/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.5347 - acc: 0.7779 - val_loss: 0.6049 - val_acc: 0.7470\nEpoch 26/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.5346 - acc: 0.7741 - val_loss: 0.5724 - val_acc: 0.7642\nEpoch 27/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.5241 - acc: 0.7808 - val_loss: 0.5728 - val_acc: 0.7662\nEpoch 28/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.5201 - acc: 0.7809 - val_loss: 0.5649 - val_acc: 0.7692\nEpoch 29/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.5114 - acc: 0.7879 - val_loss: 0.5679 - val_acc: 0.7682\nEpoch 30/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.5114 - acc: 0.7845 - val_loss: 0.5635 - val_acc: 0.7622\nEpoch 31/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.4957 - acc: 0.7904 - val_loss: 0.5632 - val_acc: 0.7672\nEpoch 32/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.4960 - acc: 0.7927 - val_loss: 0.5716 - val_acc: 0.7725\nEpoch 33/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.4995 - acc: 0.7889 - val_loss: 0.5407 - val_acc: 0.7751\nEpoch 34/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.4831 - acc: 0.7963 - val_loss: 0.5470 - val_acc: 0.7758\nEpoch 35/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.4778 - acc: 0.8011 - val_loss: 0.5476 - val_acc: 0.7811\nEpoch 36/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.4690 - acc: 0.8053 - val_loss: 0.5505 - val_acc: 0.7748\nEpoch 37/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.4694 - acc: 0.8049 - val_loss: 0.5479 - val_acc: 0.7765\nEpoch 38/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.4693 - acc: 0.8025 - val_loss: 0.5460 - val_acc: 0.7755\nEpoch 39/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.4534 - acc: 0.8075 - val_loss: 0.5319 - val_acc: 0.7837\nEpoch 40/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.4641 - acc: 0.8056 - val_loss: 0.5341 - val_acc: 0.7811\nEpoch 41/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.4456 - acc: 0.8128 - val_loss: 0.5629 - val_acc: 0.7731\nEpoch 42/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.4481 - acc: 0.8137 - val_loss: 0.5563 - val_acc: 0.7808\nEpoch 43/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.4459 - acc: 0.8156 - val_loss: 0.5207 - val_acc: 0.7894\nEpoch 44/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.4362 - acc: 0.8171 - val_loss: 0.5270 - val_acc: 0.7804\nEpoch 45/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.4276 - acc: 0.8194 - val_loss: 0.5759 - val_acc: 0.7741\nEpoch 46/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.4286 - acc: 0.8228 - val_loss: 0.5500 - val_acc: 0.7745\nEpoch 47/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.4254 - acc: 0.8187 - val_loss: 0.5156 - val_acc: 0.7946\nEpoch 48/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.4196 - acc: 0.8294 - val_loss: 0.5742 - val_acc: 0.7712\nEpoch 49/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.4169 - acc: 0.8279 - val_loss: 0.5143 - val_acc: 0.8006\nEpoch 50/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.4060 - acc: 0.8348 - val_loss: 0.5397 - val_acc: 0.7837\nEpoch 51/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.4008 - acc: 0.8393 - val_loss: 0.5559 - val_acc: 0.7834\nEpoch 52/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.4013 - acc: 0.8370 - val_loss: 0.5136 - val_acc: 0.8032\nEpoch 53/250\n12096/12096 [==============================] - 1s 92us/sample - loss: 0.3941 - acc: 0.8370 - val_loss: 0.5223 - val_acc: 0.7857\nEpoch 54/250\n12096/12096 [==============================] - 1s 92us/sample - loss: 0.3919 - acc: 0.8410 - val_loss: 0.5207 - val_acc: 0.7860\nEpoch 55/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.3941 - acc: 0.8386 - val_loss: 0.5073 - val_acc: 0.7973\nEpoch 56/250\n12096/12096 [==============================] - 1s 92us/sample - loss: 0.3778 - acc: 0.8444 - val_loss: 0.5400 - val_acc: 0.7966\nEpoch 57/250\n12096/12096 [==============================] - 1s 90us/sample - loss: 0.3728 - acc: 0.8467 - val_loss: 0.5124 - val_acc: 0.8022\nEpoch 58/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 1s 96us/sample - loss: 0.3796 - acc: 0.8454 - val_loss: 0.5208 - val_acc: 0.8049\nEpoch 59/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3630 - acc: 0.8506 - val_loss: 0.5068 - val_acc: 0.8056\nEpoch 60/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3692 - acc: 0.8479 - val_loss: 0.4809 - val_acc: 0.8185\nEpoch 61/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.3643 - acc: 0.8531 - val_loss: 0.5012 - val_acc: 0.8049\nEpoch 62/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.3545 - acc: 0.8562 - val_loss: 0.5141 - val_acc: 0.8029\nEpoch 63/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.3499 - acc: 0.8582 - val_loss: 0.4986 - val_acc: 0.8069\nEpoch 64/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.3502 - acc: 0.8539 - val_loss: 0.5340 - val_acc: 0.7900\nEpoch 65/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.3451 - acc: 0.8578 - val_loss: 0.5182 - val_acc: 0.8122\nEpoch 66/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.3453 - acc: 0.8598 - val_loss: 0.5226 - val_acc: 0.7963\nEpoch 67/250\n12096/12096 [==============================] - 1s 92us/sample - loss: 0.3393 - acc: 0.8602 - val_loss: 0.5008 - val_acc: 0.8069\nEpoch 68/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3325 - acc: 0.8660 - val_loss: 0.5065 - val_acc: 0.8102\nEpoch 69/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.3288 - acc: 0.8631 - val_loss: 0.5166 - val_acc: 0.8062\nEpoch 70/250\n12096/12096 [==============================] - 1s 93us/sample - loss: 0.3300 - acc: 0.8643 - val_loss: 0.5245 - val_acc: 0.8069\nEpoch 71/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.3263 - acc: 0.8656 - val_loss: 0.5137 - val_acc: 0.8065\nEpoch 72/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.3247 - acc: 0.8696 - val_loss: 0.5072 - val_acc: 0.8102\nEpoch 73/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.3217 - acc: 0.8706 - val_loss: 0.5028 - val_acc: 0.8056\nEpoch 74/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.3139 - acc: 0.8745 - val_loss: 0.5063 - val_acc: 0.8118\nEpoch 75/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3147 - acc: 0.8704 - val_loss: 0.5538 - val_acc: 0.7920\nEpoch 76/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.3128 - acc: 0.8731 - val_loss: 0.5050 - val_acc: 0.8201\nEpoch 77/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.3023 - acc: 0.8775 - val_loss: 0.5044 - val_acc: 0.8224\nEpoch 78/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.3019 - acc: 0.8770 - val_loss: 0.4830 - val_acc: 0.8201\nEpoch 79/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.3049 - acc: 0.8781 - val_loss: 0.5051 - val_acc: 0.8221\nEpoch 80/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.2975 - acc: 0.8787 - val_loss: 0.5161 - val_acc: 0.8211\nEpoch 81/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.2926 - acc: 0.8817 - val_loss: 0.5185 - val_acc: 0.8171\nEpoch 82/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.2973 - acc: 0.8795 - val_loss: 0.5391 - val_acc: 0.8112\nEpoch 83/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.2859 - acc: 0.8829 - val_loss: 0.5001 - val_acc: 0.8274\nEpoch 84/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.2775 - acc: 0.8867 - val_loss: 0.5677 - val_acc: 0.8082\nEpoch 85/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.2833 - acc: 0.8860 - val_loss: 0.5422 - val_acc: 0.8168\nEpoch 86/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.2776 - acc: 0.8886 - val_loss: 0.5177 - val_acc: 0.8151\nEpoch 87/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.2797 - acc: 0.8846 - val_loss: 0.5090 - val_acc: 0.8254\nEpoch 88/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2693 - acc: 0.8931 - val_loss: 0.5362 - val_acc: 0.8165\nEpoch 89/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.2702 - acc: 0.8903 - val_loss: 0.5241 - val_acc: 0.8234\nEpoch 90/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.2677 - acc: 0.8915 - val_loss: 0.5267 - val_acc: 0.8247\nEpoch 91/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2640 - acc: 0.8917 - val_loss: 0.5157 - val_acc: 0.8218\nEpoch 92/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2626 - acc: 0.8930 - val_loss: 0.5536 - val_acc: 0.8075\nEpoch 93/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2600 - acc: 0.8934 - val_loss: 0.5722 - val_acc: 0.8095\nEpoch 94/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.2651 - acc: 0.8915 - val_loss: 0.5247 - val_acc: 0.8280\nEpoch 95/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2530 - acc: 0.8964 - val_loss: 0.5303 - val_acc: 0.8224\nEpoch 96/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.2567 - acc: 0.8964 - val_loss: 0.5086 - val_acc: 0.8313\nEpoch 97/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.2430 - acc: 0.9035 - val_loss: 0.5180 - val_acc: 0.8221\nEpoch 98/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.2449 - acc: 0.9021 - val_loss: 0.5427 - val_acc: 0.8277\nEpoch 99/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.2513 - acc: 0.8977 - val_loss: 0.5012 - val_acc: 0.8304\nEpoch 100/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2414 - acc: 0.9041 - val_loss: 0.5495 - val_acc: 0.8307\nEpoch 101/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.2356 - acc: 0.9044 - val_loss: 0.5029 - val_acc: 0.8376\nEpoch 102/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.2420 - acc: 0.9024 - val_loss: 0.5409 - val_acc: 0.8231\nEpoch 103/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.2358 - acc: 0.9054 - val_loss: 0.5578 - val_acc: 0.8257\nEpoch 104/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.2340 - acc: 0.9063 - val_loss: 0.5493 - val_acc: 0.8237\nEpoch 105/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.2273 - acc: 0.9108 - val_loss: 0.5484 - val_acc: 0.8188\nEpoch 106/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.2337 - acc: 0.9052 - val_loss: 0.5671 - val_acc: 0.8234\nEpoch 107/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.2300 - acc: 0.9066 - val_loss: 0.5186 - val_acc: 0.8317\nEpoch 108/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2235 - acc: 0.9110 - val_loss: 0.5646 - val_acc: 0.8198\nEpoch 109/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.2285 - acc: 0.9080 - val_loss: 0.5468 - val_acc: 0.8251\nEpoch 110/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2216 - acc: 0.9116 - val_loss: 0.5460 - val_acc: 0.8267\nEpoch 111/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.2143 - acc: 0.9122 - val_loss: 0.5632 - val_acc: 0.8204\nEpoch 112/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.2144 - acc: 0.9120 - val_loss: 0.5577 - val_acc: 0.8257\nEpoch 113/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.2200 - acc: 0.9103 - val_loss: 0.5403 - val_acc: 0.8416\nEpoch 114/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.2085 - acc: 0.9148 - val_loss: 0.5678 - val_acc: 0.8300\nEpoch 115/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.2163 - acc: 0.9143 - val_loss: 0.5665 - val_acc: 0.8360\n","name":"stdout"},{"output_type":"stream","text":"Epoch 116/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2057 - acc: 0.9168 - val_loss: 0.5773 - val_acc: 0.8294\nEpoch 117/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2084 - acc: 0.9156 - val_loss: 0.5805 - val_acc: 0.8307\nEpoch 118/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.2050 - acc: 0.9164 - val_loss: 0.5853 - val_acc: 0.8290\nEpoch 119/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.2032 - acc: 0.9168 - val_loss: 0.6131 - val_acc: 0.8218\nEpoch 120/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.2010 - acc: 0.9178 - val_loss: 0.6063 - val_acc: 0.8294\nEpoch 121/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.2011 - acc: 0.9173 - val_loss: 0.5579 - val_acc: 0.8251\nEpoch 122/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.2030 - acc: 0.9169 - val_loss: 0.6054 - val_acc: 0.8188\nEpoch 123/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.2032 - acc: 0.9160 - val_loss: 0.5811 - val_acc: 0.8337\nEpoch 124/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.1964 - acc: 0.9186 - val_loss: 0.5908 - val_acc: 0.8271\nEpoch 125/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1944 - acc: 0.9195 - val_loss: 0.5990 - val_acc: 0.8313\nEpoch 126/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1852 - acc: 0.9241 - val_loss: 0.6253 - val_acc: 0.8211\nEpoch 127/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1899 - acc: 0.9231 - val_loss: 0.5953 - val_acc: 0.8373\nEpoch 128/250\n12096/12096 [==============================] - 1s 94us/sample - loss: 0.1900 - acc: 0.9228 - val_loss: 0.5923 - val_acc: 0.8237\nEpoch 129/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1886 - acc: 0.9228 - val_loss: 0.6052 - val_acc: 0.8284\nEpoch 130/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1842 - acc: 0.9233 - val_loss: 0.6127 - val_acc: 0.8284\nEpoch 131/250\n12096/12096 [==============================] - 2s 127us/sample - loss: 0.1868 - acc: 0.9265 - val_loss: 0.5986 - val_acc: 0.8287\nEpoch 132/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.1727 - acc: 0.9300 - val_loss: 0.5986 - val_acc: 0.8350\nEpoch 133/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.1839 - acc: 0.9268 - val_loss: 0.6012 - val_acc: 0.8310\nEpoch 134/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.1764 - acc: 0.9289 - val_loss: 0.6123 - val_acc: 0.8277\nEpoch 135/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1788 - acc: 0.9262 - val_loss: 0.6521 - val_acc: 0.8307\nEpoch 136/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.1733 - acc: 0.9293 - val_loss: 0.5925 - val_acc: 0.8370\nEpoch 137/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.1650 - acc: 0.9313 - val_loss: 0.6747 - val_acc: 0.8284\nEpoch 138/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.1780 - acc: 0.9285 - val_loss: 0.6466 - val_acc: 0.8333\nEpoch 139/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.1691 - acc: 0.9322 - val_loss: 0.6128 - val_acc: 0.8313\nEpoch 140/250\n12096/12096 [==============================] - 1s 115us/sample - loss: 0.1750 - acc: 0.9289 - val_loss: 0.6070 - val_acc: 0.8393\nEpoch 141/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.1681 - acc: 0.9326 - val_loss: 0.6184 - val_acc: 0.8297\nEpoch 142/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.1686 - acc: 0.9311 - val_loss: 0.6963 - val_acc: 0.8115\nEpoch 143/250\n12096/12096 [==============================] - 1s 111us/sample - loss: 0.1755 - acc: 0.9305 - val_loss: 0.6123 - val_acc: 0.8271\nEpoch 144/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.1584 - acc: 0.9352 - val_loss: 0.6611 - val_acc: 0.8178\nEpoch 145/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1553 - acc: 0.9358 - val_loss: 0.6685 - val_acc: 0.8317\nEpoch 146/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.1612 - acc: 0.9352 - val_loss: 0.6427 - val_acc: 0.8370\nEpoch 147/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.1648 - acc: 0.9334 - val_loss: 0.6813 - val_acc: 0.8274\nEpoch 148/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.1596 - acc: 0.9339 - val_loss: 0.6690 - val_acc: 0.8221\nEpoch 149/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1562 - acc: 0.9371 - val_loss: 0.6534 - val_acc: 0.8317\nEpoch 150/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1591 - acc: 0.9366 - val_loss: 0.6446 - val_acc: 0.8294\nEpoch 151/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1512 - acc: 0.9394 - val_loss: 0.6459 - val_acc: 0.8347\nEpoch 152/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1589 - acc: 0.9381 - val_loss: 0.6675 - val_acc: 0.8234\nEpoch 153/250\n12096/12096 [==============================] - 1s 95us/sample - loss: 0.1476 - acc: 0.9420 - val_loss: 0.6753 - val_acc: 0.8380\nEpoch 154/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1563 - acc: 0.9372 - val_loss: 0.6575 - val_acc: 0.8323\nEpoch 155/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.1390 - acc: 0.9426 - val_loss: 0.6653 - val_acc: 0.8323\nEpoch 156/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.1487 - acc: 0.9396 - val_loss: 0.6808 - val_acc: 0.8304\nEpoch 157/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.1525 - acc: 0.9369 - val_loss: 0.6533 - val_acc: 0.8370\nEpoch 158/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.1468 - acc: 0.9431 - val_loss: 0.6764 - val_acc: 0.8320\nEpoch 159/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1586 - acc: 0.9341 - val_loss: 0.6661 - val_acc: 0.8347\nEpoch 160/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1410 - acc: 0.9421 - val_loss: 0.6704 - val_acc: 0.8307\nEpoch 161/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.1324 - acc: 0.9460 - val_loss: 0.7490 - val_acc: 0.8267\nEpoch 162/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.1624 - acc: 0.9344 - val_loss: 0.6921 - val_acc: 0.8333\nEpoch 163/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.1326 - acc: 0.9478 - val_loss: 0.6633 - val_acc: 0.8380\nEpoch 164/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1501 - acc: 0.9400 - val_loss: 0.6545 - val_acc: 0.8426\nEpoch 165/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1338 - acc: 0.9454 - val_loss: 0.6848 - val_acc: 0.8327\nEpoch 166/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.1199 - acc: 0.9515 - val_loss: 0.6827 - val_acc: 0.8313\nEpoch 167/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1422 - acc: 0.9444 - val_loss: 0.6901 - val_acc: 0.8327\nEpoch 168/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.1572 - acc: 0.9379 - val_loss: 0.6770 - val_acc: 0.8433\nEpoch 169/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.1316 - acc: 0.9477 - val_loss: 0.7013 - val_acc: 0.8287\nEpoch 170/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.1339 - acc: 0.9476 - val_loss: 0.7287 - val_acc: 0.8380\nEpoch 171/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.1365 - acc: 0.9467 - val_loss: 0.7070 - val_acc: 0.8317\nEpoch 172/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1238 - acc: 0.9503 - val_loss: 0.7636 - val_acc: 0.8290\nEpoch 173/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 1s 100us/sample - loss: 0.1300 - acc: 0.9478 - val_loss: 0.7367 - val_acc: 0.8320\nEpoch 174/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1270 - acc: 0.9488 - val_loss: 0.7041 - val_acc: 0.8370\nEpoch 175/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1303 - acc: 0.9486 - val_loss: 0.7231 - val_acc: 0.8356\nEpoch 176/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.1236 - acc: 0.9496 - val_loss: 0.7575 - val_acc: 0.8350\nEpoch 177/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.1363 - acc: 0.9454 - val_loss: 0.7801 - val_acc: 0.8194\nEpoch 178/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.1349 - acc: 0.9474 - val_loss: 0.7546 - val_acc: 0.8320\nEpoch 179/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1297 - acc: 0.9503 - val_loss: 0.7376 - val_acc: 0.8330\nEpoch 180/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1245 - acc: 0.9503 - val_loss: 0.7370 - val_acc: 0.8271\nEpoch 181/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.1166 - acc: 0.9533 - val_loss: 0.7591 - val_acc: 0.8376\nEpoch 182/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.1270 - acc: 0.9482 - val_loss: 0.7721 - val_acc: 0.8251\nEpoch 183/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.1355 - acc: 0.9482 - val_loss: 0.7202 - val_acc: 0.8264\nEpoch 184/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.1248 - acc: 0.9516 - val_loss: 0.7884 - val_acc: 0.8304\nEpoch 185/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.1120 - acc: 0.9564 - val_loss: 0.8169 - val_acc: 0.8327\nEpoch 186/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.1196 - acc: 0.9543 - val_loss: 0.8132 - val_acc: 0.8185\nEpoch 187/250\n12096/12096 [==============================] - 1s 111us/sample - loss: 0.1227 - acc: 0.9508 - val_loss: 0.7453 - val_acc: 0.8390\nEpoch 188/250\n12096/12096 [==============================] - 1s 113us/sample - loss: 0.1161 - acc: 0.9535 - val_loss: 0.7528 - val_acc: 0.8313\nEpoch 189/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.1211 - acc: 0.9506 - val_loss: 0.7929 - val_acc: 0.8419\nEpoch 190/250\n12096/12096 [==============================] - 1s 105us/sample - loss: 0.1182 - acc: 0.9519 - val_loss: 0.7534 - val_acc: 0.8353\nEpoch 191/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.1067 - acc: 0.9563 - val_loss: 0.7909 - val_acc: 0.8350\nEpoch 192/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1207 - acc: 0.9521 - val_loss: 0.7295 - val_acc: 0.8383\nEpoch 193/250\n12096/12096 [==============================] - 1s 100us/sample - loss: 0.1134 - acc: 0.9545 - val_loss: 0.8388 - val_acc: 0.8347\nEpoch 194/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.0972 - acc: 0.9614 - val_loss: 0.7886 - val_acc: 0.8360\nEpoch 195/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1231 - acc: 0.9521 - val_loss: 0.8265 - val_acc: 0.8271\nEpoch 196/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1161 - acc: 0.9538 - val_loss: 0.7793 - val_acc: 0.8373\nEpoch 197/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1104 - acc: 0.9568 - val_loss: 0.7860 - val_acc: 0.8300\nEpoch 198/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1058 - acc: 0.9575 - val_loss: 0.8734 - val_acc: 0.8287\nEpoch 199/250\n12096/12096 [==============================] - 1s 98us/sample - loss: 0.1227 - acc: 0.9510 - val_loss: 0.8133 - val_acc: 0.8313\nEpoch 200/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.1074 - acc: 0.9559 - val_loss: 0.8348 - val_acc: 0.8280\nEpoch 201/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.1075 - acc: 0.9568 - val_loss: 0.7594 - val_acc: 0.8403\nEpoch 202/250\n12096/12096 [==============================] - 1s 115us/sample - loss: 0.1089 - acc: 0.9544 - val_loss: 0.8807 - val_acc: 0.8280\nEpoch 203/250\n12096/12096 [==============================] - 1s 122us/sample - loss: 0.1139 - acc: 0.9544 - val_loss: 0.7909 - val_acc: 0.8373\nEpoch 204/250\n12096/12096 [==============================] - 1s 124us/sample - loss: 0.1042 - acc: 0.9568 - val_loss: 0.7771 - val_acc: 0.8469\nEpoch 205/250\n12096/12096 [==============================] - 1s 124us/sample - loss: 0.1109 - acc: 0.9538 - val_loss: 0.7862 - val_acc: 0.8383\nEpoch 206/250\n12096/12096 [==============================] - 2s 136us/sample - loss: 0.0912 - acc: 0.9631 - val_loss: 0.8537 - val_acc: 0.8380\nEpoch 207/250\n12096/12096 [==============================] - 2s 150us/sample - loss: 0.1145 - acc: 0.9552 - val_loss: 0.7911 - val_acc: 0.8370\nEpoch 208/250\n12096/12096 [==============================] - 2s 155us/sample - loss: 0.1070 - acc: 0.9589 - val_loss: 0.8472 - val_acc: 0.8294\nEpoch 209/250\n12096/12096 [==============================] - 2s 134us/sample - loss: 0.1170 - acc: 0.9541 - val_loss: 0.8139 - val_acc: 0.8406\nEpoch 210/250\n12096/12096 [==============================] - 1s 120us/sample - loss: 0.0981 - acc: 0.9623 - val_loss: 0.8448 - val_acc: 0.8426\nEpoch 211/250\n12096/12096 [==============================] - 1s 117us/sample - loss: 0.0979 - acc: 0.9589 - val_loss: 0.8467 - val_acc: 0.8376\nEpoch 212/250\n12096/12096 [==============================] - 1s 115us/sample - loss: 0.1072 - acc: 0.9583 - val_loss: 0.8493 - val_acc: 0.8376\nEpoch 213/250\n12096/12096 [==============================] - 1s 113us/sample - loss: 0.0988 - acc: 0.9634 - val_loss: 0.8553 - val_acc: 0.8337\nEpoch 214/250\n12096/12096 [==============================] - 1s 113us/sample - loss: 0.1019 - acc: 0.9598 - val_loss: 0.8747 - val_acc: 0.8350\nEpoch 215/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.1053 - acc: 0.9597 - val_loss: 0.8160 - val_acc: 0.8317\nEpoch 216/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.0948 - acc: 0.9625 - val_loss: 0.8782 - val_acc: 0.8436\nEpoch 217/250\n12096/12096 [==============================] - 1s 112us/sample - loss: 0.0986 - acc: 0.9605 - val_loss: 0.8150 - val_acc: 0.8390\nEpoch 218/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.1055 - acc: 0.9568 - val_loss: 0.8529 - val_acc: 0.8363\nEpoch 219/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.1107 - acc: 0.9567 - val_loss: 0.8342 - val_acc: 0.8469\nEpoch 220/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.0995 - acc: 0.9612 - val_loss: 0.7921 - val_acc: 0.8472\nEpoch 221/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.0888 - acc: 0.9655 - val_loss: 0.8185 - val_acc: 0.8423\nEpoch 222/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.0953 - acc: 0.9639 - val_loss: 0.8781 - val_acc: 0.8366\nEpoch 223/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.0959 - acc: 0.9620 - val_loss: 0.8659 - val_acc: 0.8307\nEpoch 224/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.1158 - acc: 0.9542 - val_loss: 0.8230 - val_acc: 0.8403\nEpoch 225/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.0868 - acc: 0.9669 - val_loss: 0.9479 - val_acc: 0.8307\nEpoch 226/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.1030 - acc: 0.9590 - val_loss: 0.8988 - val_acc: 0.8446\nEpoch 227/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.0978 - acc: 0.9616 - val_loss: 0.8493 - val_acc: 0.8423\nEpoch 228/250\n12096/12096 [==============================] - 1s 108us/sample - loss: 0.0864 - acc: 0.9670 - val_loss: 0.9193 - val_acc: 0.8409\nEpoch 229/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.0804 - acc: 0.9690 - val_loss: 0.8798 - val_acc: 0.8409\nEpoch 230/250\n","name":"stdout"},{"output_type":"stream","text":"12096/12096 [==============================] - 1s 111us/sample - loss: 0.0818 - acc: 0.9683 - val_loss: 0.9085 - val_acc: 0.8254\nEpoch 231/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.1061 - acc: 0.9609 - val_loss: 0.8988 - val_acc: 0.8370\nEpoch 232/250\n12096/12096 [==============================] - 1s 107us/sample - loss: 0.0828 - acc: 0.9674 - val_loss: 0.9100 - val_acc: 0.8370\nEpoch 233/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.1086 - acc: 0.9563 - val_loss: 0.8849 - val_acc: 0.8380\nEpoch 234/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.0811 - acc: 0.9679 - val_loss: 0.8260 - val_acc: 0.8446\nEpoch 235/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.0978 - acc: 0.9630 - val_loss: 0.8608 - val_acc: 0.8419\nEpoch 236/250\n12096/12096 [==============================] - 1s 96us/sample - loss: 0.1022 - acc: 0.9614 - val_loss: 0.8818 - val_acc: 0.8323\nEpoch 237/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.0902 - acc: 0.9657 - val_loss: 0.8600 - val_acc: 0.8363\nEpoch 238/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.0769 - acc: 0.9682 - val_loss: 0.9042 - val_acc: 0.8323\nEpoch 239/250\n12096/12096 [==============================] - 1s 103us/sample - loss: 0.0955 - acc: 0.9646 - val_loss: 0.9081 - val_acc: 0.8396\nEpoch 240/250\n12096/12096 [==============================] - 1s 104us/sample - loss: 0.0926 - acc: 0.9643 - val_loss: 0.8811 - val_acc: 0.8433\nEpoch 241/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.0804 - acc: 0.9702 - val_loss: 0.9104 - val_acc: 0.8380\nEpoch 242/250\n12096/12096 [==============================] - 1s 110us/sample - loss: 0.0956 - acc: 0.9645 - val_loss: 0.8833 - val_acc: 0.8476\nEpoch 243/250\n12096/12096 [==============================] - 1s 106us/sample - loss: 0.0833 - acc: 0.9703 - val_loss: 0.9259 - val_acc: 0.8274\nEpoch 244/250\n12096/12096 [==============================] - 1s 109us/sample - loss: 0.0880 - acc: 0.9668 - val_loss: 0.9144 - val_acc: 0.8330\nEpoch 245/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.0911 - acc: 0.9673 - val_loss: 0.9086 - val_acc: 0.8323\nEpoch 246/250\n12096/12096 [==============================] - 1s 102us/sample - loss: 0.1045 - acc: 0.9587 - val_loss: 0.8935 - val_acc: 0.8386\nEpoch 247/250\n12096/12096 [==============================] - 1s 97us/sample - loss: 0.0920 - acc: 0.9656 - val_loss: 0.8864 - val_acc: 0.8383\nEpoch 248/250\n12096/12096 [==============================] - 1s 99us/sample - loss: 0.0805 - acc: 0.9686 - val_loss: 0.8832 - val_acc: 0.8462\nEpoch 249/250\n12096/12096 [==============================] - 1s 101us/sample - loss: 0.0805 - acc: 0.9697 - val_loss: 0.9047 - val_acc: 0.8380\nEpoch 250/250\n12096/12096 [==============================] - 1s 122us/sample - loss: 0.0704 - acc: 0.9729 - val_loss: 0.9617 - val_acc: 0.8300\n","name":"stdout"},{"output_type":"execute_result","execution_count":10,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7f5af414cdd8>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Predict!!\ny_predict = model.predict(x_predict)","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(10):\n\tprint(y_predict[i], np.argmax(y_predict[i])+1)","execution_count":12,"outputs":[{"output_type":"stream","text":"[8.414865e-01 7.237909e-02 4.306114e-05 2.715460e-08 6.398066e-05\n 8.602745e-02 2.677694e-19] 1\n[7.4224502e-01 2.5773385e-01 2.1245764e-13 2.0493008e-17 1.4452861e-05\n 6.7365795e-06 7.5400352e-14] 1\n[6.2340701e-01 3.7651679e-01 1.0213161e-09 1.8340593e-13 5.2895448e-05\n 2.3341872e-05 4.0457043e-10] 1\n[4.71093059e-01 5.28418005e-01 3.24236964e-07 9.71750198e-12\n 3.89786524e-04 9.87956882e-05 1.23649615e-11] 2\n[1.3467045e-01 8.6281317e-01 4.4106127e-06 4.8256316e-11 2.0503860e-03\n 4.6154790e-04 2.5410815e-15] 2\n[5.0568670e-01 4.9399433e-01 1.4308684e-07 6.5175539e-12 2.5272643e-04\n 6.6201501e-05 8.6369453e-11] 1\n[5.9468442e-01 4.0517539e-01 1.3997897e-08 1.3008982e-12 1.0347926e-04\n 3.6679743e-05 3.0776687e-10] 1\n[9.0184182e-01 9.8130278e-02 2.2322337e-11 1.4830036e-14 2.0826541e-05\n 7.0796727e-06 2.5347999e-10] 1\n[9.9848640e-01 1.5116740e-03 4.5566169e-11 8.5353376e-14 5.4590782e-07\n 1.3468494e-06 1.2924902e-11] 1\n[9.0687191e-01 9.3070656e-02 1.8019919e-09 5.0467210e-13 3.2119693e-05\n 2.5265143e-05 5.8928536e-11] 1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save predictions to a file for submission\n#argmax give us the highest probable label\n# we add one to the predictions to scale from 0..6 to 1..7\noutput = pd.DataFrame({'Id': Ids,\n                       'Cover_Type': y_predict.argmax(axis=1)+1})\noutput.to_csv('submission.csv', index=False)","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a link to download the file    \nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","execution_count":14,"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"/kaggle/working/submission.csv","text/html":"<a href='submission.csv' target='_blank'>submission.csv</a><br>"},"metadata":{}}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}